{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408925bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import gzip\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "\n",
    "from video_writer import VideoWriter\n",
    "\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as linalg\n",
    "from matplotlib import cm\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import correlate1d\n",
    "from scipy.stats import t as t_dist\n",
    "from scipy.stats import multivariate_normal as mvnd\n",
    "from scipy.signal import (\n",
    "    correlate,\n",
    "    cspline1d,\n",
    "    find_peaks,\n",
    "    cwt,\n",
    "    ricker,\n",
    "    medfilt,\n",
    "    morlet2,\n",
    "    hilbert,\n",
    "    filtfilt,\n",
    "    butter,\n",
    "    sepfir2d,\n",
    ")\n",
    "import ruptures as rpt\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "# from dtw import dtw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4011013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smooth_and_norm_real(vals, winsize=95, smoothing=0.2, zcutoff=3.0):\n",
    "    win = np.ones((winsize,))\n",
    "    win_count = correlate(np.ones_like(vals), win)\n",
    "    signal = np.copy(vals)\n",
    "    init_rms = np.sqrt(correlate(np.square(signal), win) / win_count)\n",
    "    init_cond = np.where(\n",
    "        init_rms[: 1 - winsize] < init_rms[winsize - 1 :],\n",
    "        init_rms[: 1 - winsize],\n",
    "        init_rms[winsize - 1 :],\n",
    "    )\n",
    "    for _ in range(8):\n",
    "        moving_rms = np.sqrt(correlate(np.square(signal), win).clip(0.0) / win_count)\n",
    "        min_rms = np.minimum(moving_rms[: 1 - winsize], moving_rms[winsize - 1 :]).clip(\n",
    "            1e-5\n",
    "        )\n",
    "        magclip = (\n",
    "            zcutoff * min_rms / np.maximum(min_rms * zcutoff, np.abs(signal)).clip(1e-5)\n",
    "        )\n",
    "        signal *= magclip\n",
    "    moving_avg = correlate(signal / min_rms, win) / win_count\n",
    "    min_avg = np.where(\n",
    "        init_cond,\n",
    "        moving_avg[: 1 - winsize],\n",
    "        moving_avg[winsize - 1 :],\n",
    "    )\n",
    "    min_avg = moving_avg[winsize // 2 : -(winsize // 2)]\n",
    "\n",
    "    smoothed = cspline1d(signal / min_rms - min_avg, lamb=smoothing)\n",
    "\n",
    "    return smoothed, vals / min_rms - min_avg\n",
    "\n",
    "\n",
    "def get_rms(signal, winsize=95):\n",
    "    win = np.ones((winsize,))\n",
    "    win_count = correlate(np.ones_like(signal), win, mode=\"same\")\n",
    "    rms = np.sqrt(correlate(np.square(signal), win, mode=\"same\") / win_count)\n",
    "    return rms\n",
    "\n",
    "def norm_complex(real, imag, winsize=95, smoothing=0.2, zcutoff=3.0):\n",
    "    win = np.ones((winsize,))\n",
    "    win_count = correlate(np.ones_like(real), win)\n",
    "    signal = real + 1j * imag\n",
    "    for _ in range(8):\n",
    "        moving_rms = np.sqrt(correlate(np.square(np.abs(signal)), win) / win_count)\n",
    "        min_rms = np.minimum(moving_rms[: 1 - winsize], moving_rms[winsize - 1 :])\n",
    "        magclip = (\n",
    "            zcutoff * min_rms / np.maximum(min_rms * zcutoff, np.abs(signal)).clip(1e-5)\n",
    "        )\n",
    "        signal *= magclip\n",
    "    return (real + 1.0j * imag) / min_rms\n",
    "\n",
    "\n",
    "def smooth_and_norm_complex_stitch(\n",
    "    real_lead, imag_lead, real_lag, imag_lag, winsize=95, smoothing=0.2, zcutoff=3.0\n",
    "):\n",
    "    orig_ld_re = cspline1d(real_lead, lamb=smoothing)\n",
    "    orig_ld_im = cspline1d(imag_lead, lamb=smoothing)\n",
    "    orig_lg_re = cspline1d(real_lag, lamb=smoothing)\n",
    "    orig_lg_im = cspline1d(imag_lag, lamb=smoothing)\n",
    "    win = np.ones((winsize,))\n",
    "    win_count = correlate(np.ones_like(real_lead), win)\n",
    "    lead_signal = orig_ld_re + 1.0j * orig_ld_im\n",
    "    lag_signal = orig_lg_re + 1.0j * orig_lg_im\n",
    "    # TODO: pick a better way of stitching together, eg which is waviest?\n",
    "    # TODO: also should do a moving average of each individual component and subtract it\n",
    "    # currently there are jumps when it switches from one to the other. These suck.\n",
    "    # TODO: if a component is NOT very wavy, should shrink it by, eg, 1/3\n",
    "    # currently some components are very noisy in some places and this causes problems.\n",
    "    lead_rms = np.sqrt(correlate(np.square(np.abs(lead_signal)), win) / win_count)\n",
    "    lag_rms = np.sqrt(correlate(np.square(np.abs(lag_signal)), win) / win_count)\n",
    "    signal = np.where(\n",
    "        lead_rms[: 1 - winsize] < lag_rms[winsize - 1 :], lead_signal, lag_signal\n",
    "    )\n",
    "    orig_sig = np.copy(signal)\n",
    "    for _ in range(8):\n",
    "        moving_rms = np.sqrt(correlate(np.square(np.abs(signal)), win) / win_count)\n",
    "        min_rms = np.minimum(moving_rms[: 1 - winsize], moving_rms[winsize - 1 :])\n",
    "        magclip = (\n",
    "            zcutoff * min_rms / np.maximum(min_rms * zcutoff, np.abs(signal)).clip(1e-5)\n",
    "        )\n",
    "        signal *= magclip\n",
    "    return orig_sig / min_rms\n",
    "\n",
    "\n",
    "def fit_linear_lead_and_lag(yi, winsize=155):\n",
    "    k = winsize - 1\n",
    "    sd = k * (k + 1) / 2\n",
    "    sdsq = k * (k + 1) * (2 * k + 1) / 6\n",
    "    win = np.ones(winsize)\n",
    "    invdenom = 1.0 / (sdsq * winsize - sd ** 2)\n",
    "    slp_kern = np.arange(winsize)\n",
    "    avg_kern = np.full(winsize, 1.0 / winsize)\n",
    "    fslp_lag = correlate(yi, slp_kern, mode=\"full\")[winsize - 1:]\n",
    "    fslp_lead = correlate(yi, -slp_kern[::-1], mode=\"full\")[:1 - winsize]\n",
    "    fsum = correlate(yi, win, mode=\"full\")\n",
    "    fsum_lead = fsum[:1 - winsize]\n",
    "    fsum_lag = fsum[winsize - 1:]\n",
    "    yi2 = np.square(yi)\n",
    "    sumsq = correlate(yi2, win, mode=\"full\")\n",
    "    sumsq_lead = sumsq[:1 - winsize]\n",
    "    sumsq_lag = sumsq[winsize - 1:]\n",
    "    m_lag = (fslp_lag * winsize - sd * fsum_lag) * invdenom\n",
    "    b_lag = (fsum_lag * sdsq - sd * fslp_lag) * invdenom\n",
    "    m_lead = (fslp_lead * winsize + sd * fsum_lead) * invdenom\n",
    "    b_lead = (fsum_lead * sdsq + sd * fslp_lead) * invdenom\n",
    "    serr_lag = (\n",
    "        np.square(b_lag) * winsize\n",
    "        - 2 * b_lag * fsum_lag\n",
    "        + 2 * b_lag * m_lag * sd\n",
    "        + sumsq_lag\n",
    "        - 2 * m_lag * fslp_lag\n",
    "        + np.square(m_lag) * sdsq\n",
    "    )\n",
    "    serr_lead = (\n",
    "        np.square(b_lead) * winsize\n",
    "        - 2 * b_lead * fsum_lead\n",
    "        - 2 * b_lead * m_lead * sd\n",
    "        + sumsq_lead\n",
    "        - 2 * m_lead * fslp_lead\n",
    "        + np.square(m_lead) * sdsq\n",
    "    )\n",
    "    \n",
    "    sst_lead = sumsq_lead - np.square(fsum_lead) / winsize\n",
    "    sst_lag = sumsq_lag - np.square(fsum_lag) / winsize\n",
    "    \n",
    "    r2_lead = 1.0 - serr_lead / sst_lead.clip(1e-6)\n",
    "    r2_lag = 1.0 - serr_lag / sst_lag.clip(1e-6)\n",
    "\n",
    "    return m_lead * winsize, m_lag * winsize, b_lead, b_lag, np.sqrt(serr_lead.clip(min=0.0) / winsize), np.sqrt(serr_lag.clip(min=0.0)  / winsize), r2_lead, r2_lag\n",
    "\n",
    "\n",
    "def compute_stats(xi, yi, winsize=255, mode=\"nearest\"):\n",
    "    hw = winsize // 2\n",
    "    hwmo = (winsize - 1) // 2\n",
    "    win = np.ones((winsize,))\n",
    "    xiyi = xi * yi\n",
    "    xi2 = np.square(xi)\n",
    "    yi2 = np.square(yi)\n",
    "    win_xiyi = correlate1d(xiyi, win, mode=mode)\n",
    "    win_xi2 = correlate1d(xi2, win, mode=mode)\n",
    "    win_yi2 = correlate1d(yi2, win, mode=mode)\n",
    "    m = win_xiyi / win_xi2.clip(1e-8)\n",
    "    b = correlate1d(yi, win, mode=mode)\n",
    "    idx = np.arange(xi.shape[0])[:, None] + np.arange(winsize) - hwmo\n",
    "    ywin = yi[idx.clip(min=0, max=xi.size - 1)]\n",
    "    yhat = idx * m[:, None] + b[:, None]\n",
    "    sse = np.sum(np.square(ywin - yhat), axis=1)\n",
    "    r2 = 1 - sse / win_yi2.clip(min=1e-9)\n",
    "\n",
    "    # angles = -np.arctan(m)\n",
    "    # angles = np.concatenate(\n",
    "    #     (np.full((hwmo,), angles[0]), angles, np.full((hw,), angles[-1]))\n",
    "    # )\n",
    "    # R = np.array([[np.cos(angles), -np.sin(angles)], [np.sin(angles), np.cos(angles)]])\n",
    "    # input = np.stack((xi, yi), axis=0)[:, None, :]\n",
    "    # res = np.sum(R * input, axis=1)[0]\n",
    "\n",
    "    # res3 = res ** 3\n",
    "    # win_res3 = correlate(res3, win, mode=mode)\n",
    "    # skew = np.concatenate(\n",
    "    #     (np.full((hw,), win_res3[0]), win_res3, np.full((hw,), win_res3[-1]))\n",
    "    # )\n",
    "\n",
    "    return b, m, r2, sse / winsize\n",
    "\n",
    "\n",
    "def correlate_peaks(sig, start, length, min_shift, chunk):\n",
    "    seg = sig[start : start + chunk]\n",
    "    comp = sig[start + min_shift : start + length + min_shift]\n",
    "    return correlate(comp, seg, mode=\"valid\"), seg, comp\n",
    "\n",
    "\n",
    "def get_turning_points(sig):\n",
    "    deltas = sig[1:] - sig[:-1]\n",
    "    turn_pts = deltas[1:] * deltas[:-1] <= 0.0\n",
    "    curvature = correlate(\n",
    "        sig, np.array([5.0, 0.0, -3.0, -4.0, -3.0, 0.0, 5.0]), mode=\"same\"\n",
    "    )\n",
    "\n",
    "    peaks = np.argwhere(np.logical_and(deltas[:-1] > 0.0, turn_pts))\n",
    "    troughs = np.argwhere(np.logical_and(deltas[:-1] < 0.0, turn_pts))\n",
    "\n",
    "    # unlikely that a true breath would be faster than 3 seconds, ie 15 frames\n",
    "    dist = 15\n",
    "    peak_dists = peaks[1:] - peaks[:-1]\n",
    "    trough_dists = troughs[1:] - troughs[:-1]\n",
    "\n",
    "    # find the highest peak, eliminate any peaks too closeby, which means also\n",
    "    # eliminating some troughs... but which ones... fuck, I have to think about this.\n",
    "    while False:\n",
    "        pass\n",
    "\n",
    "\n",
    "def compute_r2(data, fit, winsize=255):\n",
    "    hw = winsize // 2\n",
    "    win = np.ones((winsize,))\n",
    "    err = data - fit\n",
    "    avgs = correlate(data, win, mode=\"same\") / winsize\n",
    "    sum_err2 = correlate(np.square(err), win, mode=\"same\")\n",
    "    idx = np.arange(data.shape[0])[:, None] + np.arange(winsize)\n",
    "    data_ext = np.concatenate((np.zeros(hw), data, np.zeros(hw)), axis=0)\n",
    "    sum_var = np.sum(np.square(data_ext[idx] - avgs[:, None]), axis=1)\n",
    "    sum_var = np.var(data_ext[idx] - avgs[:, None], axis=1) * winsize\n",
    "    r2 = 1 - sum_err2 / sum_var.clip(1e-5)\n",
    "\n",
    "    return r2, err, sum_err2, sum_var\n",
    "\n",
    "\n",
    "def get_findiff_curvature(data):\n",
    "    curvature = np.zeros_like(data)\n",
    "    curvature[1:-1] = data[:-2] + data[2:] - 2.0 * data[1:-1]\n",
    "    curvature[0] = curvature[1]\n",
    "    curvature[-1] = curvature[-2]\n",
    "    return curvature\n",
    "\n",
    "\n",
    "def morlet_real(*args, **kwargs):\n",
    "    return np.real(morlet2(*args, **kwargs))\n",
    "\n",
    "\n",
    "def get_slopes(data):\n",
    "    dx = data[:, 1:] - data[:, :-1]  # x right\n",
    "    dy = data[1:] - data[:-1]  # y down\n",
    "    de = data[1:, 1:] - data[:-1, :-1]  # y down, x right\n",
    "    do = data[1:, :-1] - data[:-1, 1:]  # y down, x left\n",
    "\n",
    "    tl = de[:-1, :-1]\n",
    "    tc = dy[:-1, 1:-1]\n",
    "    tr = do[:-1, 1:]\n",
    "    rc = -dx[1:-1, 1:]\n",
    "    br = -de[1:, 1:]\n",
    "    bc = -dy[1:, 1:-1]\n",
    "    bl = -do[1:, :-1]\n",
    "    lc = dx[1:-1, :-1]\n",
    "\n",
    "    res = np.zeros((8, data.shape[0], data.shape[1]))\n",
    "    res[:, 1:-1, 1:-1] = np.stack((tl, tc, tr, rc, br, bc, bl, lc), axis=0)\n",
    "    return res\n",
    "\n",
    "\n",
    "def wavefinding_cwt(signal, widths, omega=5):\n",
    "    output = np.empty((len(widths), len(signal)), dtype=np.complex128)\n",
    "    for ind, width in enumerate(widths):\n",
    "        # go for an odd window length about 8x the length of the width\n",
    "        N = round(4 * width - 0.5) * 2 + 1\n",
    "        N = np.min([N, len(signal)])\n",
    "        wavelet_data = morlet2(N, width, omega)\n",
    "        # using correlate instead of convolve\n",
    "        output[ind] = correlate(\n",
    "            signal.astype(np.complex128), wavelet_data, mode=\"same\"\n",
    "        ) * np.exp(-1.0j * omega * np.arange(len(signal)) / width)\n",
    "    return output\n",
    "\n",
    "\n",
    "def suppress_noise(real, imag):\n",
    "    \"\"\"Normalizes the traces, then compares them and weights the wavier one higher.\"\"\"\n",
    "    real_norm, _ = smooth_and_norm_real(real)\n",
    "    imag_norm, _ = smooth_and_norm_real(imag)\n",
    "    omega = 20.0\n",
    "    fs = 5.0\n",
    "    freqs = np.logspace(0.1, -1.4, 150)  # ~50-85 are breathing frequencies\n",
    "    widths_morlet = omega * fs / (freqs[50:85] * 2 * np.pi)\n",
    "    real_wave = wavefinding_cwt(real_norm, widths_morlet, omega)\n",
    "    mags_real = np.sum(np.square(np.abs(real_wave)), axis=0)\n",
    "    imag_wave = wavefinding_cwt(imag_norm, widths_morlet, omega)\n",
    "    mags_imag = np.sum(np.square(np.abs(imag_wave)), axis=0)\n",
    "    # clip at 4 - don't want to be making up bs waves\n",
    "    ratio = np.square(mags_real / mags_imag).clip(0.25, 4.0)\n",
    "    # normalize again after the ratio thing\n",
    "    normed = norm_complex(real_norm * ratio, imag_norm / ratio)\n",
    "    # these work pretty well I've found\n",
    "    widths_peak = np.arange(4, 80) * 0.2\n",
    "    cplx_peaks = cwt(normed, ricker, widths_peak, dtype=np.complex128)\n",
    "    cplx_slopes = get_slopes(np.abs(cplx_peaks))\n",
    "    peak_cond = np.sum(np.sign(cplx_slopes), axis=0) == 8.0\n",
    "    peak_vals = cplx_peaks[peak_cond]\n",
    "    arg = np.mod(np.angle(np.square(cplx_peaks)) * 0.5 / np.pi, 1.0)\n",
    "    cols = cm.hsv(arg)[..., :3]\n",
    "    mags = np.abs(cplx_peaks)\n",
    "    mags *= 999.0 / np.max(mags)\n",
    "    mags += 1.0\n",
    "    cols *= np.log10(mags[..., None]) / 3.0\n",
    "    cols[peak_cond, :] = 0.0\n",
    "\n",
    "    amax = np.argmax(np.abs(cplx_peaks[:26]), axis=0).astype(float)\n",
    "    amax_sm = np.around(cspline1d(medfilt(amax, 1023), 5000000)).astype(int)\n",
    "    asym_trace = np.mean(np.abs(cplx_peaks[5:25]), axis=0)\n",
    "    max_trace = np.abs(cplx_peaks[amax_sm, np.arange(amax.size)])\n",
    "\n",
    "    # find the minima of both traces\n",
    "    asym_min = np.logical_and(\n",
    "        asym_trace[:-2] > asym_trace[1:-1], asym_trace[1:-1] < asym_trace[2:]\n",
    "    )\n",
    "    max_min = np.logical_and(\n",
    "        max_trace[:-2] > max_trace[1:-1], max_trace[1:-1] < max_trace[2:]\n",
    "    )\n",
    "    asym_mean = np.mean(cplx_peaks[6:35], axis=0)  # empirical values\n",
    "    asym_rot = np.angle(asym_mean[1:] / asym_mean[:-1])\n",
    "    rot_conv = correlate(asym_rot, np.ones(3), mode=\"same\")\n",
    "    asym_pk_arg, *_ = find_peaks(\n",
    "        np.abs(rot_conv), height=0.9, distance=4, prominence=0.2\n",
    "    )\n",
    "    asym_min_arg = np.argwhere(asym_min).squeeze()\n",
    "    max_min_arg = np.argwhere(max_min).squeeze()\n",
    "    # min_max_min = max_min_arg - 2\n",
    "    # max_max_min = max_min_arg + 2\n",
    "    # min_cond = np.any(\n",
    "    #     np.logical_and(\n",
    "    #         asym_min_arg[:, None] >= min_max_min[None, :],\n",
    "    #         asym_min_arg[:, None] <= max_max_min[None, :],\n",
    "    #     ),\n",
    "    #     axis=1,\n",
    "    # )\n",
    "    # rem_args = asym_min_arg[min_cond] + 1\n",
    "    rem_args = asym_pk_arg\n",
    "    arg_diffs = rem_args[1:] - rem_args[:-1]\n",
    "    mid_heights = max_trace[np.around(0.5 * (rem_args[1:] + rem_args[:-1])).astype(int)]\n",
    "\n",
    "    # used for calculations inside\n",
    "    nfold = np.sqrt(np.square(normed))\n",
    "    nsq = np.abs(normed) * normed\n",
    "    rotated = np.zeros_like(normed, dtype=np.complex128)\n",
    "\n",
    "    # now we need to segment where the signal goes outside of the range (-3.0, 3.0)\n",
    "    past_3 = np.argwhere(np.abs(normed) > 3.0).squeeze()\n",
    "    num_outliers = len(past_3)\n",
    "    outliers = np.zeros(num_outliers + 2, dtype=int)\n",
    "    outliers[1:-1] = past_3\n",
    "    outliers[-1] = len(normed) - 1\n",
    "    for idx in range(num_outliers + 1):\n",
    "        start = outliers[idx]\n",
    "        stop = outliers[idx + 1]\n",
    "        if start < 109805 < stop:\n",
    "            print(\"stop here\")\n",
    "        inside_cond = np.logical_and(rem_args > start, rem_args < stop)\n",
    "        inside_args = np.argwhere(inside_cond).squeeze()\n",
    "        num_mins = np.count_nonzero(inside_cond)\n",
    "        if num_mins >= 3:\n",
    "            # this is where we can start to feel pretty confident.\n",
    "            # But we'll still rotate each peak based on the average within the peak.\n",
    "            # outside the peak, I guess just rotate based on the closest peak?\n",
    "            first_min = np.min(inside_args)\n",
    "            last_min = np.max(inside_args - (1 if num_mins % 2 == 0 else 0))\n",
    "            diffs = arg_diffs[first_min:last_min]\n",
    "            hghts = mid_heights[first_min:last_min]\n",
    "            if num_mins > 4:\n",
    "                diff_scores = (\n",
    "                    (np.mean(diffs) - diffs) / np.std(diffs).clip(1e-5)\n",
    "                ).clip(-3.0, 3.0)\n",
    "                height_scores = (\n",
    "                    (hghts - np.mean(hghts)) / np.std(hghts).clip(1e-5)\n",
    "                ).clip(-3.0, 3.0)\n",
    "                parity = (\n",
    "                    np.mean(diff_scores[::2] + height_scores[::2])\n",
    "                    - np.mean(diff_scores[1::2] + height_scores[1::2])\n",
    "                ) < 0  # zero if even are the peaks, else odd are\n",
    "            else:\n",
    "                seg = normed[rem_args[first_min] : rem_args[last_min]]\n",
    "                frac_re = np.sum(np.square(np.real(seg) / np.abs(seg)))\n",
    "                if frac_re > 0.5:\n",
    "                    nseg = np.real(seg) - np.mean(np.real(seg))\n",
    "                    # frac_under = np.count_nonzero(\n",
    "                    #     (np.real(seg) < nseg\n",
    "                    # )) / len(seg)\n",
    "                else:\n",
    "                    nseg = np.imag(seg) - np.mean(np.imag(seg))\n",
    "                    # frac_under = np.count_nonzero(\n",
    "                    #     (np.imag(seg) < np.mean(np.imag(seg)))\n",
    "                    # ) / len(seg)\n",
    "                parity = (\n",
    "                    np.mean(np.abs(nseg[: diffs[0]]))\n",
    "                    - np.mean(np.abs(nseg[diffs[0] :]))\n",
    "                    > 0\n",
    "                )\n",
    "            # next, average across each peak.\n",
    "            rot_angles = np.zeros((num_mins - 1) // 2, dtype=np.complex128)\n",
    "            for jdx in range((num_mins - 1) // 2):\n",
    "                first = rem_args[inside_args[jdx * 2]]\n",
    "                mid = rem_args[inside_args[jdx * 2 + 1]]\n",
    "                last = rem_args[inside_args[jdx * 2 + 2]]\n",
    "                parity_av = np.mean(normed[first:mid]) - np.mean(normed[mid:last])\n",
    "                parity_av /= np.abs(parity_av)\n",
    "                if parity:\n",
    "                    parity_av *= -1.0\n",
    "                # first_pk = rem_args[inside_args[jdx * 2 + parity]]\n",
    "                # last_pk = rem_args[inside_args[jdx * 2 + 1 + parity]]\n",
    "                # wav_av = np.mean(nfold[first:last])\n",
    "                # peak_av = np.mean(normed[first_pk:last_pk])\n",
    "                rot_angles[jdx] = np.conj(parity_av)\n",
    "                # rot_angles[jdx] = np.conj(\n",
    "                #     wav_av * np.sign(np.real(wav_av / peak_av)) / np.abs(wav_av)\n",
    "                # )\n",
    "                rotated[first:last] = normed[first:last] * rot_angles[jdx]\n",
    "                if first < 109805 < last:\n",
    "                    print(first, last, rot_angles[jdx], wav_av, parity_av)\n",
    "            # finally, do the parts on either side of the peaks...\n",
    "            rotated[start : rem_args[first_min]] = (\n",
    "                normed[start : rem_args[first_min]] * rot_angles[0]\n",
    "            )\n",
    "            rotated[rem_args[last_min] : stop] = (\n",
    "                normed[rem_args[last_min] : stop] * rot_angles[-1]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # hard to tell what's going on really. We have one peak or less.\n",
    "            # get the distance-weighted average, and rotate by that.\n",
    "            wav_av = np.mean(nfold[start:stop])\n",
    "            nsq_av = np.mean(nsq[start:stop])\n",
    "            rot_angle = np.conj(\n",
    "                wav_av * np.sign(np.real(wav_av / nsq_av)) / np.abs(wav_av)\n",
    "            )\n",
    "            diff = rotated[start - 1] - normed[start] * rot_angle\n",
    "            ndiff = rotated[start - 1] + normed[start] * rot_angle\n",
    "            result = np.sign(np.abs(ndiff) - np.abs(diff))\n",
    "            rotated[start:stop] = normed[start:stop] * rot_angle * result\n",
    "\n",
    "    _, (ax1, ax2) = plt.subplots(2, 1, sharex=\"col\")\n",
    "    ax1.imshow(cols, aspect=\"auto\")\n",
    "    ax2.plot(np.real(asym_mean))\n",
    "    ax2.plot(np.abs(rot_conv))\n",
    "    ax2.plot(asym_trace)\n",
    "    ax2.plot(np.real(normed))\n",
    "    ax2.plot(np.imag(normed))\n",
    "    ax2.plot(real_norm, linestyle=\":\")\n",
    "    ax2.plot(imag_norm, linestyle=\":\")\n",
    "    ax2.plot(ratio, linestyle=\"--\")\n",
    "    ax2.scatter(asym_pk_arg, -4.5 * np.ones_like(asym_pk_arg))\n",
    "    ax2.plot(np.real(rotated))\n",
    "    # ax2.legend(\n",
    "    #     [\n",
    "    #         \"asymmean\",\n",
    "    #         \"rotconv\",\n",
    "    #         \"asymtrace\",\n",
    "    #         \"re(normed)\",\n",
    "    #         \"im(normed)\",\n",
    "    #         \"realnorm\",\n",
    "    #         \"imnorm\",\n",
    "    #         \"ratio\",\n",
    "    #         \"rotated\",\n",
    "    #         \"crossing\",\n",
    "    #     ]\n",
    "    # )\n",
    "    plt.show()\n",
    "\n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(real_norm)\n",
    "    # ax.plot(imag_norm)\n",
    "    # ax.plot(np.log2(ratio))\n",
    "    # plt.show()\n",
    "    return (real_norm * ratio, imag_norm / ratio, ratio, cwt_real, cwt_imag)\n",
    "\n",
    "\n",
    "def flip_signal(signal):\n",
    "    smoothed, normed = smooth_and_norm_real(signal, smoothing=20.0)\n",
    "\n",
    "    r2, *_ = compute_r2(normed.clip(-3.0, 3.0), smoothed, winsize=255)\n",
    "\n",
    "    curv = get_findiff_curvature(smoothed)\n",
    "\n",
    "    bmp = 21 / 60  # max breaths per second\n",
    "    fs = 5  # sample rate\n",
    "    max_curv_exag = (2 * np.pi * bmp / fs) ** 4\n",
    "\n",
    "    curv_exag = (curv * np.abs(curv)) / max_curv_exag\n",
    "\n",
    "    curv_exag_sm = correlate(curv_exag, np.ones(1001) / 10.0, mode=\"same\")\n",
    "\n",
    "    past_3 = np.argwhere(np.abs(normed) > 5.0).squeeze()\n",
    "    num_outliers = len(past_3)\n",
    "    outliers = np.zeros(num_outliers + 2, dtype=int)\n",
    "    outliers[1:-1] = past_3\n",
    "    outliers[-1] = len(signal) - 1\n",
    "\n",
    "    righted = np.zeros_like(signal)\n",
    "    stdev = 20.0\n",
    "    windows = np.zeros_like(signal)\n",
    "\n",
    "    for idx in range(num_outliers + 1):\n",
    "        start = outliers[idx]\n",
    "        stop = outliers[idx + 1]\n",
    "\n",
    "        win_range = np.minimum(np.arange(stop - start), np.arange(stop - start)[::-1])\n",
    "        window = 1.0 - np.exp(-0.5 * np.square(win_range.clip(max=3 * stdev) / stdev))\n",
    "        windows[start:stop] = window\n",
    "\n",
    "        right = np.sum(curv_exag[start:stop] * window) <= 0.0\n",
    "\n",
    "        if right:\n",
    "            righted[start:stop] = normed[start:stop]\n",
    "        else:\n",
    "            righted[start:stop] = -normed[start:stop]\n",
    "\n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(normed)\n",
    "    # ax.plot(smoothed)\n",
    "    # ax.plot(windows)\n",
    "    # ax.plot(righted, linestyle=\"--\")\n",
    "    # ax.plot(curv_exag)\n",
    "    # ax.plot(curv_exag_sm)\n",
    "    # ax.plot(r2)\n",
    "    # ax.legend([\"norm\", \"sm\", \"win\", \"right\", \"curv\", \"curvsm\", \"r2\"])\n",
    "    # plt.show()\n",
    "\n",
    "    return smoothed, normed, r2, righted, curv_exag\n",
    "\n",
    "\n",
    "def flip_components_indiv_and_combine(sig_x, sig_y, segments):\n",
    "    \"\"\"Normalizes the traces, then compares them and weights the wavier one higher.\"\"\"\n",
    "    sig_x_copy = np.copy(sig_x)\n",
    "    sig_y_copy = np.copy(sig_y)\n",
    "\n",
    "    for sdx, seg in enumerate(segments[:-1]):\n",
    "        scale_x = flip_signal(sig_x[seg:segments[sdx + 1]])\n",
    "        scale_y = flip_signal(sig_y[seg:segments[sdx + 1]])\n",
    "        sig_x_copy[seg:segments[sdx + 1]] *= scale_x\n",
    "        sig_y_copy[seg:segments[sdx + 1]] *= scale_y\n",
    "\n",
    "    omega = 20.0\n",
    "    fs = 5.0\n",
    "    freqs = np.logspace(0.1, -1.4, 150)  # ~50-85 are breathing frequencies\n",
    "    widths_morlet = omega * fs / (freqs[55:80] * 2 * np.pi)\n",
    "    real_wave = wavefinding_cwt(real_righted.clip(-3.0, 3.0), widths_morlet, omega)\n",
    "    mags_real = np.sum(np.square(np.abs(real_wave)), axis=0)\n",
    "    imag_wave = wavefinding_cwt(imag_righted.clip(-3.0, 3.0), widths_morlet, omega)\n",
    "    mags_imag = np.sum(np.square(np.abs(imag_wave)), axis=0)\n",
    "\n",
    "    rr_align = np.zeros_like(real_righted)\n",
    "    ir_align = np.zeros_like(imag_righted)\n",
    "\n",
    "    past_3 = np.argwhere(\n",
    "        np.logical_or(np.abs(real_norm) > 5.0, np.abs(imag_norm) > 5.0)\n",
    "    ).squeeze()\n",
    "    num_outliers = len(past_3)\n",
    "    outliers = np.zeros(num_outliers + 2, dtype=int)\n",
    "    outliers[1:-1] = past_3\n",
    "    outliers[-1] = len(real_righted) - 1\n",
    "\n",
    "    stdev = 20.0\n",
    "\n",
    "    for idx in range(num_outliers + 1):\n",
    "        start = outliers[idx]\n",
    "        stop = outliers[idx + 1]\n",
    "\n",
    "        if start < 97250 < stop:\n",
    "            print(\"shit\")\n",
    "\n",
    "        win_range = np.minimum(np.arange(stop - start), np.arange(stop - start)[::-1])\n",
    "        window = 1.0 - np.exp(-0.5 * np.square(win_range.clip(max=3 * stdev) / stdev))\n",
    "\n",
    "        agree = (\n",
    "            np.sum(real_righted[start:stop] * imag_righted[start:stop] * window) >= 0.0\n",
    "        )\n",
    "\n",
    "        if agree:\n",
    "            rr_align[start:stop] = real_righted[start:stop]\n",
    "            ir_align[start:stop] = imag_righted[start:stop]\n",
    "        else:\n",
    "            # who has the higher sum of curvatures on their side?\n",
    "            real_flip = np.sign(\n",
    "                np.abs(np.sum(real_curv[start:stop]))\n",
    "                - np.abs(np.sum(imag_curv[start:stop]))\n",
    "            )\n",
    "            rr_align[start:stop] = real_righted[start:stop] * real_flip\n",
    "            ir_align[start:stop] = imag_righted[start:stop] * -real_flip\n",
    "\n",
    "    real_frac = mags_real / (mags_real + mags_imag)\n",
    "    imag_frac = 1.0 - real_frac\n",
    "\n",
    "    result = real_frac * real_righted + imag_frac * imag_righted\n",
    "\n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(real_righted)\n",
    "    # # ax.plot(real_smooth)\n",
    "    # ax.plot(imag_righted)\n",
    "    # ax.plot(result)\n",
    "    # # ax.plot(imag_smooth)\n",
    "    # # ax.plot(result)\n",
    "    # # ax.plot(real_nm_curv_conv + 3.0, linestyle=\"--\")\n",
    "    # # ax.plot(imag_nm_curv_conv + 3.0, linestyle=\"--\")\n",
    "    # # ax.plot(real_r2 - 3.0)\n",
    "    # # ax.plot(imag_r2 - 3.0, linestyle=\"--\")\n",
    "    # # ax.plot(mags_real * 0.005)\n",
    "    # # ax.plot(mags_imag * 0.005)\n",
    "    # ax.plot(real_frac)\n",
    "    # ax.plot(imag_frac)\n",
    "    # ax.legend([\"rr\", \"ir\", \"res\", \"rf\", \"if\"])\n",
    "    # plt.show()\n",
    "\n",
    "    return result\n",
    "\n",
    "def adjust_for_zero_trend(signal, lam=25.0):\n",
    "    new_sig = np.copy(signal)\n",
    "    sm = cspline1d(signal, lam)\n",
    "    segs = np.argwhere(np.logical_and(sm[1:] * sm[:-1] < 0.0, sm[1:] < sm[:-1])).squeeze()\n",
    "    segs[0] = 0\n",
    "    for idx in range(len(segs) - 1):\n",
    "        start = segs[idx]\n",
    "        stop = segs[idx + 1]\n",
    "        seg_offset = np.mean(signal[start:stop])\n",
    "        new_sig[start:stop] -= seg_offset\n",
    "        bnd_offset = np.mean(np.cumsum(new_sig[start:stop]))\n",
    "        new_sig[start] -= 0.5 * bnd_offset\n",
    "        new_sig[stop] += 0.5 * bnd_offset\n",
    "    \n",
    "    return new_sig, sm\n",
    "\n",
    "def estimate_oob(signal_x, signal_y, r2_x, r2_y, r2_med_x, r2_med_y, ab_x, ab_y, pos_x, pos_y):\n",
    "    \n",
    "    sm = 81\n",
    "    sm_pos_x = medfilt(pos_x, sm)\n",
    "    sm_pos_y = medfilt(pos_y, sm)\n",
    "\n",
    "    dx = np.abs(pos_x - sm_pos_x)\n",
    "    dy = np.abs(pos_y - sm_pos_y)\n",
    "\n",
    "    med_dx = np.median(dx)\n",
    "    med_dy = np.median(dy)\n",
    "    x_cond = dx > 12.0 * med_dx\n",
    "    y_cond = dy > 12.0 * med_dy\n",
    "    dx_extr = np.where(x_cond, 10.0, 0.0)\n",
    "    dy_extr = np.where(y_cond, 8.0, 0.0)\n",
    "\n",
    "    exp_k = 4\n",
    "    either = np.logical_or(x_cond, y_cond).astype(float)\n",
    "    expand = np.concatenate((np.full(exp_k, either[0]), either, np.full(exp_k, either[-1])))\n",
    "    exp_cs = np.cumsum(expand)\n",
    "    mov = exp_cs[2 * exp_k:] - exp_cs[:-2 * exp_k] > 0.5\n",
    "    mov_mult = 1.0 + 9.0 * medfilt(mov.astype(float), 2 * exp_k + 1)\n",
    "    \n",
    "    best_r2 = np.maximum(r2_x, r2_y).clip(0.05, 0.99)\n",
    "    best_med = np.maximum(r2_med_x, r2_med_y).clip(0.05, 0.99)\n",
    "    sqmag = np.sum(np.square(ab_x), axis=1) + np.sum(np.square(ab_y), axis=1).clip(1e-12)\n",
    "    log_mag = np.log(sqmag)\n",
    "    avg_log_mag = np.mean(log_mag[best_r2 >= 0.8])\n",
    "    std_log_mag = np.std(log_mag[best_r2 >= 0.8])\n",
    "    \n",
    "    oob_buf = 101\n",
    "    mov_buf = 7\n",
    "    \n",
    "    # rate non-periodic by magnitude, and by med_r^2\n",
    "    # if it's low r^2 and low magnitude, it's probably out of bed\n",
    "    # odds that it's nonperiodic, where 0.8 = 1:1 odds\n",
    "    # z-score for magnitudes, and odds that the signal is small\n",
    "    log_mag_z = (avg_log_mag - log_mag) / std_log_mag\n",
    "    med_zed = medfilt(log_mag_z, 501).clip(-1.6, 1.6)\n",
    "    diff_z = med_zed - log_mag_z\n",
    "    std_diff = np.std(diff_z[best_r2 >= 0.8])\n",
    "    odds_spike = (0.667 * diff_z / std_diff - 1.0).clip(0.0) * 4.0 + np.where(np.logical_or(diff_z > 0.0, mov), 0.1, 0.0)\n",
    "    # odds_spike = (0.22 * diff_z / std_diff).clip(0.333) * 3.0 \n",
    "    odds_small = (log_mag_z - 1.0).clip(0.0)\n",
    "    \n",
    "    odds_nonper_inst = 16.0 * np.square(1.0 - best_r2)\n",
    "    odds_large = 1.0 #np.square((-1.0 - log_mag_z).clip(0.01))\n",
    "    odds_mov = odds_nonper_inst * odds_large * mov_mult * odds_spike\n",
    "    p_mov = odds_mov / (odds_mov + 1.0)\n",
    "    is_mov = medfilt((p_mov > 0.5).astype(float), mov_buf)\n",
    "    \n",
    "    # odds_non_per = 25.0 * np.square(1.0 - best_med)\n",
    "    # inst_nonper_odds = 25.0 * np.square(np.square(1.0 - best_r2.clip(min=0.8)))\n",
    "    # odds_non_per *= 625.0 * np.square(np.square(1.0 - best_r2.clip(min=0.8)))\n",
    "    odds_nonper_med = 25.0 * np.square(1.0 - best_med)\n",
    "    odds_nonper = odds_nonper_inst  # np.where(low_overflow > 0.0, odds_nonper_inst, odds_nonper_med)\n",
    "    odds_ratio = odds_nonper * odds_small\n",
    "    \n",
    "    p_oob = medfilt(odds_ratio / (odds_ratio + 1.0), oob_buf)\n",
    "    # let's plot that shit to see where we're at\n",
    "    _, ax = plt.subplots(1, 1)\n",
    "    ax.plot(odds_nonper.clip(0.0, 20.0), label=\"nonper odds\", alpha=0.3, linestyle=\":\")\n",
    "    # ax.plot(odds_small, label=\"small odds\", alpha=0.3, linestyle=\":\")\n",
    "    ax.plot(best_r2, label=\"inst\", alpha=0.3, linestyle=\":\")\n",
    "    ax.plot(best_med, label=\"med\", alpha=0.3, linestyle=\":\")\n",
    "    ax.plot(is_mov, label=\"med\", alpha=0.3, linestyle=\":\")\n",
    "    ax.plot(log_mag_z, label=\"zscore\", alpha=0.6, linestyle=(0, (3, 1, 1, 1)))\n",
    "    # ax.plot(odds_spike, label=\"spike\", alpha=0.6, linestyle=(0, (2, 1, 1, 1)))\n",
    "    # ax.plot(r2_x, label=\"r2x\")\n",
    "    # ax.plot(r2_y, label=\"r2y\")\n",
    "    ax.plot(1e3 * signal_x, alpha=0.2, linestyle=\":\")\n",
    "    ax.plot(1e3 * signal_y, alpha=0.2, linestyle=\":\")\n",
    "    # ax.plot(odds_ratio, alpha=0.5, linestyle=\":\", label=\"OR\")\n",
    "    ax.plot(p_oob, label=\"poob\")\n",
    "    ax.plot(p_mov, label=\"pmov\", linestyle=(0, (3,1,1,1,1,1)))\n",
    "    # ax.plot(mov)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    mov = p_mov > 0.8\n",
    "    meb_mov = p_mov > 0.55\n",
    "    mov[0] = True\n",
    "    mov[-1] = True\n",
    "    starts = np.argwhere(np.logical_and(np.logical_not(mov[:-1]), mov[1:])).squeeze()\n",
    "    stops = np.argwhere(np.logical_and(np.logical_not(mov[1:]), mov[:-1])).squeeze()\n",
    "    n_segs = starts.size\n",
    "    for idx in range(n_segs - 1):\n",
    "        seg = np.arange(stops[idx], starts[idx])\n",
    "        mov[seg] = np.all(meb_mov[seg])\n",
    "    return mov\n",
    "    \n",
    "\n",
    "def find_discont(signal_x, signal_y, r2_x, r2_y, pos_x, pos_y, ab_x, ab_y, min_compare=37, max_compare=250):\n",
    "    best_r2 = np.maximum(r2_x, r2_y)\n",
    "    sqmag = np.sum(np.square(ab_x), axis=1) + np.sum(np.square(ab_y), axis=1).clip(1e-12)\n",
    "    mag = np.sqrt(sqmag)\n",
    "\n",
    "    # for each non periodic segment, check the probability that the position did not change\n",
    "    # guarantees: per_begin[0] < per_end[0] and per_end[-1] > per_begin[-1] and lengths are the same\n",
    "    thresh = 0.75\n",
    "    per_begin = np.argwhere(np.logical_and(best_r2[:-1] < thresh, best_r2[1:] >= thresh)).squeeze()\n",
    "    per_end = np.argwhere(np.logical_and(best_r2[:-1] >= thresh, best_r2[1:] < thresh)).squeeze()\n",
    "    n_per = per_begin.size\n",
    "\n",
    "    keep_begin = np.ones(n_per, dtype=bool)\n",
    "    keep_end = np.ones(n_per, dtype=bool)\n",
    "    mov_odds = np.ones(n_per + 1)\n",
    "    rms_odds = np.ones(n_per + 1)\n",
    "    shr_odds = np.ones(n_per + 1)\n",
    "    npr_odds = np.ones(n_per + 1)\n",
    "    \n",
    "    # first trim the nonper that are too short (<=1.2s), and likely not movement or sighing\n",
    "    # larger than the surrounding signal? Probably want to test it.\n",
    "    # r^2 value still pretty high throughout? likely want to merge it\n",
    "    for idx in range(n_per - 1):\n",
    "        # get the lengths of the segments\n",
    "        lg_dur = per_begin[idx + 1] - per_end[idx]\n",
    "        short_or = np.log2(lg_dur / 8)\n",
    "        # more sensitive to lower r^2 values\n",
    "        rms_r2 = 1.0 - np.sqrt(np.mean(np.square(1.0 - best_r2[per_end[idx]:per_begin[idx + 1]])))\n",
    "        nonper_or = np.exp(-8 * (rms_r2 - 0.65))\n",
    "        rms_bef = np.sqrt(np.mean(sqmag[per_begin[idx]:per_end[idx]]))\n",
    "        rms_dur = np.sqrt(np.mean(sqmag[per_end[idx]:per_begin[idx + 1]]))\n",
    "        rms_aft = np.sqrt(np.mean(sqmag[per_begin[idx + 1]:per_end[idx + 1]]))\n",
    "        rms_ratio = rms_dur / max(rms_bef, rms_aft) - 1.0\n",
    "        # if significantly larger than the other bits of the signal, quite likely that it should be kept\n",
    "        rms_or = 0.275 / np.square(max(-1.0, min(0.5, rms_ratio - 0.7)))\n",
    "        keep_odds = short_or * nonper_or * rms_or\n",
    "        keep_begin[idx + 1] = keep_odds > 1\n",
    "        keep_end[idx] = keep_odds > 1\n",
    "        mov_odds[idx + 1] = keep_odds\n",
    "        rms_odds[idx + 1] = rms_or\n",
    "        shr_odds[idx + 1] = short_or\n",
    "        npr_odds[idx + 1] = nonper_or\n",
    "        \n",
    "\n",
    "    merged_begin = per_begin[keep_begin]\n",
    "    merged_end = per_end[keep_end]\n",
    "    merged_mov = mov_odds[1:][keep_end]\n",
    "    merged_rms = rms_odds[1:][keep_end]\n",
    "    merged_shr = shr_odds[1:][keep_end]\n",
    "    merged_npr = npr_odds[1:][keep_end]\n",
    "    n_merged = merged_begin.size\n",
    "    \n",
    "    print(n_per, n_merged)\n",
    "    print(merged_end - merged_begin)\n",
    "    \n",
    "    # next up, pruning the periodic parts that are too short.\n",
    "    keep_merged = merged_end - merged_begin >= min_compare\n",
    "    final_begin = merged_begin[keep_merged]\n",
    "    final_end = merged_end[keep_merged]\n",
    "    n_final = final_begin.size\n",
    "    p_mov = np.ones(n_final - 1)\n",
    "    \n",
    "    # use ks_test to find the likely discontinuities\n",
    "    for idx in range(n_final - 1):\n",
    "        bef_lg = final_end[idx] - final_begin[idx]\n",
    "        aft_lg = final_end[idx + 1] - final_begin[idx + 1]\n",
    "        bef_lg = min(max_compare, 2 * aft_lg, bef_lg)\n",
    "        aft_lg = min(max_compare, 2 * bef_lg, aft_lg)\n",
    "        bef_coords = np.arange(final_end[idx] - bef_lg, final_end[idx])\n",
    "        aft_coords = np.arange(final_begin[idx + 1], final_begin[idx + 1] + aft_lg)\n",
    "        mn = round((np.sum(bef_coords) + np.sum(aft_coords)) / (bef_lg + aft_lg))\n",
    "        print(idx, final_end[idx])\n",
    "        \n",
    "        T_x, p_x = split_test(pos_x[bef_coords], pos_x[aft_coords], bef_coords, aft_coords, idx==85)\n",
    "        T_y, p_y = split_test(pos_y[bef_coords], pos_y[aft_coords], bef_coords, aft_coords, idx==85)\n",
    "        p_mov[idx] = max(T_x, T_y)\n",
    "        \n",
    "        \n",
    "    # page 31 of this pdf is very important and helpful:\n",
    "    # https://scholar.princeton.edu/sites/default/files/bstewart/files/lecture7_handout_2018.pdf\n",
    "    # the sigma_squared_estimator * inv(XT X) gives the covariance matrix\n",
    "    # T statistic to test if hypothesis that discontinuity doesn't exist (split param is 0)\n",
    "    # is given by split/SE(split), where SE(split) = sqrt(Variance(split)) = sqrt(Cov(split,split))\n",
    "    # page 32 and 24 then show how to use the t distribution. When estimating k+1 parameters, use\n",
    "    # the t distribution t_(n - (k + 1)) what is the sigma_squared_estimator?\n",
    "    # (uT u) / (n - (k + 1)) where u are the errors i.e. the sample average sum of square errors,\n",
    "    # ie population mean square error\n",
    "    \n",
    "    _, (ax1,ax2) = plt.subplots(2, 1, sharex=True)\n",
    "    ax1.plot(signal_x * 1e3, alpha=0.3, linestyle=\":\")\n",
    "    ax1.plot(signal_y * 1e3, alpha=0.3, linestyle=\":\")\n",
    "    ax1.plot(best_r2, alpha=0.3, linestyle=\":\")\n",
    "    ax1.plot(sqmag * 1e6, alpha=0.3, linestyle=(0, (1, 3)))\n",
    "    \n",
    "    ax2.plot(pos_x)#medfilt(pos_x, 101))\n",
    "    ax2.plot(pos_y)#medfilt(pos_y, 101))\n",
    "    \n",
    "    for idx in range(n_final):\n",
    "        ax1.axvspan(final_begin[idx], final_end[idx], color=\"g\", alpha=0.2)\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        ax1.plot(np.array([final_end[idx - 1], final_begin[idx]]), np.ones(2) * p_mov[idx - 1], linestyle=(0, ()))\n",
    "    #     ax.plot(np.array([merged_end[idx - 1], merged_begin[idx]]), np.ones(2) * min(10.0, merged_rms[idx - 1]), linestyle=(0, (1, 1)))\n",
    "    #     ax.plot(np.array([merged_end[idx - 1], merged_begin[idx]]), np.ones(2) * min(10.0, merged_shr[idx - 1]), linestyle=(0, (3, 1)))\n",
    "    #     ax.plot(np.array([merged_end[idx - 1], merged_begin[idx]]), np.ones(2) * min(10.0, merged_npr[idx - 1]), linestyle=(0, (5, 1, 1, 1)))\n",
    "    # ax.legend([\"sigx\", \"sigy\", \"r2\", \"mag\", \"mov\", \"rms\", \"shr\", \"npr\"])\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "    keep_start[1:] = mov_or_sigh > 0.5\n",
    "    keep_stop[:-1] = mov_or_sigh > 0.5\n",
    "    contig_starts = starts[keep_start]\n",
    "    contig_stops = stops[keep_stop]\n",
    "    discont = np.zeros(contig_starts.size - 1)\n",
    "\n",
    "    for idx in range(contig_starts.size - 1):\n",
    "        # get the lengths of the segments\n",
    "        lg_bef = contig_stops[idx] - contig_starts[idx]\n",
    "        lg_dur = contig_starts[idx + 1] - contig_stops[idx]\n",
    "        lg_aft = contig_stops[idx + 1] - contig_starts[idx + 1]\n",
    "        # segments should not differ in length by TOO much, or else the KS test will fail\n",
    "        use_lg = 2 * min(250, lg_bef, lg_aft)\n",
    "        bef_b = contig_stops[idx] - min(use_lg, lg_bef)\n",
    "        aft_e = contig_starts[idx + 1] + min(use_lg, lg_aft)\n",
    "        bef_t = np.arange(bef_b, contig_stops[idx], dtype=float)\n",
    "        aft_t = np.arange(contig_starts[idx + 1], aft_e, dtype=float)\n",
    "        p_x = ks_test(n[bef_b:contig_stops[idx], 2], n[contig_starts[idx + 1]:aft_e, 2], bef_t, aft_t)\n",
    "        p_y = ks_test(n[bef_b:contig_stops[idx], 3], n[contig_starts[idx + 1]:aft_e, 3], bef_t, aft_t)\n",
    "        discont[idx] = 1.0 - p_x * p_y\n",
    "\n",
    "\n",
    "def fit_piecewise_contin(y_bef, t_bef, y_aft, t_aft, sig, prt=False):\n",
    "    ctr = 0.5 * (t_bef[-1] + t_aft[0])\n",
    "    rad = t_aft[0] - ctr\n",
    "    xs = np.concatenate((t_bef, t_aft)) - ctr\n",
    "    ys = np.concatenate((y_bef, y_aft))\n",
    "    ys -= np.mean(ys)\n",
    "    lg = xs.size\n",
    "    szb = y_bef.size\n",
    "    sza = y_aft.size\n",
    "    X = np.stack(\n",
    "        (\n",
    "            np.minimum(xs + rad, 0.0),\n",
    "            np.maximum(xs - rad, 0.0),\n",
    "            np.minimum(rad, np.maximum(xs, -rad)),\n",
    "        ), axis=-1)\n",
    "    X -= np.mean(X, axis=0)\n",
    "    cov = linalg.inv(np.dot(X.T, X)) # covariance of X, sorta\n",
    "    pinv = linalg.pinv(X)\n",
    "    coeffs = np.dot(pinv, ys)\n",
    "    disc_var = sig * np.sqrt(cov[2, 2])\n",
    "    s3 = np.sqrt(1/3)\n",
    "    # tf = np.array([[s3, s3, s3],[-1, 0, 1],[0, -1, 1]])\n",
    "    tf = np.array([[1.0, 0, -1], [1, 1, 0], [0, -1, 1]]) # middle one is arbitrary... suspiciously like the other one xD\n",
    "    Xtf = np.dot(X, np.linalg.inv(tf))\n",
    "    covtf = np.linalg.inv(np.dot(Xtf.T, Xtf))\n",
    "    pinvtf = linalg.pinv(Xtf)\n",
    "    coeffstf = np.dot(pinvtf, ys)\n",
    "    if prt:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "        ax.plot(xs, X)\n",
    "        ax.plot(xs, ys)\n",
    "        ax.plot(xs, np.dot(X, coeffs))\n",
    "        ax.plot(xs, np.dot(X, tf.T), linestyle=\":\")\n",
    "        # ax.plot(xs, shit / mbef)\n",
    "        # u,s,v = linalg.svd(X.T, full_matrices=False)\n",
    "        # ax.plot(xs, v.T)\n",
    "        plt.show()\n",
    "        print(cov)\n",
    "        print(covtf)\n",
    "        print(coeffs, np.dot(tf, coeffs), coeffstf)\n",
    "        print(np.dot(tf, sig**2 * cov))\n",
    "    # how is that derived? let slopes of before, after and middle be x, y, z respectively\n",
    "    # start off with the condition z + const > max(x, y) OR z - const < min(x, y)\n",
    "    # we need to rotate this space so that it can be integrated with scipy's multivariate normal distribution\n",
    "    # so first, rotate about the z axis by 1/4 turn so that x=y lies along x axis\n",
    "    # next rotate about y axis by 35 degrees (asin(sqrt(1/3)) so that z=x=y lies along x axis\n",
    "    # stretch along the z axis by sqrt(3) so that the dihedral angle becomes a right angle\n",
    "    # finally rotate about x axis so that the appropriate boundaries are parallel to xy nd xz planes\n",
    "    # now you can just integrate both regions, as they are rectangular and have appropriate boundaries\n",
    "\n",
    "    # cond_1 = mvnd.cdf(np.array([np.inf, -disc_var, -disc_var]), mean=np.dot(tf, coeffs), cov=np.dot(tf, sig**2 * cov))\n",
    "    # cond_2 = mvnd.cdf(np.array([np.inf, -disc_var, -disc_var]), mean=np.dot(tf, -coeffs), cov=-np.dot(tf, sig**2 * cov))\n",
    "    # cond_1 = mvnd.cdf(np.array([np.inf, -disc_var, -disc_var]), mean=coeffstf, cov=sig**2 * covtf)\n",
    "    # cond_2 = mvnd.cdf(np.array([np.inf, -disc_var, -disc_var]), mean=-coeffstf, cov=sig**2 * covtf)\n",
    "    cond_1 = mvnd.cdf(np.array([0.25 * sig / rad, np.inf, -0.25 * sig / rad]), mean=coeffstf, cov=sig**2 * covtf)\n",
    "    cond_2 = mvnd.cdf(np.array([0.25 * sig / rad, np.inf, -0.25 * sig / rad]), mean=-coeffstf, cov=sig**2 * covtf)\n",
    "    cond_3 = mvnd.cdf(np.array([np.inf, np.inf, 0.25 * sig / rad]), mean=coeffstf, cov=sig**2 * covtf)\n",
    "    cond_4 = mvnd.cdf(np.array([np.inf, np.inf, -0.25 * sig / rad]), mean=coeffstf, cov=sig**2 * covtf)\n",
    "    if prt:\n",
    "        print(cond_1, cond_2, cond_3, cond_4, 0.25 * sig / rad, np.sqrt(np.diag(covtf * sig ** 2)))\n",
    "    return cond_1 + cond_2 + cond_3 - cond_4\n",
    "\n",
    "\n",
    "def find_discont_piecewise(pos_x, pos_y, mov, min_length=100, max_length=300, buf=20, sm=81):\n",
    "    px = medfilt(pos_x, sm)\n",
    "    py = medfilt(pos_y, sm)\n",
    "    starts = np.argwhere(np.logical_and(np.logical_not(mov[:-1]), mov[1:])).squeeze()\n",
    "    stops = np.argwhere(np.logical_and(np.logical_not(mov[1:]), mov[:-1])).squeeze()\n",
    "    n_seg = starts.size\n",
    "    pdx = 0\n",
    "    ndx = 1\n",
    "    shift = np.zeros((px.size, 2))\n",
    "    while ndx < n_seg:\n",
    "        bend = starts[pdx] - buf\n",
    "        astr = stops[ndx] + buf\n",
    "        bl = starts[pdx] - 2 * buf - stops[pdx]\n",
    "        al = starts[ndx] - 2 * buf - stops[ndx]\n",
    "        dl = stops[ndx] + 2 * buf - starts[pdx]\n",
    "        if al < min_length:\n",
    "            ndx += 1\n",
    "            continue\n",
    "        bef = np.arange(bend - min(max_length, bl), bend)\n",
    "        aft = np.arange(astr, astr + min(max_length, al))\n",
    "        dur = np.arange(bend, astr)\n",
    "\n",
    "        prob_x = fit_piecewise_contin(px[bef], bef, px[aft], aft, 1.2, pdx == 185)\n",
    "        prob_y = fit_piecewise_contin(py[bef], bef, py[aft], aft, 1.2, pdx == 185)\n",
    "        print(pdx, ndx, stops[pdx], stops[ndx], prob_x, prob_y)\n",
    "        shift[dur, 0] = 1.0 - prob_x\n",
    "        shift[dur, 1] = 1.0 - prob_y\n",
    "        pdx = ndx\n",
    "        ndx += 1\n",
    "    return shift\n",
    "\n",
    "    \n",
    "def find_periodic_fit_best(signal_x, signal_y, freqs, pos_x, pos_y, bx=50, by=5):\n",
    "    phase_freq_x, k_freq_x, r2_x, ab_x, r2_med_x = find_periodic(signal_x, freqs)\n",
    "    phase_freq_y, k_freq_y, r2_y, ab_y, r2_med_y = find_periodic(signal_y, freqs)\n",
    "    \n",
    "    ratio = r2_x ** 2 / (r2_x ** 2 + r2_y ** 2).clip(min=1e-8)\n",
    "    freq = ratio * k_freq_x + (1 - ratio) * k_freq_y\n",
    "    _, ax = plt.subplots(1, 1)\n",
    "    ax.plot(freq)\n",
    "    plt.show()\n",
    "    \n",
    "    mov = estimate_oob(signal_x, signal_y, r2_x, r2_y, r2_med_x, r2_med_y, ab_x, ab_y, pos_x, pos_y)\n",
    "    \n",
    "    shift = find_discont_piecewise(pos_x, pos_y, mov)\n",
    "    \n",
    "    return 1, 2, 3, 4, 5, 6, shift\n",
    "    r2 = np.square(r2_x) + np.square(r2_y)\n",
    "    N = r2_x.shape[0]\n",
    "    # Y = np.sum(r2 * np.arange(N)[:, None], axis=0)\n",
    "    # W = np.sum(r2, axis=0)\n",
    "    # x = np.arange(k) - k // 2\n",
    "    # o = np.ones(k)\n",
    "    # x2 = np.square(x)\n",
    "    # a = correlate(W, x2, \"valid\")\n",
    "    # d = correlate(W, o, \"valid\")\n",
    "    # bc = correlate(W, x, \"valid\")\n",
    "    # det = a * d - np.square(bc)\n",
    "    # xwy = correlate(Y, x, \"valid\")\n",
    "    # owy = correlate(Y, o, \"valid\")\n",
    "    # intercept = (a * owy - xwy * bc) / det\n",
    "    # plotx = np.arange(Y.size - k + 1) + k // 2\n",
    "    # allx = np.arange(Y.size)\n",
    "    \n",
    "    # essentially a gaussian blur...\n",
    "    blurx = np.exp(-0.5 * np.square((np.arange(4 * bx + 1) - 2 * bx) / bx))\n",
    "    blury = np.exp(-0.5 * np.square((np.arange(4 * by + 1) - 2 * by) / by))\n",
    "    gbd = sepfir2d(r2, blurx, blury)\n",
    "    peaks = np.logical_and(gbd[1:-1] > gbd[:-2], gbd[1:-1] > gbd[2:])\n",
    "    right = np.logical_and(peaks[:, :-1], peaks[:, 1:])\n",
    "    up = np.logical_and(peaks[1:, :-1], peaks[:-1, 1:])\n",
    "    down = np.logical_and(peaks[:-1, :-1], peaks[1:, 1:])\n",
    "    ends = np.copy(peaks)\n",
    "    ends[:, :-1] &= np.logical_not(right)\n",
    "    ends[1:, :-1] &= np.logical_not(up)\n",
    "    ends[:-1, :-1] &= np.logical_not(down)\n",
    "    end_idxs = np.argwhere(ends)\n",
    "    n_ends = end_idxs.shape[0]\n",
    "    trace_scores = np.zeros((n_ends + 1, 2))\n",
    "    r2pk = np.zeros_like(peaks, dtype=float)\n",
    "    r2pk[peaks] = r2[1:-1][peaks]\n",
    "    pkidx = np.zeros_like(peaks, dtype=int)\n",
    "    traces = [None] * n_ends\n",
    "    for idx in trange(1, n_ends + 1):\n",
    "        y, x = end_idxs[idx - 1]\n",
    "        pkidx[y, x] = idx\n",
    "        traces[idx - 1] = (x, y, [y])\n",
    "        while x > 0:\n",
    "            l = peaks[y, x - 1]\n",
    "            u = y > 0 and peaks[y - 1, x - 1]\n",
    "            d = y < N - 3 and peaks[y + 1, x - 1]\n",
    "            sr2pk = np.sum(r2pk[max(0, y - 1):min(y + 1, N - 2), x - 1])\n",
    "            if l or u or d:\n",
    "                trace_scores[idx, 0] += 1.0\n",
    "                trace_scores[idx, 1] += sr2pk\n",
    "                x -= 1\n",
    "                y += (1 if d else 0) - (1 if u else 0)\n",
    "                pkidx[y, x] = idx\n",
    "                traces[idx - 1][2].append(y)\n",
    "            else:\n",
    "                break\n",
    "    scores = trace_scores[:, 1] / np.maximum(np.sqrt(trace_scores[:, 0]), 1.0)\n",
    "    score_pic = scores[pkidx]\n",
    "    maxes = np.max(score_pic, axis=0)\n",
    "    winners = set(maxes.tolist())\n",
    "    main_trace = np.zeros_like(peaks)\n",
    "    rem_traces = []\n",
    "    for val in winners:\n",
    "        if val == 0.0:\n",
    "            continue\n",
    "        main_trace |= score_pic == val\n",
    "        trace_idx = np.nonzero(scores == val)[0][0] - 1\n",
    "        trace = traces[trace_idx][2]\n",
    "        trace_len = len(trace)\n",
    "        max_count = np.count_nonzero(maxes == val)\n",
    "        if max_count < 300 or trace_len < 300:\n",
    "            continue\n",
    "        print(val, trace_idx, trace_len)\n",
    "        trace_array = np.zeros((2, trace_len))\n",
    "        trace_array[1] = np.flip(np.array(trace))\n",
    "        trace_array[0] = np.arange(trace_len) + traces[trace_idx][0] - trace_len + 1\n",
    "        rem_traces.append(trace_array)\n",
    "    \n",
    "    r2normpk = np.max(gbd, axis=0) / np.sum(blurx) / np.sum(blury) / 2.0\n",
    "    stat = 4.12 * r2normpk\n",
    "    p_oob = 1.0 - np.square(np.square(stat)) / (np.square(np.square(stat)) + 1.0)\n",
    "    \n",
    "    r2norm = gbd / np.sum(gbd, axis=0).clip(min=1e-9)\n",
    "    mean = np.dot(np.arange(N), r2norm)\n",
    "    xmm = np.arange(N)[:, None] - mean[None, :]\n",
    "    var = np.sum(np.square(xmm) * r2norm, axis=0).clip(min=1e-9)\n",
    "    mu4 = np.sum(np.square(np.square(xmm)) * r2norm, axis=0)\n",
    "    kurt = mu4 / np.square(var)\n",
    "    \n",
    "    _, ax = plt.subplots(1, 1)\n",
    "    ax.imshow(r2norm, aspect=\"auto\")\n",
    "    for trace in rem_traces:\n",
    "        ax.plot(trace[0], trace[1] + 1.0, c=\"r\", linestyle=\":\")\n",
    "    # ax.plot(r2normpk * 100.0)\n",
    "    ax.plot(p_oob * 10)\n",
    "    ax.plot(mean)\n",
    "    ax.plot(np.sqrt(var) + 10)\n",
    "    ax.plot(kurt + 20)\n",
    "    plt.show()\n",
    "    \n",
    "    return fr2_x, fr2_y, lgs, rem_traces, r2, p_oob\n",
    "\n",
    "\n",
    "def find_periodic(signal, freqs):\n",
    "    m = 3.0\n",
    "    tl = signal.size\n",
    "    fs = 5\n",
    "    coeffs = np.zeros((freqs.size, tl, 4))\n",
    "    r2 = np.zeros((freqs.size, tl))\n",
    "    lgs = np.around(m * fs / freqs)\n",
    "    sss = np.zeros((freqs.size, tl))\n",
    "    sqsig = np.square(signal)\n",
    "    # for fdx, freq in enumerate(freqs):\n",
    "    #     lg = round(m * fs / freq)\n",
    "    #     cos = np.cos(2.0 * m * np.pi * np.arange(lg) / lg)\n",
    "    #     sin = np.sin(2.0 * m * np.pi * np.arange(lg) / lg)\n",
    "    #     dcos = np.cos(4.0 * m * np.pi * np.arange(lg) / lg)\n",
    "    #     dsin = np.sin(4.0 * m * np.pi * np.arange(lg) / lg)\n",
    "    #     a = np.correlate(signal, cos) * 2.0 / lg\n",
    "    #     b = np.correlate(signal, sin) * 2.0 / lg\n",
    "    #     c = np.correlate(signal, dcos) * 2.0 / lg\n",
    "    #     d = np.correlate(signal, dsin) * 2.0 / lg\n",
    "    #     abcd = np.stack([a, b, c, d], axis=1)\n",
    "    #     abmag = np.maximum(np.sum(np.square(abcd[:, :2]), axis=1), 1e-9)\n",
    "    #     cdmag = np.sum(np.square(abcd[:, 2:]), axis=1)\n",
    "    #     ratio = np.maximum(cdmag / abmag, 1.0)\n",
    "    #     abcd[:, 2:] /= ratio[:, None]\n",
    "    #     sss[fdx, :(tl - lg + 1)] = np.correlate(sqsig, np.ones(lg))  # sum of square signal\n",
    "    #     # can derive this expression, all terms of sse (acos(wt)+bsin(wt)... - sig)^2\n",
    "    #     # all come out to be 0 or -N/2 * the square of (a or b or c or d)\n",
    "    #     part_sse = lg * 0.5 * np.sum(np.square(abcd), axis=1)\n",
    "    #     # sse = sss - part_sse, r2 = 1 - sse / sss -> simplified it\n",
    "    #     # only works if we assume zero mean, which I do in the fit.\n",
    "    #     r2[fdx, :(tl - lg + 1)] = part_sse / sss[fdx, :(tl - lg + 1)]\n",
    "    #     coeffs[fdx, :(tl - lg + 1)] = abcd\n",
    "    # best_freqs = np.argmax(correlate1d(r2, np.ones(75), mode=\"nearest\"), axis=0)\n",
    "    \n",
    "#     m = 1.0\n",
    "#     k_0 = int(0.5 * (m * fs / freqs[0]))\n",
    "#     k_m1 = int(0.5 * (m * fs / freqs[-1]))\n",
    "#     k_freqs = (np.arange(k_0, k_m1 + 1) * 2.0 + 1.0)\n",
    "#     s_ab = np.zeros((k_freqs.size, tl, 2))\n",
    "#     s_r2 = np.zeros((k_freqs.size, tl))\n",
    "#     for kdx in range(k_m1 - k_0 + 1):\n",
    "#         k = kdx + k_0\n",
    "#         lg = 2 * k + 1\n",
    "#         rng = np.arange(lg) - k\n",
    "#         cos = np.cos(2.0 * np.pi * rng / lg)\n",
    "#         sin = np.sin(2.0 * np.pi * rng / lg)\n",
    "#         dcos = np.cos(4.0 * np.pi * rng / lg)\n",
    "#         dsin = np.sin(4.0 * np.pi * rng / lg)\n",
    "#         a = np.correlate(signal, cos, \"same\") * 2.0 / lg\n",
    "#         b = np.correlate(signal, sin, \"same\") * 2.0 / lg\n",
    "#         c = np.correlate(signal, dcos, \"same\") * 2.0 / lg\n",
    "#         d = np.correlate(signal, dsin, \"same\") * 2.0 / lg\n",
    "#         abcd = np.stack([a, b, c, d], axis=1)\n",
    "#         abmag = np.maximum(np.sum(np.square(abcd[:, :2]), axis=1), 1e-9)\n",
    "#         cdmag = np.sum(np.square(abcd[:, 2:]), axis=1)\n",
    "#         ratio = np.maximum(cdmag / abmag, 1.0)\n",
    "#         abcd[:, 2:] /= ratio[:, None]\n",
    "#         sss = np.correlate(sqsig, np.ones(lg), \"same\")  # sum of square signal\n",
    "#         # can derive this expression, all terms of sse (acos(wt)+bsin(wt)... - sig)^2\n",
    "#         # all come out to be 0 or -N/2 * the square of (a or b or c or d)\n",
    "#         part_sse = lg * 0.5 * np.sum(np.square(abcd), axis=1)\n",
    "#         # sse = sss - part_sse, r2 = 1 - sse / sss -> simplified it\n",
    "#         # only works if we assume zero mean, which I do in the fit.\n",
    "#         s_r2[kdx] = part_sse / sss\n",
    "#         s_ab[kdx] = abcd[:, :2]\n",
    "#     best_ks = np.argmax(s_r2, axis=0)[None,:,None]\n",
    "#     ab = np.take_along_axis(s_ab, best_ks, axis=0).squeeze()\n",
    "#     invert = np.zeros((*ab.shape, 2))\n",
    "#     invert[:, 0, :] = ab\n",
    "#     invert[:, 1, 0] = -ab[:, 1]\n",
    "#     invert[:, 1, 1] = ab[:, 0]\n",
    "#     invert /= linalg.norm(ab, axis=1).clip(min=1e-9)[:, None, None]\n",
    "#     rotated = np.matmul(invert[:-1], ab[1:, :, None]).squeeze()\n",
    "#     phase_diffs = np.arctan2(rotated[:, 1], rotated[:, 0])\n",
    "    \n",
    "#     _, ax = plt.subplots(1, 1)\n",
    "#     ax.plot(phase_diffs * 1e-3)\n",
    "#     ax.plot(ab, linestyle=\":\")\n",
    "#     ax.plot(signal, c=\"k\")\n",
    "#     ax.plot(np.arctan2(ab[:, 1], ab[:, 0]) * 1e-3, c=\"m\", alpha=0.1)\n",
    "#     plt.show()\n",
    "    \n",
    "    # doing this again, but with weighted least squares - 1.5 wavelengths, but weighted by exp(-0.5 * (2x/lg)^2)\n",
    "    # this effectively shrinks the wings down (and the function to fit), making errors there count for less\n",
    "    # the weight matrix is less simple, but the math is not much more complicated\n",
    "    m = 2.0\n",
    "    N = 50\n",
    "    k_0 = int(0.5 * m * fs / freqs[0])\n",
    "    k_m1 = int(0.5 * m * fs / freqs[-1])\n",
    "    k_exact = np.linspace(k_0, k_m1, N)\n",
    "    w_ab = np.zeros((N, tl, 2))\n",
    "    ssws = np.zeros((N, tl))\n",
    "    swbfms = np.zeros((N, tl))\n",
    "    sqsws = np.zeros((N, tl))\n",
    "    swdp = np.zeros((N, tl))\n",
    "    w_r2 = np.zeros((N, tl))\n",
    "    for kdx in trange(N):\n",
    "        ke = k_exact[kdx]\n",
    "        lge = 2.0 * ke + 1.0\n",
    "        k = round(ke)\n",
    "        lg = 2 * k + 1\n",
    "        rng = np.arange(lg) - k\n",
    "        cos = np.cos(2.0 * m * np.pi * rng / lge)\n",
    "        sin = np.sin(2.0 * m * np.pi * rng / lge)\n",
    "        dcos = np.cos(4.0 * m * np.pi * rng / lge)\n",
    "        dsin = np.sin(4.0 * m * np.pi * rng / lge)\n",
    "        exp = np.exp(-0.5 * np.square(3.0 * rng / ke))\n",
    "        sqrt_exp = np.exp(-0.25 * np.square(3.0 * rng / ke))\n",
    "        A_W = np.square(cos) * exp\n",
    "        C_W = np.square(sin) * exp\n",
    "        E_W = np.square(dcos) * exp\n",
    "        F_W = np.square(dsin) * exp\n",
    "        A = np.sum(A_W)\n",
    "        B = np.sum(cos * dcos * exp)\n",
    "        C = np.sum(C_W)\n",
    "        D = np.sum(sin * dsin * exp)\n",
    "        E = np.sum(E_W)\n",
    "        F = np.sum(F_W)\n",
    "        G = B ** 2 - A * E\n",
    "        H = D ** 2 - C * F\n",
    "        w = np.correlate(signal, cos * exp, \"same\")\n",
    "        x = np.correlate(signal, sin * exp, \"same\")\n",
    "        y = np.correlate(signal, dcos * exp, \"same\")\n",
    "        z = np.correlate(signal, dsin * exp, \"same\")\n",
    "        wxyz = np.stack([w, x, y, z], axis=1)\n",
    "        # the solution, (X'T X')^-1 * X'T * y' - X'T X' is a 4x4 with a simple inverse:\n",
    "        a = (B / G) * y - (E / G) * w\n",
    "        b = (D / H) * z - (F / H) * x\n",
    "        c = (B / G) * w - (A / G) * y\n",
    "        d = (D / H) * x - (C / H) * z\n",
    "        abcd = np.stack([a, b, c, d], axis=1)\n",
    "        abmag = np.maximum(np.sum(np.square(abcd[:, :2]), axis=1), 1e-9)\n",
    "        cdmag = np.sum(np.square(abcd[:, 2:]), axis=1)\n",
    "        # ratio = np.maximum(cdmag / abmag * 100, 1.0)\n",
    "        # sum of square weighted signal - (y')^2 = exp * (sig)^2\n",
    "        ssws = np.correlate(sqsig, exp, \"same\")\n",
    "        # sum of weighted best fit model, squared (X' b)^2 = exp * (cos...dsin)^2 * abcd^2\n",
    "        swbfms = np.square(a) * A\n",
    "        swbfms += np.square(b) * C\n",
    "        swbfms += np.square(c) * E\n",
    "        swbfms += np.square(d) * F\n",
    "        # squared sum of whitened signal - see wiki article on weighted least squares in diagonal weighting case\n",
    "        # this part is for adjusting the average - because of the weighting, can no longer assume mean = 0\n",
    "        # cross term from (y' - avg(y'))^2 turns out to be -2 times the square of the average\n",
    "        # -1/lg * (y * sqrt(exp))\n",
    "        sqsws = np.square(np.correlate(signal, sqrt_exp / np.sqrt(lg), \"same\"))\n",
    "        denom = ssws - sqsws\n",
    "        # abcd[:, 2:] /= ratio[:, None]\n",
    "        # sum of weighted dot product - cross term from (y' - X' b)^2\n",
    "        # -2 * y' X' b = -2 * exp * sig * (cos...dsin) * abcd = -2 * wxyz * abcd\n",
    "        swdp = np.sum(wxyz * abcd, axis=1)\n",
    "        w_r2[kdx] = (2.0 * swdp - swbfms - sqsws) / denom\n",
    "        w_ab[kdx] = abcd[:, :2]\n",
    "        \n",
    "    pbar = tqdm(total=8)\n",
    "    pbar.set_description(\"finding best k values   \")\n",
    "    # empirically, this blur is about 6 breaths long\n",
    "    nbl = 37 * 3\n",
    "    wr2_blur = correlate1d(w_r2, np.ones(nbl) / nbl, mode=\"nearest\")\n",
    "    k_best_spline = cspline1d(np.argmax(wr2_blur, axis=0).astype(float), 1.0)\n",
    "    k_best_sm = np.around(k_best_spline).astype(int).clip(0, N - 1)\n",
    "    best_ks = k_best_sm[None,:,None]\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"creating rotation matrix\")\n",
    "    # ab is how  well cos and sin fit, respectively. From this we can determine phase.\n",
    "    ab = np.take_along_axis(w_ab, best_ks, axis=0).squeeze()\n",
    "    # start constructing a rotation matrix to rotate points by the inverse of that phase\n",
    "    invert = np.zeros((*ab.shape, 2))\n",
    "    invert[:, 0, :] = ab\n",
    "    invert[:, 1, 0] = -ab[:, 1]\n",
    "    invert[:, 1, 1] = ab[:, 0]\n",
    "    invert /= linalg.norm(ab, axis=1).clip(min=1e-9)[:, None, None]\n",
    "    # multiplying ab by invert with an offset of 1 gets the sin and cos components, but rotated\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"calculating phase diffs \")\n",
    "    rotated = np.matmul(invert[:-1], ab[1:, :, None]).squeeze()\n",
    "    u_bnd = 2 * m * np.pi / (2 * k_0 + 1.0)\n",
    "    l_bnd = 1.25 * m * np.pi / (2 * k_m1 + 1.0)\n",
    "    # finally, grab the rotated sin and cos components and use arctan2 to get the phase difference\n",
    "    # my math seems to have been wrong, so we need to multipy by -1\n",
    "    phase_diffs = -np.arctan2(rotated[:, 1], rotated[:, 0])\n",
    "    # phase_diffs = np.where(phase_diffs <= 0.0, phase_diffs + 2.0 * np.pi, phase_diffs)\n",
    "    cumulative_phase = np.cumsum(phase_diffs.clip(l_bnd, u_bnd))\n",
    "    \n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"estimating frequencies  \")\n",
    "    mean_pd = np.mean(phase_diffs[np.logical_and(phase_diffs < u_bnd, phase_diffs > l_bnd)])\n",
    "    avg_phase_offset = round(4.0 * np.pi / mean_pd) # how long would the mean phase differenc take to sum to 2 wavelengths?\n",
    "    \n",
    "    # provide an estimate of the instantaneous frequencies\n",
    "    est_inst_freqs = np.zeros(tl)\n",
    "    est_inst_freqs[:-1] = phase_diffs * (fs / (2.0 * np.pi))\n",
    "    est_inst_freqs[-1] = est_inst_freqs[-2]\n",
    "    \n",
    "    # provide the second estimate of frequencies\n",
    "    smooth_k_freqs = 0.5 * m * fs / (k_best_spline * (k_m1 - k_0) / N + k_0)\n",
    "    \n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"dynamically warping time\")\n",
    "    # apply phase warp, find the error and then the r^2 value\n",
    "    interp = np.interp(cumulative_phase - 2.0 * np.pi, cumulative_phase, np.arange(cumulative_phase.size))\n",
    "    x_max = int(np.max(interp))\n",
    "    resamp = np.interp(np.arange(1, x_max), interp, signal[:-1])\n",
    "    err = signal[1:x_max] - resamp\n",
    "    sse = np.correlate(np.square(err), np.ones(avg_phase_offset) / avg_phase_offset, mode=\"full\")\n",
    "    sst = np.correlate(sqsig[1:x_max], np.ones(avg_phase_offset) / avg_phase_offset, mode=\"full\")\n",
    "    r2_fast = 1.0 - sse / sst\n",
    "    \n",
    "    # go the other way, interpolate from the phase warp to the original\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"reversing the warp      \")\n",
    "    rev_interp = np.interp(interp, np.arange(tl), signal)\n",
    "    \n",
    "    rev_err = signal[:-1] - rev_interp\n",
    "    rev_sse = np.correlate(np.square(rev_err), np.ones(avg_phase_offset) / avg_phase_offset, mode=\"full\")\n",
    "    rev_sst = np.correlate(sqsig[:-1], np.ones(avg_phase_offset) / avg_phase_offset, mode=\"full\")\n",
    "    rev_r2_fast = 1.0 - rev_sse / rev_sst\n",
    "    \n",
    "    gsn = [1.0, 6.0, 15.0, 20.0, 15.0, 6.0, 1.0]\n",
    "    gsn /= np.sum(gsn)\n",
    "    \n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"calculating r^2         \")\n",
    "    r2_max_fast = np.maximum(r2_fast[:1 - avg_phase_offset], r2_fast[avg_phase_offset - 1:])\n",
    "    rev_r2_max_fast = np.maximum(rev_r2_fast[:1 - avg_phase_offset], rev_r2_fast[avg_phase_offset - 1:])\n",
    "    fwd_r2 = np.correlate(np.concatenate((np.ones(4) * r2_max_fast[0], r2_max_fast, np.ones(tl - x_max + 3) * r2_max_fast[-1])), gsn, mode=\"same\").clip(0.0)[3:-3]\n",
    "    rev_r2 = np.correlate(np.concatenate((np.ones(3) * rev_r2_max_fast[0], rev_r2_max_fast, np.ones(4) * rev_r2_max_fast[-1])), gsn, mode=\"same\").clip(0.0)[3:-3]\n",
    "    \n",
    "    skew_win = 8 * avg_phase_offset + 1\n",
    "    r2_mu = np.correlate(r2_max_fast, np.ones(skew_win) / skew_win, mode=\"full\")\n",
    "    r2_mu = np.maximum(r2_mu[:1-skew_win], r2_mu[skew_win - 1:])\n",
    "    # r2_sig = np.sqrt(np.correlate(np.square(r2_max_fast - r2_mu), np.ones(skew_win) / skew_win, mode=\"same\")).clip(1e-7)\n",
    "    # r2_mncub = np.correlate(np.square(r2_max_fast) * r2_max_fast, np.ones(skew_win) / skew_win, mode=\"same\")\n",
    "    # r2_skew = (r2_mncub - 3.0 * r2_mu * np.square(r2_sig) - np.square(r2_mu) * r2_mu) # / (np.square(r2_sig) * r2_sig)\n",
    "    # r2_med = np.zeros(tl)\n",
    "    # wk = 8 * avg_phase_offset\n",
    "    \n",
    "    # median filter - if the median is less than the segments marked periodic, it's probably the middle of a long period\n",
    "    # of movement, or a period of being out of bed.\n",
    "    # Anything less than 1 minute is likely to be filtered out by the median filter. This can be changed.\n",
    "    periodic_r2 = np.maximum(fwd_r2, rev_r2)\n",
    "    periodic_r2[0] = 0.0\n",
    "    periodic_r2[-1] = 0.0\n",
    "    r2_med = medfilt(periodic_r2, skew_win)\n",
    "    \n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"getting best sinusoid r2\")\n",
    "    r2_sin = np.take_along_axis(w_r2, best_ks.reshape(1, -1), axis=0).squeeze()\n",
    "    \n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"Done. Plotting..........\")\n",
    "    pbar.close()\n",
    "    print(mean_pd, avg_phase_offset)\n",
    "    \n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(rng, signal[rng + x_igt])\n",
    "    # ax.plot(interpd, signal[rng + x_nxt])\n",
    "    # ax.plot(rng, signal[rng + x_nxt], \"k:\")\n",
    "    # ax.plot(rng, (cumulative_phase[rng + x_igt] - ph_igt) * 2e-5)\n",
    "    # ax.plot(rng, (cumulative_phase[rng + x_nxt] - 2.0 * np.pi - ph_igt) * 2e-5)\n",
    "    # plt.show()\n",
    "    \n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(lrng, cumulative_phase[lrng + x_igt])\n",
    "    # ax.plot(rng, cumulative_phase[rng + x_nxt] - 2.0 * np.pi)\n",
    "    # ax.plot(interpd, cumulative_phase[rng + x_nxt] - 2.0 * np.pi, \"k:\")\n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "    _, ax = plt.subplots(1, 1)\n",
    "    ax.plot(np.arange(cumulative_phase.size), signal[:-1])\n",
    "    ax.plot(interp, signal[:-1], linestyle=\":\")\n",
    "    ax.plot(np.arange(1, x_max), err, alpha=0.1)\n",
    "    # ax.plot(np.arange(1, x_max), np.maximum(r2_max_fast, rev_r2_max_fast[1:x_max]) * 1e-3)\n",
    "    ax.plot(np.arange(tl), fwd_r2 * 1e-3, linestyle=\":\")\n",
    "    ax.plot(np.arange(tl), rev_r2 * 1e-3, linestyle=\":\")\n",
    "    ax.plot(np.arange(tl), r2_med * 1e-3, alpha=0.2)\n",
    "    ax.plot(np.arange(1, x_max), r2_mu * 1e-3, alpha=0.2)\n",
    "    ax.plot(r2_sin * 1e-3, alpha=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(cos)\n",
    "    # ax.plot(sin)\n",
    "    # ax.plot(cos * np.sqrt(exp))\n",
    "    # ax.plot(sin * np.sqrt(exp))\n",
    "    # plt.show()\n",
    "    \n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(phase_diffs * 1e-3)\n",
    "    # ax.plot(ab, linestyle=\":\", alpha=0.1)\n",
    "    # ax.plot(signal, c=\"k\", alpha=0.1)\n",
    "    # ax.plot(np.arctan2(ab[:, 1], ab[:, 0]) * 1e-3, c=\"r\", alpha=0.1)\n",
    "    # ax.axhline(u_bnd * 1e-3)\n",
    "    # ax.axhline(l_bnd * 1e-3)\n",
    "    # plt.show()\n",
    "    \n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.imshow(wr2_blur.clip(min=0.0), aspect=\"auto\")\n",
    "    # ax.plot(best_ks.squeeze())\n",
    "    # plt.show()\n",
    "    \n",
    "    # _, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True)\n",
    "    # ax1.imshow(ssws[:, 12000:13000], aspect=\"auto\")\n",
    "    # ax2.imshow(swbfms[:, 12000:13000], aspect=\"auto\")\n",
    "    # ax3.imshow((2.0 * swdp[:, 12000:13000] - swbfms[:, 12000:13000] - sqsws[:, 12000:13000]), aspect=\"auto\")\n",
    "    # ax4.imshow(swdp[:, 12000:13000], aspect=\"auto\")\n",
    "    # plt.show()\n",
    "    \n",
    "#     final_r2 = np.zeros((tl, ))\n",
    "#     final_lg = np.zeros((tl, ))\n",
    "#     sigma = 2.0  # this gets us to 12 timesteps away... meaning we can span ~5 sec gaps\n",
    "#     fs = 2.0\n",
    "#     n_std = 6  # 6 gets us down to 1.5e-8, which is nonzero enough for me\n",
    "#     c_normal = np.exp(-0.5 * np.square((np.arange(fs * sigma * n_std * 2 + 1) - (fs * sigma * n_std)) / sigma))\n",
    "#     c_normal /= np.sum(c_normal)\n",
    "#     for idx, fdx in trange(0):\n",
    "# #     for idx, fdx in tqdm(enumerate(best_freqs), total=best_freqs.size, ncols=100):\n",
    "#         offset = round(lgs[fdx] / m)\n",
    "#         lg = int(lgs[fdx])\n",
    "#         final_lg[idx] = lg\n",
    "#         if idx + offset + lg >= tl:\n",
    "#             break\n",
    "\n",
    "#         # calculate dynamic time warp alignment\n",
    "#         alignment = dtw(signal[idx + offset:idx + lg + offset], signal[idx:idx + lg], keep_internals=True)\n",
    "#         # alignment.plot(type=\"twoway\", offset=-2 * np.sqrt(np.sum(np.square(coeffs[fdx, idx]))))\n",
    "#         # this will necessarily be a function even in the worst case\n",
    "#         new_xs = 0.5 * (alignment.index1 + alignment.index2)\n",
    "#         new_ys = alignment.index2 - alignment.index1\n",
    "#         # interpolate it, then correlate, rotate it back, then perform the warp\n",
    "#         # cut off the first and last bit, helping to guarantee it's in the interp range\n",
    "#         ixs = np.arange(lg * 2 - 5) * 0.5 + 1.0\n",
    "#         # 1-1e-8 is a bit of a kludge to get back a function but it's fine innit\n",
    "#         iys = interp1d(new_xs, new_ys * (1.0 - 1e-8))(ixs)\n",
    "#         max_dev = 5.0  # can be up to 1 second off\n",
    "#         miys = np.mean(iys).clip(min=-max_dev, max=max_dev)\n",
    "#         siys = iys.clip(miys - max_dev, miys + max_dev)\n",
    "#         spys = correlate1d(siys, c_normal, mode=\"constant\")\n",
    "#         rix = ixs - 0.5 * spys\n",
    "#         riy = ixs + 0.5 * spys\n",
    "#         sse_dtw = np.sum(np.square(spys)) * 16.0  # abs(abs(x-30)-15)-7.5 -> r^2 = 0\n",
    "#         sst_dtw = np.sum(np.square(ixs - np.mean(ixs)))\n",
    "#         qys = interp1d(np.arange(lg), signal[idx + offset:idx + lg + offset])(rix)\n",
    "#         tys = interp1d(np.arange(lg), signal[idx:idx + lg])(riy)\n",
    "#         sse_fin = np.sum(np.square(qys - tys))\n",
    "#         sst_fin = np.sum(np.square(tys - np.mean(tys)))\n",
    "#         cr2 = 1.0 - sse_dtw / sst_dtw - sse_fin / sst_fin\n",
    "\n",
    "#         if idx == 130:\n",
    "#             _, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#             ax1.plot(rix, riy)\n",
    "#             ax1.plot(alignment.index1, alignment.index2)\n",
    "#             ax2.plot(np.arange(lg), signal[idx + offset:idx + lg + offset])\n",
    "#             ax2.plot(riy, qys)\n",
    "#             ax2.plot(riy, tys)\n",
    "#             plt.show()\n",
    "\n",
    "        # sse = np.sum(np.square(signal[idx:idx + lg] - signal[idx + offset:idx + lg + offset]))\n",
    "        # cr2 = 1.0 - sse / max(sss[fdx, idx], sss[fdx, idx + offset])\n",
    "        # final_r2[idx:idx + offset + lg] = np.maximum(final_r2[idx:idx + offset + lg], cr2)\n",
    "    return est_inst_freqs, smooth_k_freqs, periodic_r2, ab, r2_med\n",
    "\n",
    "def split_test(sig_a, sig_b, coords_a, coords_b, output=False):\n",
    "    # prepare X matrix\n",
    "    n_a = sig_a.size\n",
    "    n_b = sig_b.size\n",
    "    all_samples = np.concatenate((sig_a, sig_b))\n",
    "    all_coords = np.concatenate((coords_a, coords_b), dtype=float)\n",
    "    all_coords -= np.mean(all_coords)\n",
    "    split_level = np.concatenate((np.full(n_a, -0.5), np.full(n_b, 0.5)))\n",
    "    ones = np.ones(n_a + n_b)\n",
    "    X = np.stack((all_coords, split_level, ones), axis=-1)\n",
    "    \n",
    "    # perform the fit, get the residuals\n",
    "    xcov = np.linalg.inv(np.dot(X.T, X))\n",
    "    beta = np.dot(np.dot(xcov, X.T), all_samples)\n",
    "    res = all_samples - np.dot(X, beta)\n",
    "    \n",
    "    # calculate covariance - currently using estimate of sig\n",
    "    sig2_est = 0.2 ** 2 # np.sum(np.square(res)) / (n_a + n_b - 3)\n",
    "    cov = sig2_est * xcov\n",
    "    \n",
    "    # calculate statistic and probability\n",
    "    T = np.abs(beta[1] / np.sqrt(cov[1, 1]))\n",
    "    p = 2.0 - 2.0 * t_dist.cdf(T, n_a + n_b - 3) # two tailed\n",
    "    \n",
    "    if output:\n",
    "        print(cov)\n",
    "        print(beta)\n",
    "        print(T, p)\n",
    "        print(n_a, n_b)\n",
    "        print(sig2_est)\n",
    "        print(res)\n",
    "        \n",
    "        _, ax = plt.subplots(1, 1)\n",
    "        ax.plot(np.concatenate((coords_a, coords_b), dtype=float), all_samples)\n",
    "        ax.plot(np.concatenate((coords_a, coords_b), dtype=float), np.dot(X, beta))\n",
    "        plt.show()\n",
    "    \n",
    "    return T, p\n",
    "\n",
    "def get_mse(sig, winsize=75):\n",
    "    c0 = correlate(np.square(sig), np.ones(winsize)) / winsize\n",
    "    return np.minimum(c0[:1 - winsize], c0[winsize - 1:])\n",
    "\n",
    "def get_mse_fracs(sig, winsize=75):\n",
    "    mse0 = get_mse(sig[:, 0], winsize)\n",
    "    mse1 = get_mse(sig[:, 1], winsize)\n",
    "    return mse0 / (mse0 + mse1), mse1 / (mse0 + mse1)\n",
    "\n",
    "def get_cdf_diff(residuals_1, residuals_2, plot=False):\n",
    "    ord1 = np.sort(residuals_1)\n",
    "    ord2 = np.sort(residuals_2)\n",
    "    n = residuals_1.size\n",
    "    m = residuals_2.size\n",
    "    start = 0\n",
    "    if ord1[0] < ord2[0]:\n",
    "        start = np.max(np.argwhere(ord1 < ord2[0]).squeeze())\n",
    "        sup = (start + 1) / n\n",
    "#         print(1, sup)\n",
    "    elif ord2[0] < ord1[0]:\n",
    "        sup = np.max(np.argwhere(ord2 < ord1[0]).squeeze()) / m\n",
    "#         print(2, sup)\n",
    "    else:\n",
    "        sup = 0.0\n",
    "    \n",
    "    for idx in range(start, n-1):\n",
    "        lw = ord2 <= ord1[idx]\n",
    "        up = ord2 < ord1[idx + 1]\n",
    "        if np.count_nonzero(up) == 0:\n",
    "            # none of them are less than the upper bound\n",
    "            sup = max(sup, (idx + 1) / n)\n",
    "#             print(3, sup)\n",
    "        else:\n",
    "            if np.count_nonzero(lw) == 0:\n",
    "                ldx = 0\n",
    "            else:\n",
    "                ldx = np.max(np.argwhere(lw).squeeze())\n",
    "            udx = np.max(np.argwhere(up).squeeze())\n",
    "            for jdx in range(ldx, udx + 1):\n",
    "                sup = max(sup, abs((idx +  1) / n - (jdx + 1) / m))\n",
    "    if ord1[-1] < ord2[-1]:\n",
    "        lw = np.max(np.argwhere(ord2 <= ord1[-1]).squeeze()) / m\n",
    "        sup = max(sup, 1.0 - lw)\n",
    "#         print(5, sup, lw)\n",
    "    if plot:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "        o1e = np.concatenate((np.ones(1) * ord1[0] - 0.15, np.repeat(ord1, 2), np.ones(1) * ord1[-1] + 0.15))\n",
    "        o2e = np.concatenate((np.ones(1) * ord2[0] - 0.15, np.repeat(ord2, 2), np.ones(1) * ord2[-1] + 0.15))\n",
    "        ax.plot(o1e, np.repeat(np.arange(n + 1) / n, 2))\n",
    "        ax.plot(o2e, np.repeat(np.arange(m + 1) / m, 2))\n",
    "        plt.show()\n",
    "#     print(sup)\n",
    "    return sup\n",
    "\n",
    "def fit_line(samples, coords):\n",
    "    A = np.stack((coords, np.ones(coords.shape)), axis=-1)\n",
    "    pi = np.linalg.pinv(A)\n",
    "    return np.dot(pi, samples)\n",
    "\n",
    "def get_residuals(samples, coords, mb):\n",
    "    A = np.stack((coords, np.ones(coords.shape)), axis=-1)\n",
    "    pred = np.dot(A, mb)\n",
    "    return samples - pred\n",
    "\n",
    "# Kolmogorov-Smirnov test - are sample_a and sample_b continuous?\n",
    "# Specifically, what is the probability that a and b are fit by a single trendline just as well as by two separate ones\n",
    "# Returns the likelihood that a and b are continuous\n",
    "def ks_test(sample_a, sample_b, coords_a, coords_b, plot=False):\n",
    "    # concatenate y and t\n",
    "    all_samples = np.concatenate((sample_a, sample_b))\n",
    "    all_coords = np.concatenate((coords_a, coords_b))\n",
    "    # fit lines to the first samples, the second samples, and the concatenated samples\n",
    "    mba = fit_line(sample_a, coords_a)\n",
    "    mbb = fit_line(sample_b, coords_b)\n",
    "    mbt = fit_line(all_samples, all_coords)\n",
    "    # find the residuals for the above fits\n",
    "    res_a = get_residuals(sample_a, coords_a, mba)\n",
    "    res_b = get_residuals(sample_b, coords_b, mbb)\n",
    "    res_t = get_residuals(all_samples, all_coords, mbt)\n",
    "    # find the maximum cdf difference in the distribution of residuals\n",
    "    cdfd_t = get_cdf_diff(np.concatenate((res_a, res_b)), res_t, plot)\n",
    "    n = all_samples.size\n",
    "    # calculate the probability that they are continuous (lower means less likely to be continuous)\n",
    "    p = 2.0 * np.exp(-n * cdfd_t**2)\n",
    "    return min(0.999, p) # 0.999 - important for bayesian updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41bb2364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 36/36 [00:05<00:00,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161815, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ricky = ricker(101, 10)\n",
    "# morry = morlet2(101, 12, 1.7) * 2743 / 2168\n",
    "# _, ax = plt.subplots(1, 1)\n",
    "# ax.plot(ricky, c=\"r\")\n",
    "# ax.plot(morry, c=\"k\")\n",
    "# plt.show()\n",
    "\n",
    "# %%\n",
    "# 211107_015105 -- this is a good one\n",
    "# 211102_003909 -- another goodie. Lots of sleep, some time not in bed at the beginning.\n",
    "#               -- No wake up though.\n",
    "# 211101_002730 -- Excellent. 5 Sleep cycles visible. One spot not flipped right.\n",
    "######\n",
    "# the above ones are all old\n",
    "# 220103_232249 - this one has a long gap out of bed, very disturbed sleep, but a few deep sleep and REM blobs\n",
    "# 220105_005821 - pretty solid sleep, clear sleep cycles visible relatively evenly spaced, more deep sleep at the start and more REM at the end.\n",
    "# \n",
    "dt = \"220119_010917\"\n",
    "\n",
    "gl = sorted(glob(f\"sleepypi/run{dt}/*.pkl.gz\"))\n",
    "\n",
    "streams = []\n",
    "times = []\n",
    "\n",
    "# get timezone offset for local START of night, so that DST is handled appropriately\n",
    "uctdiff = datetime.strptime(dt, \"%y%m%d_%H%M%S\").astimezone().utcoffset()\n",
    "tzoffset = (uctdiff.days * 86400 + uctdiff.seconds) * 1000  # timezone offset from utc\n",
    "\n",
    "for idx in trange(len(gl)):\n",
    "    with gzip.open(gl[idx], \"rb\") as f:\n",
    "        p = pickle.load(f)\n",
    "        data_stream, fhist, gz, fri, tstamps, video = p\n",
    "        streams.append(data_stream)\n",
    "        times.append(tstamps.astype(np.int64) * 50 + 1609459200000 + tzoffset)\n",
    "#         with VideoWriter(gl[idx][:-6] + \"mp4\") as vid:\n",
    "#             vid.from_array(video)\n",
    "\n",
    "n = np.concatenate(streams, axis=0)\n",
    "# convert times back to epoch time in milliseconds, then to np.datetime64 in ms\n",
    "timestamps = np.concatenate(times, axis=0).astype(\"<M8[ms]\")\n",
    "print(n.shape)\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87867e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rms0 = np.sqrt(get_mse(n[:, 0]))\n",
    "# rms1 = np.sqrt(get_mse(n[:, 1]))\n",
    "# rms_comb = np.sqrt(np.square(rms0) + np.square(rms1))\n",
    "\n",
    "# sig0, sm0 = adjust_for_zero_trend(n[:, 0])\n",
    "# sig1, sm1 = adjust_for_zero_trend(n[:, 1])\n",
    "# # sig0 = n[:, 0]\n",
    "# # sig1 = n[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea15a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "omega = 10.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)  # indices ~50-85 are breathing frequencies\n",
    "# widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "# z_wave = wavefinding_cwt(np.cumsum(sig0 + 1.0j * sig1), widths_morlet, omega)\n",
    "# mags_z = np.abs(z_wave) / rms_comb.clip(1e-5)\n",
    "\n",
    "# _, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True)\n",
    "# # ax1.plot(sig0)\n",
    "# # ax1.plot(sig1)\n",
    "# ax1.plot(n[:, 0])\n",
    "# ax1.plot(n[:, 1])\n",
    "# ax1.plot(rms0, linestyle=\":\")\n",
    "# ax1.plot(rms1, linestyle=\":\")\n",
    "# # ax1.plot(sm0, linestyle=\":\")\n",
    "# # ax1.plot(sm1, linestyle=\":\")\n",
    "# ax2.plot(np.cumsum(sig0))\n",
    "# ax2.plot(np.cumsum(sig1))\n",
    "# ax3.imshow(mags_z.clip(max=np.percentile(mags_z, 98.0)), aspect=\"auto\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fd3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\n",
    "sm = 81\n",
    "pos_x = n[:, 4]\n",
    "sm_pos_x = medfilt(pos_x, sm)\n",
    "pos_y = n[:, 5]\n",
    "sm_pos_y = medfilt(pos_y, sm)\n",
    "\n",
    "mx_lead, mx_lag, bx_lead, bx_lag, rmsx_lead, rmsx_lag, r2x_lead, r2x_lag = fit_linear_lead_and_lag(sm_pos_x, winsize=51)\n",
    "my_lead, my_lag, by_lead, by_lag, rmsy_lead, rmsy_lag, r2y_lead, r2y_lag = fit_linear_lead_and_lag(sm_pos_y, winsize=51)\n",
    "\n",
    "k = 40\n",
    "a = k * (k + 1) * (2 * k + 1) * (3 * k ** 2 + 3 * k + 1) / 15.0\n",
    "b = k * (k + 1) * (2 * k + 1) / 3.0\n",
    "c = 2 * k + 1\n",
    "det = a * c - b ** 2\n",
    "xs = np.arange(c, dtype=float) - k\n",
    "\n",
    "xstx0 = correlate1d(sm_pos_x, np.square(xs), mode=\"nearest\")\n",
    "xstx1 = correlate1d(sm_pos_x, xs, mode=\"nearest\")\n",
    "xstx2 = correlate1d(sm_pos_x, np.ones(c), mode=\"nearest\")\n",
    "xsty0 = correlate1d(sm_pos_y, np.square(xs), mode=\"nearest\")\n",
    "xsty1 = correlate1d(sm_pos_y, xs, mode=\"nearest\")\n",
    "xsty2 = correlate1d(sm_pos_y, np.ones(c), mode=\"nearest\")\n",
    "\n",
    "crv_x = xstx0 * c / det - xstx2 * b / det\n",
    "crv_y = xsty0 * c / det - xsty2 * b / det\n",
    "\n",
    "slp_x = xstx1 / b\n",
    "slp_y = xsty1 / b\n",
    "\n",
    "se = 2.0 * np.sqrt(c / det)\n",
    "ses = 8.0 / np.sqrt(b)\n",
    "\n",
    "p_x = 2.0 * t_dist.cdf(np.abs(crv_x) / se, c - 3) - 1.0\n",
    "p_y = 2.0 * t_dist.cdf(np.abs(crv_y) / se, c - 3) - 1.0\n",
    "\n",
    "ps_x = 2.0 * t_dist.cdf(np.abs(slp_x) / ses, c - 3) - 1.0\n",
    "ps_y = 2.0 * t_dist.cdf(np.abs(slp_y) / ses, c - 3) - 1.0\n",
    "\n",
    "rmsx = np.minimum(rmsx_lead, rmsx_lag)\n",
    "rmsy = np.minimum(rmsy_lead, rmsy_lag)\n",
    "\n",
    "# fit the step function, just see how it goes...\n",
    "\n",
    "m = 401 # should be odd\n",
    "# first 0.5 is for half the step size (one half is negative the other is positive)\n",
    "# 4 / 2m is the coefficient in the matrix inverse, so we're still missing a factor of\n",
    "inv_coeff = 0.5 * 2.0 / m\n",
    "stfn = np.concatenate((np.full(m, -inv_coeff), np.zeros(m), np.full(m, inv_coeff)))\n",
    "step_x = correlate1d(sm_pos_x, stfn, mode=\"nearest\")\n",
    "step_y = correlate1d(sm_pos_y, stfn, mode=\"nearest\")\n",
    "\n",
    "sest = 50.0 * np.sqrt(2.0 / m)\n",
    "\n",
    "pst_x = 2.0 * t_dist.cdf(np.abs(step_x) / sest, 2 * m - 2) - 1.0\n",
    "pst_y = 2.0 * t_dist.cdf(np.abs(step_y) / sest, 2 * m - 2) - 1.0\n",
    "\n",
    "dx = np.abs(pos_x - sm_pos_x)\n",
    "dy = np.abs(pos_y - sm_pos_y)\n",
    "\n",
    "med_dx = np.median(dx)\n",
    "med_dy = np.median(dy)\n",
    "x_cond = dx > 12.0 * med_dx\n",
    "y_cond = dy > 12.0 * med_dy\n",
    "dx_extr = np.where(x_cond, 10.0, 0.0)\n",
    "dy_extr = np.where(y_cond, 8.0, 0.0)\n",
    "\n",
    "exp_k = 4\n",
    "either = np.logical_or(x_cond, y_cond).astype(float)\n",
    "expand = np.concatenate((np.full(exp_k, either[0]), either, np.full(exp_k, either[-1])))\n",
    "exp_cs = np.cumsum(expand)\n",
    "mov = exp_cs[2 * exp_k:] - exp_cs[:-2 * exp_k] > 0.5\n",
    "mov_float = medfilt(mov.astype(float), 2 * exp_k + 1)\n",
    "\n",
    "_, (ax, ax2, ax3) = plt.subplots(3, 1, sharex=True)\n",
    "ax.plot(pos_x, label=\"nx\")\n",
    "ax.plot(pos_y, label=\"ny\")\n",
    "ax.plot(sm_pos_x, label=\"medx\")\n",
    "ax.plot(sm_pos_y, label=\"medy\")\n",
    "ax.plot(mov_float * 10.0, label=\"mov\")\n",
    "ax.plot(either * 8.0, label=\"either\")\n",
    "# ax2.plot(crv_x, label=\"crvx\")\n",
    "# ax2.plot(crv_y, label=\"crvy\")\n",
    "ax2.plot(step_x, label=\"x\")\n",
    "ax2.plot(step_y, label=\"y\")\n",
    "# ax3.plot(p_x, label=\"px\", linestyle=\":\")\n",
    "# ax3.plot(p_y, label=\"py\", linestyle=\":\")\n",
    "# ax3.plot(ps_x, label=\"psx\")\n",
    "# ax3.plot(ps_y, label=\"psy\")\n",
    "ax3.plot(pst_x, label=\"x\")\n",
    "ax3.plot(pst_y, label=\"y\")\n",
    "ax.legend()\n",
    "ax2.legend()\n",
    "ax3.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2865fea4-6e76-46d5-8338-ea1bdcf19ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 17.83it/s]\n",
      "Done. Plotting..........: 100%|| 8/8 [00:04<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32682838902755607 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 19.18it/s]\n",
      "Done. Plotting..........: 100%|| 8/8 [00:04<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3310444677134384 38\n",
      "0 10 132 745 0.014442281543560309 0.5514590012874816\n",
      "10 13 745 1130 0.0019042894259631016 1.8981261049217697e-05\n",
      "13 15 1130 1687 0.17685102687725207 5.461469737166765e-08\n",
      "15 16 1687 2747 0.4873163476080391 0.9657448788258233\n",
      "16 20 2747 3133 0.7412770189317792 0.9021820974918648\n",
      "20 22 3133 4100 0.6201431718295891 0.4771955461016811\n",
      "22 23 4100 4561 0.799459932329911 0.9651568686913149\n",
      "23 36 4561 6111 0.3724845369941893 0.12372086016603634\n",
      "36 38 6111 6393 0.9905858587406322 0.031295638757127264\n",
      "38 40 6393 6931 0.9384429104769959 0.8049980163047269\n",
      "40 41 6931 8162 0.7081660380150334 0.9757123140904198\n",
      "41 42 8162 9185 0.9154878354929528 0.6577133184065\n",
      "42 43 9185 9912 1.4561903016740985e-08 7.380929688034026e-36\n",
      "43 45 9912 10872 0.9563862780835134 0.8362832223681058\n",
      "45 46 10872 11313 0.6905532832292556 0.6268903922886249\n",
      "46 49 11313 14253 0.22556263577825175 8.399658536443871e-05\n",
      "49 51 14253 18616 6.865992508753305e-05 7.4687360424790345e-47\n",
      "51 52 18616 18839 0.000428070758167193 2.717945564167934e-07\n",
      "52 58 18839 20941 0.14143514938072677 5.3762725511190755e-12\n",
      "58 68 20941 23115 2.562863378405596e-22 0.0\n",
      "68 71 23115 23639 0.002214576766239962 0.9983289707889867\n",
      "71 72 23639 25234 0.0004772713659332295 1.5071484197716852e-25\n",
      "72 82 25234 28543 0.07138092174613253 0.6934116159915891\n",
      "82 83 28543 28705 0.928256476654947 1.3849711993482268e-07\n",
      "83 84 28705 28930 0.9020360993146317 0.9548696973078791\n",
      "84 86 28930 29484 0.8612226703929452 0.27882247991875175\n",
      "86 88 29484 30819 0.9882542576571647 0.01955250216211024\n",
      "88 90 30819 31473 0.15318901224769343 3.50367823998507e-15\n",
      "90 91 31473 32058 0.05244021355208207 0.005792573683607971\n",
      "91 92 32058 32268 0.8542594440028703 0.7916728528479753\n",
      "92 93 32268 33292 0.97075369147912 0.9712849076891572\n",
      "93 96 33292 33558 0.9479375973919143 0.9402876294727674\n",
      "96 97 33558 34141 0.8981272754454951 0.9961582663751015\n",
      "97 99 34141 34520 0.9698931689818118 3.824795114090522e-06\n",
      "99 100 34520 34873 0.0 2.090397963145546e-19\n",
      "100 105 34873 36069 0.01066720010751443 1.4018625932661939e-30\n",
      "105 106 36069 36258 0.9649515722061879 0.805283152813824\n",
      "106 107 36258 36559 0.6992168793387418 0.4590068359749394\n",
      "107 108 36559 36946 0.16577398957559364 0.7533443053274139\n",
      "108 109 36946 37166 0.6594631994207145 0.5784202959316881\n",
      "109 117 37166 38528 2.248858593789649e-06 1.5084476596203699e-07\n",
      "117 118 38528 38927 0.9925137248357232 0.0\n",
      "118 119 38927 39734 0.9835960111388167 0.9953427567389983\n",
      "119 121 39734 41163 1.2410223154729966e-294 0.0\n",
      "121 125 41163 42657 0.7445572464095386 1.3100913104473307e-38\n",
      "125 126 42657 44122 0.8084513070043604 2.037862772018582e-08\n",
      "126 128 44122 44461 0.8072401584702982 0.8863839807496623\n",
      "128 129 44461 44787 0.506733172567918 0.6718142811780592\n",
      "129 153 44787 47438 0.007121259270784219 1.3867714172326703e-07\n",
      "153 154 47438 49364 0.9945488295890468 0.9878988100701207\n",
      "154 156 49364 51423 0.0002563213183305646 0.04604531423290115\n",
      "156 159 51423 53522 0.0001848883109325454 0.0\n",
      "159 162 53522 80128 0.07773995916853593 5.836766979312203e-05\n",
      "162 163 80128 80781 0.0618774972002597 0.6785772147771667\n",
      "163 165 80781 81383 0.9609642127538374 0.7008751248452365\n",
      "165 166 81383 82065 0.985123048956254 0.08051594494625314\n",
      "166 167 82065 82318 0.942295209406165 0.6084374643034265\n",
      "167 177 82318 83323 0.9581975079413896 0.9918280873959966\n",
      "177 187 83323 84694 0.999698534713443 0.8342793183723819\n",
      "187 188 84694 86643 0.8830720309268179 0.33180365569504416\n",
      "188 189 86643 87483 0.16838029453187867 0.771295059356469\n",
      "189 193 87483 88307 0.0024428446992005304 0.5419583424311851\n",
      "193 194 88307 88967 0.004481664896898269 0.23700842180675297\n",
      "194 195 88967 91136 0.9965669539455987 0.00010696822677845278\n",
      "195 196 91136 91894 0.9738932025330433 0.9777521740392179\n",
      "196 202 91894 96845 1.101668215985896e-05 6.951446538053766e-98\n",
      "202 203 96845 97545 0.9037377663663257 0.9907390557463074\n",
      "203 204 97545 106325 8.218425598393523e-07 0.4074002501886982\n",
      "204 205 106325 108245 0.9055678101083032 0.00023635848354144721\n",
      "205 206 108245 110173 0.9996340536548621 0.03232978894580194\n",
      "206 208 110173 112583 0.5628305912632023 0.8746427978291211\n",
      "208 209 112583 113500 0.8981571767778064 0.9774971364321025\n",
      "209 211 113500 115105 0.8846167080028745 0.1650924393139195\n",
      "211 212 115105 115971 0.67860053616756 0.9923480559740949\n",
      "212 213 115971 116313 0.9824432055543828 0.9905279234532807\n",
      "213 215 116313 117293 0.0014027450803847225 0.0\n",
      "215 216 117293 118230 0.9836562296812262 0.9896866892299611\n",
      "216 217 118230 118531 0.05839425672204822 0.9324381915703794\n",
      "217 218 118531 118777 0.013888712426246902 0.9890414755358727\n",
      "218 219 118777 120733 0.810652051104475 0.9931100704053636\n",
      "219 220 120733 122052 0.0012850956974768098 0.970964006458076\n",
      "220 221 122052 122271 0.9785328494606672 0.1575052599989032\n",
      "221 222 122271 123962 0.6984063722951728 0.0\n",
      "222 223 123962 128596 0.044427603997953806 0.9931533957236178\n",
      "223 224 128596 133689 4.3542110850701527e-17 0.2669772039817612\n",
      "224 226 133689 140729 8.16991937554745e-60 0.9471448219104384\n",
      "226 227 140729 140960 5.793669012734761e-10 0.8656810221511222\n",
      "227 228 140960 142700 0.9038126852058634 0.9850569696136432\n",
      "228 229 142700 143495 0.9637993663973528 0.9917054773952944\n",
      "229 230 143495 144183 0.010082753076417483 0.9803369064128791\n",
      "230 232 144183 145054 0.9683424519253907 0.8554218547392621\n",
      "232 234 145054 146601 0.9199196732455561 0.9479747321840087\n",
      "234 235 146601 154041 0.03905649894304065 0.047952582306582905\n",
      "235 237 154041 156353 0.9714471344587707 0.1032083234230527\n",
      "237 238 156353 156652 0.7807577561743695 0.8027460032085166\n"
     ]
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "r2_x, r2_y, lgs, freq_traces, r2_sin, p_notinbed, p_shift = find_periodic_fit_best(n[:, 0], n[:, 1], freqs[40:90], n[:, 4], n[:, 5], 3001, 301)\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.plot(pos_x, label=\"nx\")\n",
    "ax.plot(pos_y, label=\"ny\")\n",
    "ax.plot(sm_pos_x, label=\"medx\")\n",
    "ax.plot(sm_pos_y, label=\"medy\")\n",
    "ax.plot(p_shift * 10.0, label=\"shift\", alpha=0.5, linestyle=(0,(1,1,3,1)))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "149e7ef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33204\\1325282091.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# The non-periodic segments during out-of-bed periods are largely merged,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#  save for where I get out of and into bed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mp_oob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_notinbed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mbef_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'contig' is not defined"
     ]
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "x_env = np.abs(hilbert(n[:, 0]))\n",
    "y_env = np.abs(hilbert(n[:, 1]))\n",
    "\n",
    "fx, fy = get_mse_fracs(n)\n",
    "\n",
    "cutoff = 0.45\n",
    "\n",
    "r2 = r2_x * fx + r2_y * fy\n",
    "count = np.sum(r2 >= cutoff)\n",
    "rmses = np.sqrt(np.sum((r2 >= cutoff).astype(float) * (np.square(n[:, 0]) + np.square(n[:, 1]))) / count)\n",
    "condit = np.logical_and(r2 >= cutoff, np.logical_and(np.abs(n[:, 0]) < rms0 * 3.5, np.abs(n[:, 1]) < rms1 * 3.5))\n",
    "starts = np.argwhere(np.logical_and(condit[1:], np.logical_not(condit[:-1]))).squeeze()\n",
    "stops = np.argwhere(np.logical_and(np.logical_not(condit[1:]), condit[:-1])).squeeze()\n",
    "lrg_cond = np.argwhere(stops - starts > 35).squeeze()\n",
    "starts = starts[lrg_cond]\n",
    "stops = stops[lrg_cond]\n",
    "sm_cond = np.argwhere(starts[1:] - stops[:-1] > 2).squeeze()\n",
    "starts = starts[np.concatenate((np.array([0]), sm_cond + 1))]\n",
    "stops = stops[np.concatenate((sm_cond, np.array([stops.size - 1])))]\n",
    "    \n",
    "\n",
    "keep_start = np.ones(starts.size, dtype=bool)\n",
    "keep_stop = np.ones(starts.size, dtype=bool)\n",
    "mov_or_sigh = np.zeros(starts.size - 1)\n",
    "\n",
    "# first trim the ones that are too short (<=1.2s), and likely not movement or sighing\n",
    "# note, this will merge a lot of segments within periods of being out-of-bed\n",
    "# we'll remedy that later...\n",
    "for idx in range(starts.size - 1):\n",
    "    # get the lengths of the segments\n",
    "    lg_dur = starts[idx + 1] - stops[idx]\n",
    "    short_pen = 0.5 + 0.5 * np.tanh(0.33 * lg_dur)\n",
    "    rms_bef = np.sqrt(np.mean(np.square(n[starts[idx]:stops[idx], :2])))\n",
    "    rms_dur = np.sqrt(np.mean(np.square(n[stops[idx]:starts[idx + 1], :2])))\n",
    "    rms_aft = np.sqrt(np.mean(np.square(n[starts[idx + 1]:stops[idx + 1], :2])))\n",
    "    ratio = (rms_dur * 2.0 / (rms_bef + rms_aft) - 1.0).clip(min=0.0)\n",
    "    vel_stat = np.square(np.tanh(2.0 * ratio)) * short_pen\n",
    "    mov_or_sigh[idx] = vel_stat\n",
    "        \n",
    "\n",
    "keep_start[1:] = mov_or_sigh > 0.5\n",
    "keep_stop[:-1] = mov_or_sigh > 0.5\n",
    "contig_starts = starts[keep_start]\n",
    "contig_stops = stops[keep_stop]\n",
    "discont = np.zeros(contig_starts.size - 1)\n",
    "\n",
    "for idx in range(contig_starts.size - 1):\n",
    "    # get the lengths of the segments\n",
    "    lg_bef = contig_stops[idx] - contig_starts[idx]\n",
    "    lg_dur = contig_starts[idx + 1] - contig_stops[idx]\n",
    "    lg_aft = contig_stops[idx + 1] - contig_starts[idx + 1]\n",
    "    # segments should not differ in length by TOO much, or else the KS test will fail\n",
    "    use_lg = 2 * min(250, lg_bef, lg_aft)\n",
    "    bef_b = contig_stops[idx] - min(use_lg, lg_bef)\n",
    "    aft_e = contig_starts[idx + 1] + min(use_lg, lg_aft)\n",
    "    bef_t = np.arange(bef_b, contig_stops[idx], dtype=float)\n",
    "    aft_t = np.arange(contig_starts[idx + 1], aft_e, dtype=float)\n",
    "    p_x = ks_test(n[bef_b:contig_stops[idx], 2], n[contig_starts[idx + 1]:aft_e, 2], bef_t, aft_t)\n",
    "    p_y = ks_test(n[bef_b:contig_stops[idx], 3], n[contig_starts[idx + 1]:aft_e, 3], bef_t, aft_t)\n",
    "    discont[idx] = 1.0 - p_x * p_y\n",
    "    \n",
    "# Okay, now we decide what corresponds to not being in bed\n",
    "# Although, as a first pass, what we have here is already good enough!\n",
    "# The non-periodic segments during out-of-bed periods are largely merged,\n",
    "#  save for where I get out of and into bed\n",
    "for idx in range(contig.size):\n",
    "    p_oob = np.mean(p_notinbed[starts[idx]:stops[idx]])\n",
    "    bef_b = 0 if idx == 0 else stops[idx - 1]\n",
    "    aft_e = -1 if idx == starts.size - 1 else starts[idx + 1]\n",
    "    rms_bef = np.sqrt(np.mean(np.square(n[bef_b:starts[idx], :2])) * 2.0)\n",
    "    rms_dur = np.sqrt(np.mean(np.square(n[starts[idx]:stops[idx], :2])) * 2.0)\n",
    "    rms_aft = np.sqrt(np.mean(np.square(n[stops[idx]:aft_e, :2])) * 2.0)\n",
    "    # a few criteria to classify a segment as not in bed and delete it:\n",
    "    #   1. p_oob has to be pretty low\n",
    "    #   2. rms of the segment has to be pretty low\n",
    "\n",
    "    \n",
    "mx_lead, mx_lag, bx_lead, bx_lag, rmsx_lead, rmsx_lag = fit_linear_lead_and_lag(n[:, 2], winsize=155)\n",
    "my_lead, my_lag, by_lead, by_lag, rmsy_lead, rmsy_lag = fit_linear_lead_and_lag(n[:, 3], winsize=155)\n",
    "mw_lead, mw_lag, bw_lead, bw_lag, rmsw_lead, rmsw_lag = fit_linear_lead_and_lag(n[:, 4], winsize=31)\n",
    "mz_lead, mz_lag, bz_lead, bz_lag, rmsz_lead, rmsz_lag = fit_linear_lead_and_lag(n[:, 5], winsize=31)\n",
    "\n",
    "x_rms_frac = rmsx_lead / (rmsx_lead + rmsx_lag)\n",
    "y_rms_frac = rmsy_lead / (rmsy_lead + rmsy_lag)\n",
    "bx_best = x_rms_frac * bx_lag + (1.0 - x_rms_frac) * bx_lead # np.where(rmsx_lead < rmsx_lag, bx_lead, bx_lag)\n",
    "by_best = y_rms_frac * by_lag + (1.0 - y_rms_frac) * by_lead # np.where(rmsy_lead < rmsy_lag, by_lead, by_lag)\n",
    "w_rms_frac = rmsw_lead / (rmsw_lead + rmsw_lag)\n",
    "z_rms_frac = rmsz_lead / (rmsz_lead + rmsz_lag)\n",
    "bw_best = w_rms_frac * bw_lag + (1.0 - w_rms_frac) * bw_lead # np.where(rmsx_lead < rmsx_lag, bx_lead, bx_lag)\n",
    "bz_best = z_rms_frac * bz_lag + (1.0 - z_rms_frac) * bz_lead # np.where(rmsy_lead < rmsy_lag, by_lead, by_lag)\n",
    "\n",
    "b_best = np.stack((bx_best, by_best), axis=-1)\n",
    "\n",
    "# res_x = rpt.KernelCPD(kernel=\"linear\", min_size=100).fit(n[:, 2:4]).predict(pen=2)\n",
    "# res_y = rpt.KernelCPD(kernel=\"linear\", min_size=100).fit(n[:, 4:6]).predict(pen=25)\n",
    "# res_x = rpt.Window(width=100, model=\"l2\", jump=2).fit_predict(n[:, 2:4], pen=np.log(n.shape[0]) * 2 * 0.1**2)\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(timestamps, my_lead)\n",
    "ax1.plot(timestamps, my_lag)\n",
    "ax1.plot(timestamps, bx_best)\n",
    "ax1.plot(timestamps, by_best)\n",
    "ax1.plot(timestamps, bw_best)\n",
    "ax1.plot(timestamps, bz_best)\n",
    "ax1.plot(timestamps, rmsy_lead.clip(max=10.0) * 20)\n",
    "ax1.plot(timestamps, rmsy_lag.clip(max=10.0) * 20)\n",
    "ax1.plot(timestamps, n[:, 2:4], alpha=0.6)\n",
    "ax1.plot(timestamps, n[:, 4:6], alpha=0.2)\n",
    "# ax1.legend(\"mylead,mylag,bxbest,bybest,mrsylead,rmsylag,n2,n3,n4,n5\".split(\",\"))\n",
    "ax2.plot(timestamps, n[:, 0:2])\n",
    "ax2.axhline(y=rmses * 10.0)\n",
    "ax2.axhline(y=rmses * -10.0)\n",
    "ax2.plot(timestamps, r2 * 1e-3, c=\"k\", alpha=0.3, linestyle=\":\")\n",
    "# ax2.plot(timestamps, x_env)\n",
    "# ax2.plot(timestamps, y_env)\n",
    "\n",
    "# keep = np.ones(len(res_x), dtype=bool)\n",
    "\n",
    "# for idx in range(len(res_x) - 2):\n",
    "#     xs = np.arange(res_x[idx], res_x[idx + 2])\n",
    "#     res_a = ks_test(n[res_x[idx]: res_x[idx + 2], 2], xs, res_x[idx + 1] - res_x[idx], 0.01, 8)\n",
    "#     res_b = ks_test(n[res_x[idx]: res_x[idx + 2], 3], xs, res_x[idx + 1] - res_x[idx], 0.01, 8)\n",
    "#     if res_a and res_b:\n",
    "#         keep[idx + 1] = False\n",
    "\n",
    "# res_x = np.array(res_x)[keep]\n",
    "\n",
    "# keep = np.ones(len(res_x), dtype=bool)\n",
    "# for idx in range(len(res_x) - 2):\n",
    "#     xs = np.arange(res_x[idx], res_x[idx + 2])\n",
    "#     res_a = ks_test(n[res_x[idx]: res_x[idx + 2], 2], xs, res_x[idx + 1] - res_x[idx], 0.02, 8)\n",
    "#     res_b = ks_test(n[res_x[idx]: res_x[idx + 2], 3], xs, res_x[idx + 1] - res_x[idx], 0.02, 8)\n",
    "#     if res_a and res_b:\n",
    "#         keep[idx + 1] = False\n",
    "\n",
    "# res_x = np.array(res_x)[keep]\n",
    "\n",
    "for idx, ln in enumerate(res_x[:-1]):\n",
    "    ax1.axvline(x=timestamps[ln])\n",
    "    frac = np.count_nonzero(condit[ln:res_x[idx + 1]]) / (res_x[idx + 1] - ln)\n",
    "    if frac < 0.5:\n",
    "        ax1.axvspan(timestamps[ln], timestamps[res_x[idx + 1]], color=\"r\", alpha=0.2, ec=None)\n",
    "# for idx, ln in enumerate(res_y[:-1]):\n",
    "#     ax1.axvline(x=timestamps[ln])\n",
    "\n",
    "for idx in range(1, starts.size):\n",
    "    ts = timestamps[np.array((stops[idx - 1], starts[idx]))]\n",
    "    ax2.plot(ts, 0.01 * np.array((mov_or_sigh[idx - 1], mov_or_sigh[idx - 1])), c=\"m\")\n",
    "for idx in range(contig_starts.size):\n",
    "    ax2.axvspan(timestamps[contig_starts[idx]], timestamps[contig_stops[idx]], color=\"g\", alpha=0.2)\n",
    "    if idx > 0:\n",
    "        ts = timestamps[np.array((contig_stops[idx - 1], contig_starts[idx]))]\n",
    "        ax2.plot(ts, 0.01 * np.array((discont[idx - 1], discont[idx - 1])), c=\"k\", linestyle=\":\")\n",
    "    \n",
    "# for ln in res_y[:-1]:\n",
    "#     ax1.axvline(x=timestamps[ln], c=\"r\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "rms_slide_0 = np.zeros_like(rms0)\n",
    "rms_slide_1 = np.zeros_like(rms1)\n",
    "rms_avg_0 = np.zeros_like(starts, dtype=float)\n",
    "rms_avg_1 = np.zeros_like(starts, dtype=float)\n",
    "windows = np.zeros_like(rms0)\n",
    "windows_wide = np.zeros_like(rms0)\n",
    "rms_avg_0[2] = 1.5\n",
    "parity = np.zeros_like(rms0)\n",
    "\n",
    "for idx in range(starts.size):\n",
    "    lg = stops[idx] - starts[idx]\n",
    "    win = 1.0 / (1.0 + np.exp((np.abs(np.arange(lg) - lg / 2.0 - 0.5) + 4.0 - lg / 2.0).clip(min=-10.0)))\n",
    "    win_wide = 1.0 / (1.0 + np.exp((0.3 * np.abs(np.arange(lg) - lg / 2.0 - 0.5) + 5.0 - 0.3 * lg / 2.0).clip(min=-10.0)))\n",
    "    win_lop = 1.0 / (1.0 + np.exp((-0.015 * np.arange(lg) + 5.0).clip(min=-10.0)))\n",
    "    winsum = np.sum(win)\n",
    "    if winsum == 0:\n",
    "        print(idx, lg, starts[idx], stops[idx], starts.size)\n",
    "    windows[starts[idx]:stops[idx]] = win\n",
    "    windows_wide[starts[idx]:stops[idx]] = win_wide * win_lop\n",
    "    rms_avg_0[idx] = np.sqrt(np.sum(np.square(n[starts[idx]:stops[idx], 0]) * win) / winsum)\n",
    "    rms_avg_1[idx] = np.sqrt(np.sum(np.square(n[starts[idx]:stops[idx], 1]) * win) / winsum)\n",
    "    parity[idx] = np.sign(np.sum(n[starts[idx]:stops[idx], 0] * n[starts[idx]:stops[idx], 1]))\n",
    "    rms_slide_0[starts[idx]:stops[idx]] = rms_avg_0[idx]\n",
    "    rms_slide_1[starts[idx]:stops[idx]] = rms_avg_1[idx]\n",
    "    if idx == 0:\n",
    "        rms_slide_0[:starts[0]] = rms_avg_0[0]\n",
    "        rms_slide_1[:starts[0]] = rms_avg_1[0]\n",
    "    else:\n",
    "        lg = starts[idx] - stops[idx - 1]\n",
    "        rms_slide_0[stops[idx - 1]:starts[idx]] = np.linspace(rms_avg_0[idx - 1], rms_avg_0[idx], lg + 1)[:-1]\n",
    "        rms_slide_1[stops[idx - 1]:starts[idx]] = np.linspace(rms_avg_1[idx - 1], rms_avg_1[idx], lg + 1)[:-1]\n",
    "rms_slide_0[stops[-1]:] = rms_avg_0[-1]\n",
    "rms_slide_1[stops[-1]:] = rms_avg_1[-1]\n",
    "\n",
    "sig_norm_0 = n[:, 0] / rms_slide_0.clip(min=1e-9)\n",
    "sig_norm_1 = n[:, 1] / rms_slide_1.clip(min=1e-9)\n",
    "\n",
    "sqsecdrv = np.zeros_like(n)\n",
    "sqsecdrv[1:-1, 0] = np.square(sig_norm_0[:-2] + sig_norm_0[2:] - 2.0 * sig_norm_0[1:-1]) * 0.25\n",
    "sqsecdrv[1:-1, 1] = np.square(sig_norm_1[:-2] + sig_norm_1[2:] - 2.0 * sig_norm_1[1:-1]) * 0.25\n",
    "sqsecdrv = sqsecdrv.clip(max=4.0)\n",
    "\n",
    "spline0 = cspline1d(sig_norm_0, lamb=1)\n",
    "spline1 = cspline1d(sig_norm_1, lamb=1)\n",
    "noise0 = spline0 - sig_norm_0\n",
    "noise1 = spline1 - sig_norm_1\n",
    "b, a = butter(5, 1, fs=5, btype='low', analog=False)\n",
    "filtered = filtfilt(b, a, sig_norm_0)\n",
    "_, ax = plt.subplots(1)\n",
    "ax.plot(sig_norm_0)\n",
    "ax.plot(filtered)\n",
    "plt.show()\n",
    "\n",
    "noise0_rms = np.zeros_like(noise0)\n",
    "noise1_rms = np.zeros_like(noise0)\n",
    "flip0 = np.zeros_like(noise0)\n",
    "flip1 = np.zeros_like(noise0)\n",
    "keep = np.ones_like(starts, dtype=bool)\n",
    "slopes0 = np.zeros_like(starts, dtype=float)\n",
    "slopes1 = np.zeros_like(starts, dtype=float)\n",
    "snr0 = np.zeros_like(starts, dtype=float)\n",
    "snr1 = np.zeros_like(starts, dtype=float)\n",
    "diff_sl = np.zeros_like(starts, dtype=float)\n",
    "mult0 = np.zeros_like(starts, dtype=float)\n",
    "mult1 = np.zeros_like(starts, dtype=float)\n",
    "mid_mult0 = np.ones_like(rms_slide_0)\n",
    "mid_mult1 = np.ones_like(rms_slide_1)\n",
    "\n",
    "\n",
    "for idx in range(starts.size):\n",
    "    lg = stops[idx] - starts[idx]\n",
    "    win = 1.0 / (1.0 + np.exp((np.abs(np.arange(lg) - lg / 2.0 - 0.5) + 7.0 - lg / 2.0).clip(min=-10.0)))\n",
    "    winsum = np.sum(win)\n",
    "#     n0_val = np.sqrt(np.sum(np.square(noise0[starts[idx]:stops[idx]]) * win) / winsum)\n",
    "#     n1_val = np.sqrt(np.sum(np.square(noise1[starts[idx]:stops[idx]]) * win) / winsum)\n",
    "    n0_val = np.sum(sqsecdrv[starts[idx]:stops[idx], 0] * win) / winsum\n",
    "    n1_val = np.sum(sqsecdrv[starts[idx]:stops[idx], 1] * win) / winsum\n",
    "    per_val_0 = np.sum(r2_x[starts[idx]:stops[idx]] * win) / winsum\n",
    "    per_val_1 = np.sum(r2_y[starts[idx]:stops[idx]] * win) / winsum\n",
    "    noise0_rms[starts[idx]:stops[idx]] = n0_val\n",
    "    noise1_rms[starts[idx]:stops[idx]] = n1_val\n",
    "    m0 = spline0[starts[idx] + 1:stops[idx]] - spline0[starts[idx]:stops[idx] - 1]\n",
    "    m1 = spline1[starts[idx] + 1:stops[idx]] - spline1[starts[idx]:stops[idx] - 1]\n",
    "    slopes0[idx] = np.mean(m0[m0>0]) + np.mean(m0[m0<0])\n",
    "    slopes1[idx] = np.mean(m1[m1>0]) + np.mean(m1[m1<0])\n",
    "    diff_sl[idx] = np.sign(slopes0[idx] + parity[idx] * slopes1[idx])\n",
    "    snr0[idx] = n0_val\n",
    "    snr1[idx] = n1_val\n",
    "    if min(n0_val, n1_val) > 0.03:  # empirical value that does pretty well\n",
    "        windows[starts[idx]:stops[idx]] = 0.0\n",
    "        keep[idx] = False\n",
    "    mult0[idx] = np.exp(-35.0 * n0_val) * per_val_0\n",
    "    mult1[idx] = np.exp(-35.0 * n1_val) * per_val_1\n",
    "    mid_mult0[starts[idx]:stops[idx]] = mult0[idx]\n",
    "    mid_mult1[starts[idx]:stops[idx]] = mult1[idx]\n",
    "    flip0[starts[idx]:stops[idx]] = diff_sl[idx]\n",
    "    flip1[starts[idx]:stops[idx]] = diff_sl[idx] * parity[idx]\n",
    "    if idx == 0:\n",
    "        mid_mult0[:starts[0]] = mult0[idx]\n",
    "        mid_mult1[:starts[0]] = mult1[idx]\n",
    "        flip0[:starts[0]] = diff_sl[idx]\n",
    "        flip1[:starts[0]] = diff_sl[idx] * parity[idx]\n",
    "    else:\n",
    "        lg = starts[idx] - stops[idx - 1]\n",
    "        mid_mult0[stops[idx - 1]:starts[idx]] = np.linspace(mult0[idx - 1], mult0[idx], lg + 1)[:-1]\n",
    "        mid_mult1[stops[idx - 1]:starts[idx]] = np.linspace(mult1[idx - 1], mult1[idx], lg + 1)[:-1]\n",
    "        flip0[stops[idx - 1]:stops[idx-1] + lg // 2] = diff_sl[idx - 1]\n",
    "        flip1[stops[idx - 1]:stops[idx-1] + lg // 2] = diff_sl[idx - 1] * parity[idx - 1]\n",
    "        flip0[stops[idx-1] + lg // 2:starts[idx]] = diff_sl[idx]\n",
    "        flip1[stops[idx-1] + lg // 2:starts[idx]] = diff_sl[idx] * parity[idx]\n",
    "flip0[stops[-1]:] = diff_sl[-1]\n",
    "flip1[stops[-1]:] = diff_sl[-1] * parity[-1]\n",
    "\n",
    "starts = starts[keep]\n",
    "stops = stops[keep]\n",
    "slopes0 = slopes0[keep]\n",
    "slopes1 = slopes1[keep]\n",
    "rms_avg_0 = rms_avg_0[keep]\n",
    "rms_avg_1 = rms_avg_1[keep]\n",
    "snr0 = snr0[keep]\n",
    "snr1 = snr1[keep]\n",
    "mult0 = mult0[keep]\n",
    "mult1 = mult1[keep]\n",
    "\n",
    "for idx in range(starts.size):\n",
    "    rms_slide_0[starts[idx]:stops[idx]] = rms_avg_0[idx]\n",
    "    rms_slide_1[starts[idx]:stops[idx]] = rms_avg_1[idx]\n",
    "    mid_mult0[starts[idx]:stops[idx]] = mult0[idx]\n",
    "    mid_mult1[starts[idx]:stops[idx]] = mult1[idx]\n",
    "    if idx == 0:\n",
    "        rms_slide_0[:starts[0]] = rms_avg_0[0]\n",
    "        rms_slide_1[:starts[0]] = rms_avg_1[0]\n",
    "        mid_mult0[:starts[0]] = mult0[idx]\n",
    "        mid_mult1[:starts[0]] = mult1[idx]\n",
    "    else:\n",
    "        lg = starts[idx] - stops[idx - 1]\n",
    "        rms_slide_0[stops[idx - 1]:starts[idx]] = np.linspace(rms_avg_0[idx - 1], rms_avg_0[idx], lg + 1)[:-1]\n",
    "        rms_slide_1[stops[idx - 1]:starts[idx]] = np.linspace(rms_avg_1[idx - 1], rms_avg_1[idx], lg + 1)[:-1]\n",
    "        mid_mult0[stops[idx - 1]:starts[idx]] = np.linspace(mult0[idx - 1], mult0[idx], lg + 1)[:-1]\n",
    "        mid_mult1[stops[idx - 1]:starts[idx]] = np.linspace(mult1[idx - 1], mult1[idx], lg + 1)[:-1]\n",
    "rms_slide_0[stops[-1]:] = rms_avg_0[-1]\n",
    "rms_slide_1[stops[-1]:] = rms_avg_1[-1]\n",
    "mid_mult0[stops[-1]:] = mult0[-1]\n",
    "mid_mult1[stops[-1]:] = mult1[-1]\n",
    "\n",
    "sig_norm_0 = n[:, 0] / rms_slide_0.clip(min=1e-9)\n",
    "sig_norm_1 = n[:, 1] / rms_slide_1.clip(min=1e-9)\n",
    "\n",
    "omega = 6.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)  # indices ~50-85 are breathing frequencies\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)[30:90]\n",
    "x_wave = wavefinding_cwt(sig_norm_0 * windows, widths_morlet, omega)\n",
    "y_wave = wavefinding_cwt(sig_norm_1 * windows, widths_morlet, omega)\n",
    "mags_z = np.abs(x_wave) + np.abs(y_wave)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7c183ac-6aba-44ea-9fe3-b6fcc3ea6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "_, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True)\n",
    "ax1.plot(n[:, 0])\n",
    "ax1.plot(n[:, 1])\n",
    "ax1.plot(rms_slide_0)\n",
    "ax1.plot(rms_slide_1)\n",
    "ax1.legend([\"x\", \"y\", \"rmsx\", \"rmsy\"])\n",
    "ax2.plot(sig_norm_0 * windows)\n",
    "ax2.plot(sig_norm_1 * windows)\n",
    "ax2.plot(sig_norm_0.clip(min=-5.0, max=5.0), alpha=0.2)\n",
    "ax2.plot(sig_norm_1.clip(min=-5.0, max=5.0), alpha=0.2)\n",
    "# ax2.plot(flip0)\n",
    "# ax2.plot(flip1)\n",
    "# ax2.plot(sqsecdrv[:, 0] * windows)\n",
    "# ax2.plot(sqsecdrv[:, 1] * windows)\n",
    "# ax2.plot(r2_x * windows)\n",
    "# ax2.plot(r2_y * windows)\n",
    "# ax2.plot(mid_mult0)\n",
    "# ax2.plot(mid_mult1)\n",
    "ax2.plot(np.minimum(noise0_rms, noise1_rms) * 100.0)\n",
    "ax2.plot(filtered.clip(-4.9, 4.9))\n",
    "ax2.legend([\"sn0\", \"sn1\", \"c0\", \"c1\", \"r2_swap\", \"filt\", \"r2y\", \"mm0\", \"mm1\", \"n0\", \"n1\"])\n",
    "ax3.imshow(mags_z.clip(max=np.percentile(mags_z, 99.9)), aspect=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next up, we want to flip the signals upright\n",
    "# and then combine them into one signal (after adjusting for their SNR and r2 of their periodicity)\n",
    "# For each segment, cspline1d, determine if up-slopes or down-slopes have larger slope magnitudes\n",
    "# upright segments should have more gradual upwards slopes\n",
    "# (there's a slight pause after exhaling before inhaling, doesn't happen at inhale-exhale transition)\n",
    "plt.close(\"all\")\n",
    "\n",
    "angles = np.arctan2(mid_mult1, mid_mult0)\n",
    "\n",
    "sig0_scale = -n[:, 0] * flip0 * np.square(np.cos(angles)) / rms_slide_0.clip(min=1e-9)\n",
    "sig1_scale = -n[:, 1] * flip1 * np.square(np.sin(angles)) / rms_slide_1.clip(min=1e-9)\n",
    "sig = sig0_scale + sig1_scale\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.plot(sig_norm_0 * windows)\n",
    "ax.plot(sig_norm_1 * windows)\n",
    "ax.plot(sig * windows)\n",
    "ax.plot(np.square(np.cos(angles)) * windows)\n",
    "ax.plot(np.square(np.sin(angles)) * windows)\n",
    "ax.legend(\"sn0,sn1,comb,cos2,sin2\".split(\",\"))\n",
    "plt.show()\n",
    "\n",
    "# still TO DO:\n",
    "# funky shit going on with scales of signorm traces at 1050, ~4000, 114000, 115000, probably other places\n",
    "# 118800 is upside down\n",
    "# add in r2 from periodicity - not sure of the best way to do this - adding their effects? multiplying? It is exp so probs adding...\n",
    "# make sure at least that the parity is correct - get correlation between two signals and make sure that result follows\n",
    "# somehow 110000 gets into the periodic signal section?? How?? It's so angular and small\n",
    "# 95000 - high frequency but clearly noise. Should not have made it into periodic.\n",
    "# 140600 - also should not have made it in\n",
    "# 137200 - also should not have made it in\n",
    "# 142775 - nopers\n",
    "# 142600 - should have  made it in\n",
    "# 15350 - no\n",
    "# 14990 - no\n",
    "# 2100 - kinda no\n",
    "# 4600-4800 - no, out of bed\n",
    "# 6000 - no\n",
    "# 8950 - 9250 - no\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after combining, find breathing frequency\n",
    "omega = 5.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)  # indices ~50-85 are breathing frequencies\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "wave = wavefinding_cwt(sig * windows, widths_morlet, omega)\n",
    "wave = wavefinding_cwt(sig.clip(min=-2.5, max=2.5), widths_morlet, omega)\n",
    "\n",
    "angles = np.angle(wave)\n",
    "mags = np.abs(np.abs(wave))\n",
    "cols = np.mod(angles * 0.5 / np.pi + 0.5, 1)\n",
    "col_spec = cols - 0.5\n",
    "dcol = col_spec[:, 1:] - col_spec[:, :-1]\n",
    "dcol[dcol < -0.5] += 1.0\n",
    "dcol[dcol > 0.5] -= 1.0\n",
    "# dcols = cm.hsv(dcol.clip(-0.00625, 0.00625) * 80.0 + 0.5)[..., :3]\n",
    "# dcols *= (mags[:, 1:, None] / np.percentile(mags, 95.0)).clip(max=1.0)\n",
    "# cols = cm.hsv(cols)[..., :3]\n",
    "# cols *= mags[..., None]\n",
    "\n",
    "# ddcol = dcol[50:85] - dcol[49:84]\n",
    "scale = 500.0\n",
    "ddcol = dcol[1:] - dcol[:-1]\n",
    "sharpen_data = correlate1d(np.exp(-scale * np.abs(dcol)), np.exp(-0.5 * np.square((np.arange(101) - 50.0) / 25.0)), axis=1)\n",
    "rate_data = np.square(mags[50:90, 1:]) * ddcol[49:89].clip(0.0) * sharpen_data[50:90]\n",
    "inst_rate = np.sum(np.arange(50, 90)[:, None] * rate_data, axis=0) / np.sum(rate_data, axis=0).clip(1e-5)\n",
    "inst_var = np.sum(np.square(np.arange(50, 90)[:, None] - inst_rate) * rate_data, axis=0) / np.sum(rate_data, axis=0).clip(1e-5)\n",
    "inst_std = np.sqrt(inst_var)\n",
    "inst_std_sm = correlate(inst_std, np.ones(31), mode=\"same\")\n",
    "inst_rate_fix = np.copy(inst_rate)\n",
    "\n",
    "for idx in range(starts.size):\n",
    "    lgb = min(stops[idx] - starts[idx], 100)\n",
    "    lge = min(stops[idx - 1] - starts[idx - 1], 100)\n",
    "    begin_avg = np.mean(inst_rate[starts[idx]:starts[idx] + lgb])\n",
    "    if idx == 0:\n",
    "        start_side = begin_avg\n",
    "        inst_rate_fix[:starts[0]] = begin_avg\n",
    "    elif idx == starts.size - 1:\n",
    "        final_side = np.mean(inst_rate[stops[idx] - lge:stops[idx]])\n",
    "        inst_rate_fix[stops[-1]:] = final_side\n",
    "    else:\n",
    "        end_avg = np.mean(inst_rate[stops[idx - 1] - lge:stops[idx - 1]])\n",
    "        space = starts[idx] - stops[idx - 1]\n",
    "        inst_rate_fix[stops[idx - 1]:starts[idx]] = np.linspace(end_avg, begin_avg, space + 1)[:-1]\n",
    "\n",
    "inst_rate_fix = cspline1d(inst_rate_fix, 0.1)\n",
    "inst_rate_sm = cspline1d(inst_rate_fix, 1e10)\n",
    "        \n",
    "stats_size = 1300\n",
    "interp_rate = np.concatenate((np.ones(stats_size) * start_side, inst_rate_fix, np.ones(stats_size) * final_side))\n",
    "\n",
    "curve_size = 15\n",
    "curve_fw = curve_size * 2 + 1\n",
    "x = np.arange(curve_fw) - curve_size\n",
    "A = np.stack((np.square(x), x, np.ones(curve_fw)), axis=1)\n",
    "krn = np.dot(np.linalg.inv(np.dot(A.T, A)), A.T)\n",
    "gskrn = np.exp(-0.5 * np.square((np.arange(401) - 200)/100))\n",
    "\n",
    "crv = np.square(100.0 * correlate(interp_rate, krn[0], mode=\"same\")[stats_size:-stats_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239dacb3-295a-4df4-b8fe-a4fe56c8fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "sm = np.array([1.0, 4.0, 6.0, 4.0, 1.0])\n",
    "gr = np.array([-1.0, -2.0, 0.0, 2.0, 1.0])\n",
    "# sm = np.array([1.0, 6.0, 15.0, 20.0, 15.0, 6.0, 1.0])\n",
    "# gr = np.array([-1.0, -4.0, -5.0, 0.0, 5.0, 4.0, 1.0])\n",
    "\n",
    "# gx = sepfir2d(ddcol, gr, sm)\n",
    "# gy = sepfir2d(ddcol, sm, gr)\n",
    "\n",
    "# g2 = np.square(gx) + np.square(gy)\n",
    "ffff = np.square(mags[1:, 1:])#ddcol * sharpen_data[1:] * \n",
    "\n",
    "# ctd = ffff - np.mean(ffff, axis=1, keepdims=True)\n",
    "cov = np.cov(ffff)\n",
    "evals, evecs = linalg.eigh(cov)\n",
    "evecs = evecs[:, -20:]\n",
    "something = np.dot(evecs, np.dot(evecs.T, ffff))\n",
    "print(something.shape)\n",
    "\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.imshow(something, aspect=\"auto\")\n",
    "plt.show()\n",
    "\n",
    "_, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True)\n",
    "ax1.imshow((ddcol * scale).clip(0.0, 1.0), aspect=\"auto\")\n",
    "# ax1.imshow(dcols, aspect=\"auto\")\n",
    "# ax1.imshow(cols, aspect=\"auto\")\n",
    "# ax2.imshow(sharpen_data, aspect=\"auto\")\n",
    "# ax2.imshow(np.exp(-50 * g2), aspect=\"auto\")\n",
    "ax2.imshow(ffff.clip(min=0.0, max=np.percentile(ffff, 99.0)), aspect=\"auto\")\n",
    "ax3.imshow(np.square(mags).clip(max=np.percentile(np.square(mags), 99.0)), aspect=\"auto\")\n",
    "ax4.imshow(rate_data.clip(max=np.percentile(rate_data, 99.0)), aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7ea0900-9fdb-445a-a848-f0e5a00f2a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring 0 ( 9 )\n",
      "ignoring 3 ( 38 )\n",
      "ignoring 4 ( 5 )\n",
      "ignoring 6 ( 33 )\n",
      "ignoring 8 ( 26 )\n",
      "ignoring 9 ( 23 )\n",
      "ignoring 11 ( 26 )\n",
      "ignoring 14 ( 43 )\n",
      "ignoring 16 ( 4 )\n",
      "ignoring 17 ( 48 )\n",
      "ignoring 20 ( 33 )\n",
      "ignoring 21 ( 3 )\n",
      "ignoring 22 ( 3 )\n",
      "ignoring 24 ( 48 )\n",
      "ignoring 25 ( 44 )\n",
      "ignoring 27 ( 9 )\n",
      "ignoring 30 ( 14 )\n",
      "ignoring 36 ( 22 )\n",
      "ignoring 37 ( 17 )\n",
      "ignoring 39 ( 21 )\n",
      "ignoring 40 ( 17 )\n",
      "ignoring 41 ( 8 )\n",
      "ignoring 42 ( 26 )\n",
      "ignoring 43 ( 17 )\n",
      "ignoring 45 ( 12 )\n",
      "ignoring 47 ( 46 )\n",
      "ignoring 48 ( 21 )\n",
      "ignoring 49 ( 18 )\n",
      "ignoring 52 ( 19 )\n",
      "ignoring 56 ( 18 )\n",
      "ignoring 57 ( 47 )\n",
      "ignoring 59 ( 7 )\n",
      "ignoring 61 ( 3 )\n",
      "ignoring 64 ( 37 )\n",
      "ignoring 67 ( 38 )\n",
      "ignoring 68 ( 41 )\n",
      "ignoring 69 ( 18 )\n",
      "ignoring 70 ( 27 )\n",
      "ignoring 71 ( 35 )\n",
      "ignoring 76 ( 21 )\n",
      "ignoring 80 ( 15 )\n",
      "ignoring 81 ( 25 )\n",
      "ignoring 82 ( 49 )\n",
      "ignoring 83 ( 37 )\n",
      "ignoring 84 ( 9 )\n",
      "ignoring 86 ( 36 )\n",
      "ignoring 89 ( 17 )\n",
      "ignoring 90 ( 4 )\n",
      "ignoring 95 ( 28 )\n",
      "ignoring 97 ( 41 )\n",
      "ignoring 98 ( 7 )\n",
      "ignoring 101 ( 3 )\n",
      "ignoring 102 ( 3 )\n",
      "ignoring 103 ( 16 )\n",
      "ignoring 104 ( 3 )\n",
      "ignoring 106 ( 17 )\n",
      "ignoring 107 ( 17 )\n",
      "ignoring 108 ( 36 )\n",
      "ignoring 109 ( 8 )\n",
      "ignoring 110 ( 39 )\n",
      "ignoring 112 ( 18 )\n",
      "ignoring 113 ( 48 )\n",
      "ignoring 114 ( 25 )\n",
      "ignoring 116 ( 15 )\n",
      "ignoring 121 ( 28 )\n",
      "ignoring 122 ( 20 )\n",
      "ignoring 123 ( 3 )\n",
      "ignoring 125 ( 33 )\n",
      "ignoring 126 ( 3 )\n",
      "ignoring 127 ( 11 )\n",
      "ignoring 129 ( 16 )\n",
      "ignoring 130 ( 13 )\n",
      "ignoring 133 ( 23 )\n",
      "ignoring 135 ( 3 )\n",
      "ignoring 136 ( 10 )\n",
      "ignoring 137 ( 3 )\n",
      "ignoring 139 ( 3 )\n",
      "ignoring 141 ( 48 )\n",
      "ignoring 143 ( 49 )\n",
      "ignoring 144 ( 25 )\n",
      "ignoring 146 ( 26 )\n",
      "ignoring 149 ( 28 )\n",
      "ignoring 150 ( 10 )\n",
      "ignoring 152 ( 9 )\n",
      "ignoring 154 ( 3 )\n",
      "ignoring 157 ( 49 )\n",
      "ignoring 158 ( 39 )\n",
      "ignoring 159 ( 31 )\n",
      "0 [] []\n",
      "1 [] []\n",
      "2 [] []\n",
      "3 [] []\n",
      "4 [] []\n",
      "5 [] []\n",
      "6 [] []\n",
      "7 [] []\n",
      "8 [] []\n",
      "9 [] []\n",
      "10 [] []\n",
      "11 [] []\n",
      "12 [] []\n",
      "13 [] []\n",
      "14 [] []\n",
      "15 [] []\n",
      "16 [] []\n",
      "17 [] []\n",
      "18 [] []\n",
      "19 [] []\n",
      "20 [] []\n",
      "21 [] []\n",
      "22 [] []\n",
      "23 [] []\n",
      "24 [] []\n",
      "25 [] []\n",
      "26 [] []\n",
      "27 [] []\n",
      "454\n",
      "deleting 0\n",
      "214\n",
      "deleting 1\n",
      "162\n",
      "deleting 2\n",
      "128\n",
      "deleting 3\n",
      "554\n",
      "262\n",
      "deleting 5\n",
      "1217\n",
      "273\n",
      "deleting 7\n",
      "262\n",
      "deleting 8\n",
      "118\n",
      "deleting 9\n",
      "17\n",
      "deleting 10\n",
      "235\n",
      "deleting 11\n",
      "842\n",
      "298\n",
      "deleting 13\n",
      "222\n",
      "deleting 14\n",
      "641\n",
      "216\n",
      "deleting 16\n",
      "535\n",
      "287\n",
      "deleting 18\n",
      "64\n",
      "deleting 19\n",
      "111\n",
      "deleting 20\n",
      "365\n",
      "119\n",
      "deleting 22\n",
      "1099\n",
      "740\n",
      "203\n",
      "deleting 25\n",
      "317\n",
      "deleting 26\n",
      "224\n",
      "deleting 27\n",
      "8\n",
      "[ 48706  51832 109070 113767 117554 123050 138702 141887]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.close(\"all\")\n",
    "\n",
    "slp = np.square(10.0 * correlate(interp_rate, krn[1], mode=\"same\"))[curve_fw:-curve_fw]\n",
    "slp[stats_size-curve_fw:curve_fw-stats_size] *= windows_wide[:-1]\n",
    "slp_sm = correlate(slp, gskrn, mode=\"same\")[curve_fw:-curve_fw] / np.sum(gskrn)\n",
    "\n",
    "# thoughts:\n",
    "# if slp_sm > 0.85 (empirical) then possibly counts\n",
    "# Join together groups of those which are > 0.85 if they're within a certain distance\n",
    "# If not large enough, get rid of it.\n",
    "# But then there's also a physiology question...\n",
    "# if there's a good bout of movement, you probably don't go into REM right away? Or no...\n",
    "rem_starts = np.argwhere(np.logical_and(slp_sm[1:] >= 0.85, slp_sm[:-1] < 0.85)).squeeze()\n",
    "rem_stops = np.argwhere(np.logical_and(slp_sm[1:] < 0.85, slp_sm[:-1] >= 0.85)).squeeze()\n",
    "\n",
    "keep = np.ones(starts.size + 1, dtype=bool)\n",
    "for idx in range(starts.size - 1):\n",
    "    rem_lg = starts[idx + 1] - stops[idx]\n",
    "    if rem_lg <= 50:\n",
    "        keep[idx + 1] = False\n",
    "        print(\"ignoring\", idx, \"(\", rem_lg, \")\")\n",
    "\n",
    "ns_starts = starts[keep[:-1]] + stats_size\n",
    "ns_stops = stops[keep[1:]] + stats_size\n",
    "\n",
    "keep = np.ones(rem_starts.size + 1, dtype=bool)\n",
    "for idx in range(rem_starts.size - 1):\n",
    "    gap_lg = rem_starts[idx + 1] - rem_stops[idx]\n",
    "    if gap_lg <= 360:\n",
    "        keep[idx + 1] = False\n",
    "\n",
    "rem_starts = rem_starts[keep[:-1]]\n",
    "rem_stops = rem_stops[keep[1:]]\n",
    "\n",
    "rem_starts_cut = []\n",
    "rem_stops_cut = []\n",
    "\n",
    "# 6 possibilities:\n",
    "# starts and stops without any gaps - accept both start and stop.\n",
    "# starts within a gap, gap ends before stop - move the start to the end of the gap.\n",
    "# gap starts after start, before stop, gap ends after stop - move stop up to the start of the gap.\n",
    "# gap encompasses start and stop - delete start and stop.\n",
    "# start and stop encompass gap - add in gap stop and start into start and stop.\n",
    "# gap ends after start and a new one begins before stop - cut off the start and the end.\n",
    "\n",
    "for idx in range(rem_starts.size):\n",
    "    starts_in_gap = np.logical_and(ns_starts > rem_starts[idx], ns_starts < rem_stops[idx])\n",
    "    stops_in_gap = np.logical_and(ns_stops > rem_starts[idx], ns_stops < rem_stops[idx])\n",
    "    starts_in_gap_args = np.argwhere(starts_in_gap).squeeze(1)\n",
    "    stops_in_gap_args = np.argwhere(stops_in_gap).squeeze(1)\n",
    "    print(idx, starts_in_gap_args, stops_in_gap_args)\n",
    "    \n",
    "    if not (np.any(starts_in_gap) or np.any(stops_in_gap)):\n",
    "        last_start = np.argmax(ns_starts <= rem_starts[idx])\n",
    "        if ns_stops[last_start] < rem_starts[idx]:\n",
    "            rem_starts_cut.append(rem_starts[idx])\n",
    "            rem_stops_cut.append(rem_stops[idx])\n",
    "        continue\n",
    "    \n",
    "    if np.count_nonzero(starts_in_gap) == np.count_nonzero(stops_in_gap):\n",
    "        if np.all(starts_in_gap_args == stops_in_gap_args):\n",
    "            # we have gaps to open up in the thing, but all of them are contained\n",
    "            rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "            rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "        else:\n",
    "            # keep the start and end\n",
    "            rem_starts_cut.append(rem_starts[idx])\n",
    "            rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "            rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "            rem_stops_cut.append(rem_stops[idx])\n",
    "        continue\n",
    "    elif np.count_nonzero(starts_in_gap) == 0:\n",
    "        rem_starts_cut.append(rem_starts[idx])\n",
    "        rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "    elif np.count_nonzero(stops_in_gap) == 0:\n",
    "        rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "        rem_stops_cut.append(rem_stops[idx])\n",
    "    else:\n",
    "        if starts_in_gap_args[0] == stops_in_gap_args[0]:\n",
    "            rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "            rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "            rem_stops_cut.append(rem_stops[idx])\n",
    "        else:\n",
    "            rem_starts_cut.append(rem_starts[idx])\n",
    "            rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "            rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "        continue\n",
    "\n",
    "rem_starts = np.array(rem_starts_cut)\n",
    "rem_stops = np.array(rem_stops_cut)\n",
    "\n",
    "m_lead, m_lag, b_lead, b_lag, rmse_lead, rmse_lag = fit_linear_lead_and_lag(interp_rate, stats_size + 1)\n",
    "\n",
    "rmse_cutoff = (np.minimum(rmse_lead[stats_size:-stats_size], rmse_lag[stats_size:-stats_size]) - 2.2).clip(min=0.0)\n",
    "possible_rem = correlate(rmse_cutoff, gskrn, mode=\"same\") / np.sum(gskrn)\n",
    "\n",
    "keep = np.ones(rem_starts.size, dtype=bool)\n",
    "\n",
    "for idx in range(rem_starts.size):\n",
    "    rem_lg = rem_stops[idx] - rem_starts[idx]\n",
    "    print(rem_lg)\n",
    "    if rem_lg <= 360 or not np.any(possible_rem[rem_starts[idx] - stats_size:rem_stops[idx] - stats_size] > 0.075) or not np.any(inst_rate_sm[rem_starts[idx] - stats_size:rem_stops[idx] - stats_size] > 66.0):\n",
    "        keep[idx] = False\n",
    "        print(\"deleting\", idx)\n",
    "\n",
    "rem_starts = rem_starts[keep] - stats_size\n",
    "rem_stops = rem_stops[keep] - stats_size\n",
    "        \n",
    "print(rem_starts.size)\n",
    "\n",
    "slp = slp[stats_size-curve_fw:curve_fw-stats_size]\n",
    "slp_sm = slp_sm[stats_size-curve_fw:curve_fw-stats_size]\n",
    "\n",
    "_, (ax3, ax4) = plt.subplots(2, 1, sharex=True)\n",
    "ax3.imshow(np.square(mags).clip(max=np.percentile(np.square(mags), 99.0)), aspect=\"auto\")\n",
    "ax4.imshow(rate_data.clip(max=np.percentile(rate_data, 99.0)), aspect=\"auto\")\n",
    "ax4.plot((inst_rate - 50.0) * np.where(windows[:-1] > 0.0, 1.0, np.nan), c=\"r\", linestyle=\":\")\n",
    "ax4.plot(inst_std_sm * 0.04, c=\"y\", linestyle=\":\")\n",
    "ax4.plot(crv + 30, c=\"g\")\n",
    "ax4.plot(slp + 40, c=\"m\")\n",
    "ax4.plot(slp_sm + 40, c=\"b\")\n",
    "#// ax3.plot(m_lead[stats_size:-stats_size] + 100, c=\"r\", linestyle=\":\")\n",
    "#// ax3.plot(m_lag[stats_size:-stats_size] + 100, c=\"r\", linestyle=\":\")\n",
    "ax3.plot(np.minimum(rmse_lead[stats_size:-stats_size], rmse_lag[stats_size:-stats_size]).clip(0.0, 10.0) * 10.0, c=\"g\", linestyle=\":\")\n",
    "ax3.plot(np.maximum(rmse_lead[stats_size:-stats_size], rmse_lag[stats_size:-stats_size]) / np.minimum(rmse_lead[stats_size:-stats_size], rmse_lag[stats_size:-stats_size]).clip(1.5e-1) * 10.0, c=\"m\", linestyle=\":\")\n",
    "ax3.plot(possible_rem * 40.0, c=\"y\", linestyle=\":\")\n",
    "ax4.plot(windows_wide * 5, c=\"b\")\n",
    "ax3.plot(inst_rate_fix, c=\"r\")\n",
    "ax3.plot(inst_rate_sm, c=\"g\")\n",
    "ax3.plot((inst_rate_sm[1:] - inst_rate_sm[:-1]) * 1000 + 100, c=\"r\")\n",
    "\n",
    "\n",
    "for idx in range(rem_starts.size):\n",
    "    ax4.axvspan(rem_starts[idx], rem_stops[idx], color=\"g\", alpha=0.2)\n",
    "\n",
    "for idx in range(res_x.size):\n",
    "    ax3.axvline(x=res_x[idx])\n",
    "# for idx in range(ns_starts.size):\n",
    "#     ax3.axvspan(ns_starts[idx] - stats_size, ns_stops[idx] - stats_size, color=\"g\", alpha=0.2)\n",
    "    \n",
    "print(rem_starts)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(10, 10))\n",
    "# ax1.imshow(mags.clip(max=np.percentile(mags, 95.0)), aspect=\"auto\")\n",
    "# ax2.plot(final_sm)\n",
    "# ax2.scatter(apneas, final_sm[apneas], c=\"k\", marker=\"+\", zorder=1000)\n",
    "# ax3.imshow(dcols, aspect=\"auto\")\n",
    "# plt.show()\n",
    "\n",
    "# _, ax = plt.subplots(1, 1)\n",
    "# ax.hist(pk_dist * 0.2, bins=120, range=(0.1, 24.1))\n",
    "# plt.show()\n",
    "\n",
    "# _, ax = plt.subplots(1, 1, sharex=True)\n",
    "# ax.plot(timestamps, final)\n",
    "# plt.show()\n",
    "\n",
    "# plt.close(\"all\")\n",
    "# _, ax = plt.subplots(1, 1)\n",
    "# ax.imshow(mags_z.clip(max=np.percentile(mags_z, 99.9)), aspect=\"auto\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cc4d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056396344023511936\n",
      "[ 77820  78148 152156 153199]\n",
      "[ 77875  78206 152210 153310]\n"
     ]
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "# classify stages of wakefulness and sleep, movement, and out-of-bed periods\n",
    "nps = -n[:, 0] * flip0 - n[:, 1] * flip1\n",
    "non_per = np.where(windows == 0.0, sig, np.nan)\n",
    "print(np.count_nonzero(windows[7000:] == 0.0) / np.size(windows[7000:]))\n",
    "winsize = 51\n",
    "mse = correlate(np.square(nps), np.ones(winsize), mode=\"full\") / winsize\n",
    "mse_lead = mse[:1 - winsize]\n",
    "mse_lag = mse[winsize - 1:]\n",
    "sm_min_mse = np.correlate(np.sqrt(np.minimum(mse_lead, mse_lag)), np.ones(1501), mode=\"same\") / 1501\n",
    "sm_max_mse = np.correlate(np.sqrt(np.maximum(mse_lead, mse_lag)), np.ones(1501), mode=\"same\") / 1501\n",
    "mins = np.where(windows == 0.0, np.sqrt(np.minimum(mse_lead, mse_lag)), np.nan)\n",
    "maxs = np.where(windows == 0.0, np.sqrt(np.maximum(mse_lead, mse_lag)), np.nan)\n",
    "prob_mvmt = 1.0 / (1.0 + np.exp(4.0 - 2.0 * sm_max_mse))\n",
    "prob_nib = 1.0 / (1.0 + np.exp(6.0 * sm_min_mse - 4.5))\n",
    "nib_cond = np.logical_and(np.sqrt(np.minimum(mse_lead, mse_lag)) < 1e-4, windows == 0.0)\n",
    "nib_maybe = np.logical_and(np.sqrt(np.minimum(mse_lead, mse_lag)) < 2e-4, windows == 0.0)\n",
    "nib_cond[0] = False\n",
    "nib_cond[-1] = False\n",
    "nib_starts = np.argwhere(np.logical_and(nib_cond[1:], np.logical_not(nib_cond[:-1]))).squeeze()\n",
    "nib_stops = np.argwhere(np.logical_and(nib_cond[:-1], np.logical_not(nib_cond[1:]))).squeeze()\n",
    "\n",
    "# to properly categorize me as being out of bed...\n",
    "# first, take the windows *between* where I'm classified as moving/in bed/etc\n",
    "# if they're a) pretty short and b) don't go too much above the threshold and c) if they do, it's brief\n",
    "# then remove that stop and next start to join them up\n",
    "# finally, if any window is less than 10 seconds, it doesn't count.\n",
    "# Apneas might also be caught by this (and the under-10-second segments might also be apneas)\n",
    "# worth flagging anything that might fit!\n",
    "# also, out-of-bed signals probably need to be bracketed  by movement on either side...\n",
    "# getting movement right is important.\n",
    "\n",
    "# movement is anything that's not periodic, and has larger amplitude than everything around it.\n",
    "# I'm not sure what the shortest duration movement could be...\n",
    "# for sure as low as 1 second, but more than, say, 2 frames?\n",
    "\n",
    "# there is one other condition, where the signal was definitely part of the breathing signal\n",
    "# but was not really periodic - some abnormality. Worth flagging those.\n",
    "# not always something that weird... sometimes just a breath that I didn't pause at the bottom, or did\n",
    "# or a particularly fast inhale or exhale, like a sigh.\n",
    "# How do we tell?\n",
    "# first of all, they're short - typically 2 seconds or less.\n",
    "# second of all, they aren't significantly bigger or smaller than the surrounding signal, about the same rms\n",
    "# might be worth, once I collect enough data, classifying them with a machine learning classifier of sorts\n",
    "\n",
    "keep = nib_stops - nib_starts > 50\n",
    "nib_starts = nib_starts[keep]\n",
    "nib_stops = nib_stops[keep]\n",
    "print(nib_starts)\n",
    "print(nib_stops)\n",
    "\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.plot(sig, alpha=0.2, linestyle=\":\")\n",
    "ax.plot(non_per)\n",
    "# ax.plot(prob_mvmt, linestyle=\":\")\n",
    "# ax.plot(prob_nib, linestyle=\":\")\n",
    "ax.plot(mins)\n",
    "ax.plot(maxs)\n",
    "ax.plot(nib_cond.astype(float))\n",
    "ax.legend([\"sig\", \"sig nonper\", \"min\", \"max\", \"notinbed\"])\n",
    "\n",
    "for idx in range(nib_starts.size):\n",
    "    ax.axvspan(nib_starts[idx], nib_stops[idx], color=\"g\", alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.scatter(mins, maxs)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "# the next step is to categorize the periodic segments\n",
    "# Artificially segment sleep segments into overlapping chunks\n",
    "# try to classify everything within each chunk - maybe assign probability to all classifications somehow\n",
    "# for each overlap piece, choose the highest overall probability weighted classification\n",
    "# this happens with at least three statistics - mean, slope and stderr of the breathing frequency\n",
    "# for a decreasing slope with fairly small stderr, it's falling asleep\n",
    "# for flat slope, small stderr, low frequency, it's deep sleep\n",
    "# for moderate slope, high stderr, REM sleep\n",
    "# for mild slope, low to moderate stderr, high frequency, it's awake\n",
    "# not sure how to categorize \"light sleep\", I don't have enough data yet\n",
    "# I think I can probably use the breathing style as well\n",
    "# ie how much time does each segment spend near 0 compared to at its peaks\n",
    "# I think when I'm awake, I don't spend very much time in the exhaled state\n",
    "# whereas in deep sleep, there very often is a pause between one exhale and the next inhale\n",
    "\n",
    "# how to do probability? First off, decide on a threshold and sharpness, use sigmoid to determine which side\n",
    "# each factor I measure can then be given eg a bayes factor or something. This gets tedious but it would be good\n",
    "# give each type of sleep a weight with which it corresponds to that factor - eg low frequency is critical for deep\n",
    "# the pause between breaths is a good signal for deep sleep, but only about 80% confidence. Reverse for awake, roughly\n",
    "# etc.\n",
    "\n",
    "# can I do this for the non-periodic stuff too? Two factors: duration, and rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find apneas, and cheyne-stokes style breathing\n",
    "# ie this is peak picking - can we use scipy or do I need to do it manually by breathing freq (sliding min/max filts)?\n",
    "\n",
    "# around 55900 there's a weird bit\n",
    "# 36600 cheynish\n",
    "# 338750 - cheynish, also some got excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20153800",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flip_components_indiv_and_combine() missing 1 required positional argument: 'segments'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10928\\3858809777.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflip_components_indiv_and_combine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfinal_sm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcspline1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_peaks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_sm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprominence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpk_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: flip_components_indiv_and_combine() missing 1 required positional argument: 'segments'"
     ]
    }
   ],
   "source": [
    "\n",
    "final = flip_components_indiv_and_combine(n[:, :2], res_x)\n",
    "final_sm = cspline1d(final, lamb=5.0)\n",
    "\n",
    "pks, *_ = find_peaks(final_sm, prominence=0.7)\n",
    "pk_dist = pks[1:] - pks[:-1]\n",
    "apneas = pks[:-1][pk_dist > 30]\n",
    "\n",
    "lgth = final.shape[0]\n",
    "omega = 10.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "cwt_morlet = wavefinding_cwt(final, widths_morlet, omega)\n",
    "angles = np.angle(cwt_morlet)\n",
    "mags = np.square(np.abs(cwt_morlet))\n",
    "cols = np.mod(angles * 0.5 / np.pi + 0.5, 1)\n",
    "col_spec = cols - 0.5\n",
    "dcol = col_spec[:, 1:] - col_spec[:, :-1]\n",
    "dcol[dcol < -0.5] += 1.0\n",
    "dcol[dcol > 0.5] -= 1.0\n",
    "dcols = cm.hsv(dcol.clip(-0.00625, 0.00625) * 80.0 + 0.5)[..., :3]\n",
    "dcols *= (mags[:, 1:, None] / np.percentile(mags, 95.0)).clip(max=1.0)\n",
    "cols = cm.hsv(cols)[..., :3]\n",
    "cols *= mags[..., None]\n",
    "\n",
    "# ddcol = dcol[50:85] - dcol[49:84]\n",
    "scale = 500.0\n",
    "ddcol = dcol[1:] - dcol[:-1]\n",
    "rate_data = mags[50:85, 1:] * ddcol[49:84].clip(0.0) * np.exp(-scale * np.abs(dcol[50:85]))\n",
    "inst_rate = np.sum(np.arange(50, 85)[:, None] * rate_data, axis=0) / np.sum(rate_data, axis=0).clip(1e-5)\n",
    "inst_var = np.sum(np.square(np.arange(50, 85)[:, None] - inst_rate) * rate_data, axis=0) / np.sum(rate_data, axis=0).clip(1e-5)\n",
    "inst_std = np.sqrt(inst_var)\n",
    "inst_std_sm = correlate(inst_std, np.ones(31), mode=\"same\")\n",
    "\n",
    "_, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True)\n",
    "ax1.imshow((ddcol * scale).clip(0.0, 1.0), aspect=\"auto\")\n",
    "ax2.imshow(np.exp(-scale * np.abs(dcol)), aspect=\"auto\")\n",
    "ax3.imshow(mags.clip(max=np.percentile(mags, 98.0)), aspect=\"auto\")\n",
    "ax4.imshow(rate_data, aspect=\"auto\")\n",
    "ax4.plot(inst_rate - 50.0, c=\"r\", linestyle=\":\")\n",
    "# ax4.plot(inst_std_sm, c=\"y\", linestyle=\":\")\n",
    "plt.show()\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(10, 10))\n",
    "ax1.imshow(mags.clip(max=np.percentile(mags, 95.0)), aspect=\"auto\")\n",
    "ax2.plot(final_sm)\n",
    "ax2.scatter(apneas, final_sm[apneas], c=\"k\", marker=\"+\", zorder=1000)\n",
    "ax3.imshow(dcols, aspect=\"auto\")\n",
    "plt.show()\n",
    "\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.hist(pk_dist * 0.2, bins=120, range=(0.1, 24.1))\n",
    "plt.show()\n",
    "\n",
    "_, ax = plt.subplots(1, 1, sharex=True)\n",
    "ax.plot(timestamps, final)\n",
    "plt.show()\n",
    "\n",
    "quit()\n",
    "\n",
    "n1 = norm_complex(n0_resz, n1_resz)\n",
    "n2 = norm_complex(n2_resz, n3_resz)\n",
    "\n",
    "n1 = smooth_and_norm_complex_stitch(n0_resz, n1_resz, n2_resz, n3_resz)\n",
    "\n",
    "m1, r1, res1, skew1 = compute_stats(n[:, 0], n[:, 1], winsize)\n",
    "m2, r2, res2, skew2 = compute_stats(n[:, 2], n[:, 3], winsize)\n",
    "\n",
    "# sig1 = smooth_and_norm(res1)\n",
    "# sig2 = smooth_and_norm(res2)\n",
    "\n",
    "\n",
    "# %%\n",
    "# CWT\n",
    "\n",
    "lgth = n1.shape[0]\n",
    "omega = 2.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "cwt_morlet = wavefinding_cwt(n1, widths_morlet, omega)\n",
    "angles = np.angle(cwt_morlet)\n",
    "mags = np.square(np.abs(cwt_morlet))\n",
    "cols = np.mod(angles * 0.5 / np.pi + 0.5, 1)\n",
    "col_spec = cols - 0.5\n",
    "dcol = col_spec[:, 1:] - col_spec[:, :-1]\n",
    "dcol[dcol < -0.5] += 1.0\n",
    "dcol[dcol > 0.5] -= 1.0\n",
    "dcols = cm.hsv(dcol.clip(-0.0125, 0.0125) * 40.0 + 0.5)[..., :3]\n",
    "# dcols = cm.twilight_shifted(dcol.clip(-0.05, 0.05) * 10.0 + 0.5)[..., :3]\n",
    "dcols *= (mags[:, 1:, None] / np.percentile(mags, 95.0)).clip(max=1.0)\n",
    "cols = cm.hsv(cols)[..., :3]\n",
    "cols *= mags[..., None]\n",
    "\n",
    "sharps = np.sum(mags[:50], axis=0)\n",
    "sharps *= 0.001  # empirical constant\n",
    "sharps[0] = 0.0\n",
    "sharps[-1] = 0.0\n",
    "\n",
    "fatties = np.sum(mags[-50:], axis=0)\n",
    "fatties *= 0.002\n",
    "fatties[0] = 0.0\n",
    "fatties[-1] = 0.0\n",
    "\n",
    "mainline = np.sum(mags[50:-50], axis=0)\n",
    "mainline *= 0.004\n",
    "mainline[0] = 1.0\n",
    "mainline[-1] = 1.0\n",
    "\n",
    "summed = np.sum(mags, axis=0, keepdims=True)\n",
    "wt_mean = np.sum(np.arange(freqs.size)[:, None] * mags, axis=0, keepdims=True) / summed\n",
    "stdev = np.sqrt(\n",
    "    np.sum(mags * np.square(np.arange(freqs.size)[:, None] - wt_mean), axis=0) / summed\n",
    ")\n",
    "\n",
    "movement = np.logical_or(sharps > 0.5, np.abs(n1) > 5.00).astype(int)\n",
    "stationary = (mainline < 0.5).astype(int)\n",
    "\n",
    "mvmt_start = np.argwhere(movement[1:] - movement[:-1] == 1).squeeze()\n",
    "mvmt_end = np.argwhere(movement[1:] - movement[:-1] == -1).squeeze()\n",
    "stn_start = np.concatenate(\n",
    "    (\n",
    "        np.array([-2]),\n",
    "        np.argwhere(stationary[1:] - stationary[:-1] == 1).squeeze(),\n",
    "        np.array([lgth]),\n",
    "    )\n",
    ")\n",
    "stn_end = np.concatenate(\n",
    "    (\n",
    "        np.array([-1]),\n",
    "        np.argwhere(stationary[1:] - stationary[:-1] == -1).squeeze(),\n",
    "        np.array([lgth + 1]),\n",
    "    )\n",
    ")\n",
    "\n",
    "before_args = np.argmax(\n",
    "    np.mod(stn_end[:, None] - mvmt_start[None, :], lgth * 2), axis=0\n",
    ")\n",
    "before_okay = np.logical_and(\n",
    "    0 < mvmt_start - stn_end[before_args], mvmt_start - stn_end[before_args] < 15\n",
    ")\n",
    "before_dist = (mvmt_start - stn_start[before_args]) * before_okay.astype(int)\n",
    "mvmt_start_adj = mvmt_start - before_dist\n",
    "\n",
    "after_args = np.argmax(np.mod(mvmt_end[None, :] - stn_start[:, None], lgth * 2), axis=0)\n",
    "after_okay = np.logical_and(\n",
    "    0 < stn_start[after_args] - mvmt_end, stn_start[after_args] - mvmt_end < 15\n",
    ")\n",
    "after_dist = (stn_end[after_args] - mvmt_end) * after_okay.astype(int)\n",
    "mvmt_end_adj = mvmt_end + after_dist\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(10, 10))\n",
    "ax1.imshow(mags.clip(max=np.percentile(mags, 95.0)), aspect=\"auto\")\n",
    "ax2.plot(np.real(n1))\n",
    "ax2.plot(np.imag(n1))\n",
    "ax2.plot(np.abs(n1))\n",
    "ax2.plot(sharps.clip(max=2.0) * 10.0, linestyle=\":\")\n",
    "# ax2.plot(mainline, c=\"k\")\n",
    "# ax2.plot(stdev[0], c=\"b\")\n",
    "# ax2.plot(movement * 3.0, c=\"k\")\n",
    "# ax2.plot(stationary * 2.0, c=\"b\")\n",
    "ax2.scatter(mvmt_start_adj, np.ones_like(mvmt_start_adj), c=\"k\", zorder=1000)\n",
    "ax2.scatter(mvmt_end_adj, np.ones_like(mvmt_end_adj), c=\"b\", zorder=1001)\n",
    "ax3.imshow(dcols, aspect=\"auto\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(dcols, aspect=\"auto\")\n",
    "fig.set_size_inches(10, 4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "# dto = datetime.strptime(dt, \"%y%m%d_%H%M%S\")\n",
    "# start = dto.hour + dto.minute / 60.0 + dto.second / 3600.0\n",
    "# end = start + n.shape[0] / (5.0 * 3600.0)\n",
    "# t = start + np.arange(n.shape[0]) / (5.0 * 3600.0) - (24.0 if start > 12 else 0.0)\n",
    "\n",
    "# nfft = 64\n",
    "# f, tfft, s = spectrogram(sig1, fs=5.0, nfft=nfft, mode=\"magnitude\", nperseg=nfft)\n",
    "# spec = 10.0 * np.log10(np.square(s))\n",
    "\n",
    "# %%\n",
    "# _, (ax1, ax2) = plt.subplots(2, 1)\n",
    "# ax1.imshow(\n",
    "#     np.flip(spec, axis=0),\n",
    "#     vmin=np.percentile(spec, 20.0),\n",
    "#     vmax=np.percentile(spec, 99.5),\n",
    "#     extent=[t[0], t[-1], f[0], f[-1]],\n",
    "# )\n",
    "# ax2.plot(m1)\n",
    "# ax2.plot(r2 * 2.0 + 3)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "omega = 10.0\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "cwt_morlet = wavefinding_cwt(n1, widths_morlet, omega)\n",
    "angles = np.angle(cwt_morlet)\n",
    "mags = np.abs(cwt_morlet)\n",
    "cols = np.mod(angles * 0.5 / np.pi + 0.5, 1)\n",
    "col_spec = cols - 0.5\n",
    "dcol = col_spec[:, 1:] - col_spec[:, :-1]\n",
    "dcol[dcol < -0.5] += 1.0\n",
    "dcol[dcol > 0.5] -= 1.0\n",
    "dcols = cm.hsv(dcol.clip(-0.0125, 0.0125) * 40.0 + 0.5)[..., :3]\n",
    "# dcols = cm.twilight_shifted(dcol.clip(-0.05, 0.05) * 10.0 + 0.5)[..., :3]\n",
    "dcols *= (mags[:, 1:, None] / np.percentile(mags, 95.0)).clip(max=1.0)\n",
    "cols = cm.hsv(cols)[..., :3]\n",
    "cols *= mags[..., None]\n",
    "\n",
    "sharps = np.sum(mags[:50], axis=0)\n",
    "sharps *= 0.001 if omega > 4.0 else 0.005\n",
    "\n",
    "fatties = np.sum(mags[-50:], axis=0)\n",
    "fatties *= 0.002 if omega > 4.0 else 0.01\n",
    "\n",
    "mainline = np.sum(mags[50:-50], axis=0)\n",
    "mainline *= 0.004 if omega > 4.0 else 0.01\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(10, 10))\n",
    "ax1.imshow(mags.clip(max=np.percentile(mags, 95.0)), aspect=\"auto\")\n",
    "ax2.plot(np.real(n1))\n",
    "ax2.plot(np.imag(n1))\n",
    "ax2.plot(sharps)\n",
    "ax2.plot(fatties)\n",
    "ax2.plot(mainline, c=\"k\")\n",
    "ax3.imshow(dcols, aspect=\"auto\")\n",
    "fig.set_size_inches(10, 10)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sleepypi]",
   "language": "python",
   "name": "conda-env-sleepypi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "700936d7bb4d5ffe815e8346e65e8bf7ff615332b3b02af4e9a1154ab373e5b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
