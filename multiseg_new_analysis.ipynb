{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408925bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import gzip\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "\n",
    "from video_writer import VideoWriter\n",
    "\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as linalg\n",
    "from matplotlib import cm\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import correlate1d\n",
    "from scipy.signal import (\n",
    "    correlate,\n",
    "    cspline1d,\n",
    "    find_peaks,\n",
    "    cwt,\n",
    "    ricker,\n",
    "    medfilt,\n",
    "    morlet2,\n",
    "    hilbert,\n",
    ")\n",
    "import ruptures as rpt\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from dtw import dtw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4011013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smooth_and_norm_real(vals, winsize=95, smoothing=0.2, zcutoff=3.0):\n",
    "    win = np.ones((winsize,))\n",
    "    win_count = correlate(np.ones_like(vals), win)\n",
    "    signal = np.copy(vals)\n",
    "    init_rms = np.sqrt(correlate(np.square(signal), win) / win_count)\n",
    "    init_cond = np.where(\n",
    "        init_rms[: 1 - winsize] < init_rms[winsize - 1 :],\n",
    "        init_rms[: 1 - winsize],\n",
    "        init_rms[winsize - 1 :],\n",
    "    )\n",
    "    for _ in range(8):\n",
    "        moving_rms = np.sqrt(correlate(np.square(signal), win).clip(0.0) / win_count)\n",
    "        min_rms = np.minimum(moving_rms[: 1 - winsize], moving_rms[winsize - 1 :]).clip(\n",
    "            1e-5\n",
    "        )\n",
    "        magclip = (\n",
    "            zcutoff * min_rms / np.maximum(min_rms * zcutoff, np.abs(signal)).clip(1e-5)\n",
    "        )\n",
    "        signal *= magclip\n",
    "    moving_avg = correlate(signal / min_rms, win) / win_count\n",
    "    min_avg = np.where(\n",
    "        init_cond,\n",
    "        moving_avg[: 1 - winsize],\n",
    "        moving_avg[winsize - 1 :],\n",
    "    )\n",
    "    min_avg = moving_avg[winsize // 2 : -(winsize // 2)]\n",
    "\n",
    "    smoothed = cspline1d(signal / min_rms - min_avg, lamb=smoothing)\n",
    "\n",
    "    return smoothed, vals / min_rms - min_avg\n",
    "\n",
    "\n",
    "def get_rms(signal, winsize=95):\n",
    "    win = np.ones((winsize,))\n",
    "    win_count = correlate(np.ones_like(signal), win, mode=\"same\")\n",
    "    rms = np.sqrt(correlate(np.square(signal), win, mode=\"same\") / win_count)\n",
    "    return rms\n",
    "\n",
    "def norm_complex(real, imag, winsize=95, smoothing=0.2, zcutoff=3.0):\n",
    "    win = np.ones((winsize,))\n",
    "    win_count = correlate(np.ones_like(real), win)\n",
    "    signal = real + 1j * imag\n",
    "    for _ in range(8):\n",
    "        moving_rms = np.sqrt(correlate(np.square(np.abs(signal)), win) / win_count)\n",
    "        min_rms = np.minimum(moving_rms[: 1 - winsize], moving_rms[winsize - 1 :])\n",
    "        magclip = (\n",
    "            zcutoff * min_rms / np.maximum(min_rms * zcutoff, np.abs(signal)).clip(1e-5)\n",
    "        )\n",
    "        signal *= magclip\n",
    "    return (real + 1.0j * imag) / min_rms\n",
    "\n",
    "\n",
    "def smooth_and_norm_complex_stitch(\n",
    "    real_lead, imag_lead, real_lag, imag_lag, winsize=95, smoothing=0.2, zcutoff=3.0\n",
    "):\n",
    "    orig_ld_re = cspline1d(real_lead, lamb=smoothing)\n",
    "    orig_ld_im = cspline1d(imag_lead, lamb=smoothing)\n",
    "    orig_lg_re = cspline1d(real_lag, lamb=smoothing)\n",
    "    orig_lg_im = cspline1d(imag_lag, lamb=smoothing)\n",
    "    win = np.ones((winsize,))\n",
    "    win_count = correlate(np.ones_like(real_lead), win)\n",
    "    lead_signal = orig_ld_re + 1.0j * orig_ld_im\n",
    "    lag_signal = orig_lg_re + 1.0j * orig_lg_im\n",
    "    # TODO: pick a better way of stitching together, eg which is waviest?\n",
    "    # TODO: also should do a moving average of each individual component and subtract it\n",
    "    # currently there are jumps when it switches from one to the other. These suck.\n",
    "    # TODO: if a component is NOT very wavy, should shrink it by, eg, 1/3\n",
    "    # currently some components are very noisy in some places and this causes problems.\n",
    "    lead_rms = np.sqrt(correlate(np.square(np.abs(lead_signal)), win) / win_count)\n",
    "    lag_rms = np.sqrt(correlate(np.square(np.abs(lag_signal)), win) / win_count)\n",
    "    signal = np.where(\n",
    "        lead_rms[: 1 - winsize] < lag_rms[winsize - 1 :], lead_signal, lag_signal\n",
    "    )\n",
    "    orig_sig = np.copy(signal)\n",
    "    for _ in range(8):\n",
    "        moving_rms = np.sqrt(correlate(np.square(np.abs(signal)), win) / win_count)\n",
    "        min_rms = np.minimum(moving_rms[: 1 - winsize], moving_rms[winsize - 1 :])\n",
    "        magclip = (\n",
    "            zcutoff * min_rms / np.maximum(min_rms * zcutoff, np.abs(signal)).clip(1e-5)\n",
    "        )\n",
    "        signal *= magclip\n",
    "    return orig_sig / min_rms\n",
    "\n",
    "\n",
    "def fit_linear_lead_and_lag(yi, winsize=155):\n",
    "    k = winsize - 1\n",
    "    sd = k * (k + 1) / 2\n",
    "    sdsq = k * (k + 1) * (2 * k + 1) / 6\n",
    "    win = np.ones(winsize)\n",
    "    invdenom = 1.0 / (sdsq * winsize - sd ** 2)\n",
    "    slp_kern = np.arange(winsize)\n",
    "    fslp_lag = correlate(yi, slp_kern, mode=\"full\")[winsize - 1:]\n",
    "    fslp_lead = correlate(yi, -slp_kern[::-1], mode=\"full\")[:1 - winsize]\n",
    "    fsum = correlate(yi, win, mode=\"full\")\n",
    "    fsum_lead = fsum[:1 - winsize]\n",
    "    fsum_lag = fsum[winsize - 1:]\n",
    "    yi2 = np.square(yi)\n",
    "    sumsq = correlate(yi2, win, mode=\"full\")\n",
    "    sumsq_lead = sumsq[:1 - winsize]\n",
    "    sumsq_lag = sumsq[winsize - 1:]\n",
    "    m_lag = (fslp_lag * winsize - sd * fsum_lag) * invdenom\n",
    "    b_lag = (fsum_lag * sdsq - sd * fslp_lag) * invdenom\n",
    "    m_lead = (fslp_lead * winsize + sd * fsum_lead) * invdenom\n",
    "    b_lead = (fsum_lead * sdsq + sd * fslp_lead) * invdenom\n",
    "    serr_lag = (\n",
    "        np.square(b_lag) * winsize\n",
    "        - 2 * b_lag * fsum_lag\n",
    "        + 2 * b_lag * m_lag * sd\n",
    "        + sumsq_lag\n",
    "        - 2 * m_lag * fslp_lag\n",
    "        + np.square(m_lag) * sdsq\n",
    "    ) / winsize\n",
    "    serr_lead = (\n",
    "        np.square(b_lead) * winsize\n",
    "        - 2 * b_lead * fsum_lead\n",
    "        - 2 * b_lead * m_lead * sd\n",
    "        + sumsq_lead\n",
    "        - 2 * m_lead * fslp_lead\n",
    "        + np.square(m_lead) * sdsq\n",
    "    ) / winsize\n",
    "    # var = sumsq + 4.0 * (winsize / sdsq - 1.0) * sqsum / sdsq\n",
    "    # notice the clips - if variance is zero, r^2 is zero\n",
    "    # r2 = 1.0 - serr / var.clip(1e-8)\n",
    "\n",
    "    return m_lead * winsize, m_lag * winsize, b_lead, b_lag, np.sqrt(serr_lead.clip(min=0.0)), np.sqrt(serr_lag.clip(min=0.0))\n",
    "\n",
    "\n",
    "def compute_stats(xi, yi, winsize=255, mode=\"nearest\"):\n",
    "    hw = winsize // 2\n",
    "    hwmo = (winsize - 1) // 2\n",
    "    win = np.ones((winsize,))\n",
    "    xiyi = xi * yi\n",
    "    xi2 = np.square(xi)\n",
    "    yi2 = np.square(yi)\n",
    "    win_xiyi = correlate1d(xiyi, win, mode=mode)\n",
    "    win_xi2 = correlate1d(xi2, win, mode=mode)\n",
    "    win_yi2 = correlate1d(yi2, win, mode=mode)\n",
    "    m = win_xiyi / win_xi2.clip(1e-8)\n",
    "    b = correlate1d(yi, win, mode=mode)\n",
    "    idx = np.arange(xi.shape[0])[:, None] + np.arange(winsize) - hwmo\n",
    "    ywin = yi[idx.clip(min=0, max=xi.size - 1)]\n",
    "    yhat = idx * m[:, None] + b[:, None]\n",
    "    sse = np.sum(np.square(ywin - yhat), axis=1)\n",
    "    r2 = 1 - sse / win_yi2.clip(min=1e-9)\n",
    "\n",
    "    # angles = -np.arctan(m)\n",
    "    # angles = np.concatenate(\n",
    "    #     (np.full((hwmo,), angles[0]), angles, np.full((hw,), angles[-1]))\n",
    "    # )\n",
    "    # R = np.array([[np.cos(angles), -np.sin(angles)], [np.sin(angles), np.cos(angles)]])\n",
    "    # input = np.stack((xi, yi), axis=0)[:, None, :]\n",
    "    # res = np.sum(R * input, axis=1)[0]\n",
    "\n",
    "    # res3 = res ** 3\n",
    "    # win_res3 = correlate(res3, win, mode=mode)\n",
    "    # skew = np.concatenate(\n",
    "    #     (np.full((hw,), win_res3[0]), win_res3, np.full((hw,), win_res3[-1]))\n",
    "    # )\n",
    "\n",
    "    return b, m, r2, sse / winsize\n",
    "\n",
    "\n",
    "def correlate_peaks(sig, start, length, min_shift, chunk):\n",
    "    seg = sig[start : start + chunk]\n",
    "    comp = sig[start + min_shift : start + length + min_shift]\n",
    "    return correlate(comp, seg, mode=\"valid\"), seg, comp\n",
    "\n",
    "\n",
    "def get_turning_points(sig):\n",
    "    deltas = sig[1:] - sig[:-1]\n",
    "    turn_pts = deltas[1:] * deltas[:-1] <= 0.0\n",
    "    curvature = correlate(\n",
    "        sig, np.array([5.0, 0.0, -3.0, -4.0, -3.0, 0.0, 5.0]), mode=\"same\"\n",
    "    )\n",
    "\n",
    "    peaks = np.argwhere(np.logical_and(deltas[:-1] > 0.0, turn_pts))\n",
    "    troughs = np.argwhere(np.logical_and(deltas[:-1] < 0.0, turn_pts))\n",
    "\n",
    "    # unlikely that a true breath would be faster than 3 seconds, ie 15 frames\n",
    "    dist = 15\n",
    "    peak_dists = peaks[1:] - peaks[:-1]\n",
    "    trough_dists = troughs[1:] - troughs[:-1]\n",
    "\n",
    "    # find the highest peak, eliminate any peaks too closeby, which means also\n",
    "    # eliminating some troughs... but which ones... fuck, I have to think about this.\n",
    "    while False:\n",
    "        pass\n",
    "\n",
    "\n",
    "def compute_r2(data, fit, winsize=255):\n",
    "    hw = winsize // 2\n",
    "    win = np.ones((winsize,))\n",
    "    err = data - fit\n",
    "    avgs = correlate(data, win, mode=\"same\") / winsize\n",
    "    sum_err2 = correlate(np.square(err), win, mode=\"same\")\n",
    "    idx = np.arange(data.shape[0])[:, None] + np.arange(winsize)\n",
    "    data_ext = np.concatenate((np.zeros(hw), data, np.zeros(hw)), axis=0)\n",
    "    sum_var = np.sum(np.square(data_ext[idx] - avgs[:, None]), axis=1)\n",
    "    sum_var = np.var(data_ext[idx] - avgs[:, None], axis=1) * winsize\n",
    "    r2 = 1 - sum_err2 / sum_var.clip(1e-5)\n",
    "\n",
    "    return r2, err, sum_err2, sum_var\n",
    "\n",
    "\n",
    "def get_findiff_curvature(data):\n",
    "    curvature = np.zeros_like(data)\n",
    "    curvature[1:-1] = data[:-2] + data[2:] - 2.0 * data[1:-1]\n",
    "    curvature[0] = curvature[1]\n",
    "    curvature[-1] = curvature[-2]\n",
    "    return curvature\n",
    "\n",
    "\n",
    "def morlet_real(*args, **kwargs):\n",
    "    return np.real(morlet2(*args, **kwargs))\n",
    "\n",
    "\n",
    "def get_slopes(data):\n",
    "    dx = data[:, 1:] - data[:, :-1]  # x right\n",
    "    dy = data[1:] - data[:-1]  # y down\n",
    "    de = data[1:, 1:] - data[:-1, :-1]  # y down, x right\n",
    "    do = data[1:, :-1] - data[:-1, 1:]  # y down, x left\n",
    "\n",
    "    tl = de[:-1, :-1]\n",
    "    tc = dy[:-1, 1:-1]\n",
    "    tr = do[:-1, 1:]\n",
    "    rc = -dx[1:-1, 1:]\n",
    "    br = -de[1:, 1:]\n",
    "    bc = -dy[1:, 1:-1]\n",
    "    bl = -do[1:, :-1]\n",
    "    lc = dx[1:-1, :-1]\n",
    "\n",
    "    res = np.zeros((8, data.shape[0], data.shape[1]))\n",
    "    res[:, 1:-1, 1:-1] = np.stack((tl, tc, tr, rc, br, bc, bl, lc), axis=0)\n",
    "    return res\n",
    "\n",
    "\n",
    "def wavefinding_cwt(signal, widths, omega=5):\n",
    "    output = np.empty((len(widths), len(signal)), dtype=np.complex128)\n",
    "    for ind, width in enumerate(widths):\n",
    "        # go for an odd window length about 8x the length of the width\n",
    "        N = round(4 * width - 0.5) * 2 + 1\n",
    "        N = np.min([N, len(signal)])\n",
    "        wavelet_data = morlet2(N, width, omega)\n",
    "        # using correlate instead of convolve\n",
    "        output[ind] = correlate(\n",
    "            signal.astype(np.complex128), wavelet_data, mode=\"same\"\n",
    "        ) * np.exp(-1.0j * omega * np.arange(len(signal)) / width)\n",
    "    return output\n",
    "\n",
    "\n",
    "def suppress_noise(real, imag):\n",
    "    \"\"\"Normalizes the traces, then compares them and weights the wavier one higher.\"\"\"\n",
    "    real_norm, _ = smooth_and_norm_real(real)\n",
    "    imag_norm, _ = smooth_and_norm_real(imag)\n",
    "    omega = 20.0\n",
    "    fs = 5.0\n",
    "    freqs = np.logspace(0.1, -1.4, 150)  # ~50-85 are breathing frequencies\n",
    "    widths_morlet = omega * fs / (freqs[50:85] * 2 * np.pi)\n",
    "    real_wave = wavefinding_cwt(real_norm, widths_morlet, omega)\n",
    "    mags_real = np.sum(np.square(np.abs(real_wave)), axis=0)\n",
    "    imag_wave = wavefinding_cwt(imag_norm, widths_morlet, omega)\n",
    "    mags_imag = np.sum(np.square(np.abs(imag_wave)), axis=0)\n",
    "    # clip at 4 - don't want to be making up bs waves\n",
    "    ratio = np.square(mags_real / mags_imag).clip(0.25, 4.0)\n",
    "    # normalize again after the ratio thing\n",
    "    normed = norm_complex(real_norm * ratio, imag_norm / ratio)\n",
    "    # these work pretty well I've found\n",
    "    widths_peak = np.arange(4, 80) * 0.2\n",
    "    cplx_peaks = cwt(normed, ricker, widths_peak, dtype=np.complex128)\n",
    "    cplx_slopes = get_slopes(np.abs(cplx_peaks))\n",
    "    peak_cond = np.sum(np.sign(cplx_slopes), axis=0) == 8.0\n",
    "    peak_vals = cplx_peaks[peak_cond]\n",
    "    arg = np.mod(np.angle(np.square(cplx_peaks)) * 0.5 / np.pi, 1.0)\n",
    "    cols = cm.hsv(arg)[..., :3]\n",
    "    mags = np.abs(cplx_peaks)\n",
    "    mags *= 999.0 / np.max(mags)\n",
    "    mags += 1.0\n",
    "    cols *= np.log10(mags[..., None]) / 3.0\n",
    "    cols[peak_cond, :] = 0.0\n",
    "\n",
    "    amax = np.argmax(np.abs(cplx_peaks[:26]), axis=0).astype(float)\n",
    "    amax_sm = np.around(cspline1d(medfilt(amax, 1023), 5000000)).astype(int)\n",
    "    asym_trace = np.mean(np.abs(cplx_peaks[5:25]), axis=0)\n",
    "    max_trace = np.abs(cplx_peaks[amax_sm, np.arange(amax.size)])\n",
    "\n",
    "    # find the minima of both traces\n",
    "    asym_min = np.logical_and(\n",
    "        asym_trace[:-2] > asym_trace[1:-1], asym_trace[1:-1] < asym_trace[2:]\n",
    "    )\n",
    "    max_min = np.logical_and(\n",
    "        max_trace[:-2] > max_trace[1:-1], max_trace[1:-1] < max_trace[2:]\n",
    "    )\n",
    "    asym_mean = np.mean(cplx_peaks[6:35], axis=0)  # empirical values\n",
    "    asym_rot = np.angle(asym_mean[1:] / asym_mean[:-1])\n",
    "    rot_conv = correlate(asym_rot, np.ones(3), mode=\"same\")\n",
    "    asym_pk_arg, *_ = find_peaks(\n",
    "        np.abs(rot_conv), height=0.9, distance=4, prominence=0.2\n",
    "    )\n",
    "    asym_min_arg = np.argwhere(asym_min).squeeze()\n",
    "    max_min_arg = np.argwhere(max_min).squeeze()\n",
    "    # min_max_min = max_min_arg - 2\n",
    "    # max_max_min = max_min_arg + 2\n",
    "    # min_cond = np.any(\n",
    "    #     np.logical_and(\n",
    "    #         asym_min_arg[:, None] >= min_max_min[None, :],\n",
    "    #         asym_min_arg[:, None] <= max_max_min[None, :],\n",
    "    #     ),\n",
    "    #     axis=1,\n",
    "    # )\n",
    "    # rem_args = asym_min_arg[min_cond] + 1\n",
    "    rem_args = asym_pk_arg\n",
    "    arg_diffs = rem_args[1:] - rem_args[:-1]\n",
    "    mid_heights = max_trace[np.around(0.5 * (rem_args[1:] + rem_args[:-1])).astype(int)]\n",
    "\n",
    "    # used for calculations inside\n",
    "    nfold = np.sqrt(np.square(normed))\n",
    "    nsq = np.abs(normed) * normed\n",
    "    rotated = np.zeros_like(normed, dtype=np.complex128)\n",
    "\n",
    "    # now we need to segment where the signal goes outside of the range (-3.0, 3.0)\n",
    "    past_3 = np.argwhere(np.abs(normed) > 3.0).squeeze()\n",
    "    num_outliers = len(past_3)\n",
    "    outliers = np.zeros(num_outliers + 2, dtype=int)\n",
    "    outliers[1:-1] = past_3\n",
    "    outliers[-1] = len(normed) - 1\n",
    "    for idx in range(num_outliers + 1):\n",
    "        start = outliers[idx]\n",
    "        stop = outliers[idx + 1]\n",
    "        if start < 109805 < stop:\n",
    "            print(\"stop here\")\n",
    "        inside_cond = np.logical_and(rem_args > start, rem_args < stop)\n",
    "        inside_args = np.argwhere(inside_cond).squeeze()\n",
    "        num_mins = np.count_nonzero(inside_cond)\n",
    "        if num_mins >= 3:\n",
    "            # this is where we can start to feel pretty confident.\n",
    "            # But we'll still rotate each peak based on the average within the peak.\n",
    "            # outside the peak, I guess just rotate based on the closest peak?\n",
    "            first_min = np.min(inside_args)\n",
    "            last_min = np.max(inside_args - (1 if num_mins % 2 == 0 else 0))\n",
    "            diffs = arg_diffs[first_min:last_min]\n",
    "            hghts = mid_heights[first_min:last_min]\n",
    "            if num_mins > 4:\n",
    "                diff_scores = (\n",
    "                    (np.mean(diffs) - diffs) / np.std(diffs).clip(1e-5)\n",
    "                ).clip(-3.0, 3.0)\n",
    "                height_scores = (\n",
    "                    (hghts - np.mean(hghts)) / np.std(hghts).clip(1e-5)\n",
    "                ).clip(-3.0, 3.0)\n",
    "                parity = (\n",
    "                    np.mean(diff_scores[::2] + height_scores[::2])\n",
    "                    - np.mean(diff_scores[1::2] + height_scores[1::2])\n",
    "                ) < 0  # zero if even are the peaks, else odd are\n",
    "            else:\n",
    "                seg = normed[rem_args[first_min] : rem_args[last_min]]\n",
    "                frac_re = np.sum(np.square(np.real(seg) / np.abs(seg)))\n",
    "                if frac_re > 0.5:\n",
    "                    nseg = np.real(seg) - np.mean(np.real(seg))\n",
    "                    # frac_under = np.count_nonzero(\n",
    "                    #     (np.real(seg) < nseg\n",
    "                    # )) / len(seg)\n",
    "                else:\n",
    "                    nseg = np.imag(seg) - np.mean(np.imag(seg))\n",
    "                    # frac_under = np.count_nonzero(\n",
    "                    #     (np.imag(seg) < np.mean(np.imag(seg)))\n",
    "                    # ) / len(seg)\n",
    "                parity = (\n",
    "                    np.mean(np.abs(nseg[: diffs[0]]))\n",
    "                    - np.mean(np.abs(nseg[diffs[0] :]))\n",
    "                    > 0\n",
    "                )\n",
    "            # next, average across each peak.\n",
    "            rot_angles = np.zeros((num_mins - 1) // 2, dtype=np.complex128)\n",
    "            for jdx in range((num_mins - 1) // 2):\n",
    "                first = rem_args[inside_args[jdx * 2]]\n",
    "                mid = rem_args[inside_args[jdx * 2 + 1]]\n",
    "                last = rem_args[inside_args[jdx * 2 + 2]]\n",
    "                parity_av = np.mean(normed[first:mid]) - np.mean(normed[mid:last])\n",
    "                parity_av /= np.abs(parity_av)\n",
    "                if parity:\n",
    "                    parity_av *= -1.0\n",
    "                # first_pk = rem_args[inside_args[jdx * 2 + parity]]\n",
    "                # last_pk = rem_args[inside_args[jdx * 2 + 1 + parity]]\n",
    "                # wav_av = np.mean(nfold[first:last])\n",
    "                # peak_av = np.mean(normed[first_pk:last_pk])\n",
    "                rot_angles[jdx] = np.conj(parity_av)\n",
    "                # rot_angles[jdx] = np.conj(\n",
    "                #     wav_av * np.sign(np.real(wav_av / peak_av)) / np.abs(wav_av)\n",
    "                # )\n",
    "                rotated[first:last] = normed[first:last] * rot_angles[jdx]\n",
    "                if first < 109805 < last:\n",
    "                    print(first, last, rot_angles[jdx], wav_av, parity_av)\n",
    "            # finally, do the parts on either side of the peaks...\n",
    "            rotated[start : rem_args[first_min]] = (\n",
    "                normed[start : rem_args[first_min]] * rot_angles[0]\n",
    "            )\n",
    "            rotated[rem_args[last_min] : stop] = (\n",
    "                normed[rem_args[last_min] : stop] * rot_angles[-1]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # hard to tell what's going on really. We have one peak or less.\n",
    "            # get the distance-weighted average, and rotate by that.\n",
    "            wav_av = np.mean(nfold[start:stop])\n",
    "            nsq_av = np.mean(nsq[start:stop])\n",
    "            rot_angle = np.conj(\n",
    "                wav_av * np.sign(np.real(wav_av / nsq_av)) / np.abs(wav_av)\n",
    "            )\n",
    "            diff = rotated[start - 1] - normed[start] * rot_angle\n",
    "            ndiff = rotated[start - 1] + normed[start] * rot_angle\n",
    "            result = np.sign(np.abs(ndiff) - np.abs(diff))\n",
    "            rotated[start:stop] = normed[start:stop] * rot_angle * result\n",
    "\n",
    "    _, (ax1, ax2) = plt.subplots(2, 1, sharex=\"col\")\n",
    "    ax1.imshow(cols, aspect=\"auto\")\n",
    "    ax2.plot(np.real(asym_mean))\n",
    "    ax2.plot(np.abs(rot_conv))\n",
    "    ax2.plot(asym_trace)\n",
    "    ax2.plot(np.real(normed))\n",
    "    ax2.plot(np.imag(normed))\n",
    "    ax2.plot(real_norm, linestyle=\":\")\n",
    "    ax2.plot(imag_norm, linestyle=\":\")\n",
    "    ax2.plot(ratio, linestyle=\"--\")\n",
    "    ax2.scatter(asym_pk_arg, -4.5 * np.ones_like(asym_pk_arg))\n",
    "    ax2.plot(np.real(rotated))\n",
    "    # ax2.legend(\n",
    "    #     [\n",
    "    #         \"asymmean\",\n",
    "    #         \"rotconv\",\n",
    "    #         \"asymtrace\",\n",
    "    #         \"re(normed)\",\n",
    "    #         \"im(normed)\",\n",
    "    #         \"realnorm\",\n",
    "    #         \"imnorm\",\n",
    "    #         \"ratio\",\n",
    "    #         \"rotated\",\n",
    "    #         \"crossing\",\n",
    "    #     ]\n",
    "    # )\n",
    "    plt.show()\n",
    "\n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(real_norm)\n",
    "    # ax.plot(imag_norm)\n",
    "    # ax.plot(np.log2(ratio))\n",
    "    # plt.show()\n",
    "    return (real_norm * ratio, imag_norm / ratio, ratio, cwt_real, cwt_imag)\n",
    "\n",
    "\n",
    "def flip_signal(signal):\n",
    "    smoothed, normed = smooth_and_norm_real(signal, smoothing=20.0)\n",
    "\n",
    "    r2, *_ = compute_r2(normed.clip(-3.0, 3.0), smoothed, winsize=255)\n",
    "\n",
    "    curv = get_findiff_curvature(smoothed)\n",
    "\n",
    "    bmp = 21 / 60  # max breaths per second\n",
    "    fs = 5  # sample rate\n",
    "    max_curv_exag = (2 * np.pi * bmp / fs) ** 4\n",
    "\n",
    "    curv_exag = (curv * np.abs(curv)) / max_curv_exag\n",
    "\n",
    "    curv_exag_sm = correlate(curv_exag, np.ones(1001) / 10.0, mode=\"same\")\n",
    "\n",
    "    past_3 = np.argwhere(np.abs(normed) > 5.0).squeeze()\n",
    "    num_outliers = len(past_3)\n",
    "    outliers = np.zeros(num_outliers + 2, dtype=int)\n",
    "    outliers[1:-1] = past_3\n",
    "    outliers[-1] = len(signal) - 1\n",
    "\n",
    "    righted = np.zeros_like(signal)\n",
    "    stdev = 20.0\n",
    "    windows = np.zeros_like(signal)\n",
    "\n",
    "    for idx in range(num_outliers + 1):\n",
    "        start = outliers[idx]\n",
    "        stop = outliers[idx + 1]\n",
    "\n",
    "        win_range = np.minimum(np.arange(stop - start), np.arange(stop - start)[::-1])\n",
    "        window = 1.0 - np.exp(-0.5 * np.square(win_range.clip(max=3 * stdev) / stdev))\n",
    "        windows[start:stop] = window\n",
    "\n",
    "        right = np.sum(curv_exag[start:stop] * window) <= 0.0\n",
    "\n",
    "        if right:\n",
    "            righted[start:stop] = normed[start:stop]\n",
    "        else:\n",
    "            righted[start:stop] = -normed[start:stop]\n",
    "\n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(normed)\n",
    "    # ax.plot(smoothed)\n",
    "    # ax.plot(windows)\n",
    "    # ax.plot(righted, linestyle=\"--\")\n",
    "    # ax.plot(curv_exag)\n",
    "    # ax.plot(curv_exag_sm)\n",
    "    # ax.plot(r2)\n",
    "    # ax.legend([\"norm\", \"sm\", \"win\", \"right\", \"curv\", \"curvsm\", \"r2\"])\n",
    "    # plt.show()\n",
    "\n",
    "    return smoothed, normed, r2, righted, curv_exag\n",
    "\n",
    "\n",
    "def flip_components_indiv_and_combine(sig_x, sig_y, segments):\n",
    "    \"\"\"Normalizes the traces, then compares them and weights the wavier one higher.\"\"\"\n",
    "    sig_x_copy = np.copy(sig_x)\n",
    "    sig_y_copy = np.copy(sig_y)\n",
    "\n",
    "    for sdx, seg in enumerate(segments[:-1]):\n",
    "        scale_x = flip_signal(sig_x[seg:segments[sdx + 1]])\n",
    "        scale_y = flip_signal(sig_y[seg:segments[sdx + 1]])\n",
    "        sig_x_copy[seg:segments[sdx + 1]] *= scale_x\n",
    "        sig_y_copy[seg:segments[sdx + 1]] *= scale_y\n",
    "\n",
    "    omega = 20.0\n",
    "    fs = 5.0\n",
    "    freqs = np.logspace(0.1, -1.4, 150)  # ~50-85 are breathing frequencies\n",
    "    widths_morlet = omega * fs / (freqs[55:80] * 2 * np.pi)\n",
    "    real_wave = wavefinding_cwt(real_righted.clip(-3.0, 3.0), widths_morlet, omega)\n",
    "    mags_real = np.sum(np.square(np.abs(real_wave)), axis=0)\n",
    "    imag_wave = wavefinding_cwt(imag_righted.clip(-3.0, 3.0), widths_morlet, omega)\n",
    "    mags_imag = np.sum(np.square(np.abs(imag_wave)), axis=0)\n",
    "\n",
    "    rr_align = np.zeros_like(real_righted)\n",
    "    ir_align = np.zeros_like(imag_righted)\n",
    "\n",
    "    past_3 = np.argwhere(\n",
    "        np.logical_or(np.abs(real_norm) > 5.0, np.abs(imag_norm) > 5.0)\n",
    "    ).squeeze()\n",
    "    num_outliers = len(past_3)\n",
    "    outliers = np.zeros(num_outliers + 2, dtype=int)\n",
    "    outliers[1:-1] = past_3\n",
    "    outliers[-1] = len(real_righted) - 1\n",
    "\n",
    "    stdev = 20.0\n",
    "\n",
    "    for idx in range(num_outliers + 1):\n",
    "        start = outliers[idx]\n",
    "        stop = outliers[idx + 1]\n",
    "\n",
    "        if start < 97250 < stop:\n",
    "            print(\"shit\")\n",
    "\n",
    "        win_range = np.minimum(np.arange(stop - start), np.arange(stop - start)[::-1])\n",
    "        window = 1.0 - np.exp(-0.5 * np.square(win_range.clip(max=3 * stdev) / stdev))\n",
    "\n",
    "        agree = (\n",
    "            np.sum(real_righted[start:stop] * imag_righted[start:stop] * window) >= 0.0\n",
    "        )\n",
    "\n",
    "        if agree:\n",
    "            rr_align[start:stop] = real_righted[start:stop]\n",
    "            ir_align[start:stop] = imag_righted[start:stop]\n",
    "        else:\n",
    "            # who has the higher sum of curvatures on their side?\n",
    "            real_flip = np.sign(\n",
    "                np.abs(np.sum(real_curv[start:stop]))\n",
    "                - np.abs(np.sum(imag_curv[start:stop]))\n",
    "            )\n",
    "            rr_align[start:stop] = real_righted[start:stop] * real_flip\n",
    "            ir_align[start:stop] = imag_righted[start:stop] * -real_flip\n",
    "\n",
    "    real_frac = mags_real / (mags_real + mags_imag)\n",
    "    imag_frac = 1.0 - real_frac\n",
    "\n",
    "    result = real_frac * real_righted + imag_frac * imag_righted\n",
    "\n",
    "    # _, ax = plt.subplots(1, 1)\n",
    "    # ax.plot(real_righted)\n",
    "    # # ax.plot(real_smooth)\n",
    "    # ax.plot(imag_righted)\n",
    "    # ax.plot(result)\n",
    "    # # ax.plot(imag_smooth)\n",
    "    # # ax.plot(result)\n",
    "    # # ax.plot(real_nm_curv_conv + 3.0, linestyle=\"--\")\n",
    "    # # ax.plot(imag_nm_curv_conv + 3.0, linestyle=\"--\")\n",
    "    # # ax.plot(real_r2 - 3.0)\n",
    "    # # ax.plot(imag_r2 - 3.0, linestyle=\"--\")\n",
    "    # # ax.plot(mags_real * 0.005)\n",
    "    # # ax.plot(mags_imag * 0.005)\n",
    "    # ax.plot(real_frac)\n",
    "    # ax.plot(imag_frac)\n",
    "    # ax.legend([\"rr\", \"ir\", \"res\", \"rf\", \"if\"])\n",
    "    # plt.show()\n",
    "\n",
    "    return result\n",
    "\n",
    "def adjust_for_zero_trend(signal, lam=25.0):\n",
    "    new_sig = np.copy(signal)\n",
    "    sm = cspline1d(signal, lam)\n",
    "    segs = np.argwhere(np.logical_and(sm[1:] * sm[:-1] < 0.0, sm[1:] < sm[:-1])).squeeze()\n",
    "    segs[0] = 0\n",
    "    for idx in range(len(segs) - 1):\n",
    "        start = segs[idx]\n",
    "        stop = segs[idx + 1]\n",
    "        seg_offset = np.mean(signal[start:stop])\n",
    "        new_sig[start:stop] -= seg_offset\n",
    "        bnd_offset = np.mean(np.cumsum(new_sig[start:stop]))\n",
    "        new_sig[start] -= 0.5 * bnd_offset\n",
    "        new_sig[stop] += 0.5 * bnd_offset\n",
    "    \n",
    "    return new_sig, sm\n",
    "\n",
    "def find_periodic(signal, freqs):\n",
    "    m = 3.0\n",
    "    tl = signal.size\n",
    "    fs = 5\n",
    "    coeffs = np.zeros((freqs.size, tl, 4))\n",
    "    r2 = np.zeros((freqs.size, tl))\n",
    "    lgs = np.around(m * fs / freqs)\n",
    "    sss = np.zeros((freqs.size, tl))\n",
    "    sqsig = np.square(signal)\n",
    "    for fdx, freq in enumerate(freqs):\n",
    "        lg = round(m * fs / freq)\n",
    "        cos = np.cos(2.0 * m * np.pi * np.arange(lg) / lg)\n",
    "        sin = np.sin(2.0 * m * np.pi * np.arange(lg) / lg)\n",
    "        dcos = np.cos(4.0 * m * np.pi * np.arange(lg) / lg)\n",
    "        dsin = np.sin(4.0 * m * np.pi * np.arange(lg) / lg)\n",
    "        a = np.correlate(signal, cos) * 2.0 / lg        \n",
    "        b = np.correlate(signal, sin) * 2.0 / lg\n",
    "        c = np.correlate(signal, dcos) * 2.0 / lg\n",
    "        d = np.correlate(signal, dsin) * 2.0 / lg\n",
    "        abcd = np.stack([a, b, c, d], axis=1)\n",
    "        sss[fdx, :(tl - lg + 1)] = np.correlate(sqsig, np.ones(lg))  # sum of square signal\n",
    "        # can derive this expression, all terms of sse (acos(wt)+bsin(wt)... - sig)^2\n",
    "        # all come out to be 0 or -N/2 * the square of (a or b or c or d)\n",
    "        part_sse = lg * 0.5 * np.sum(np.square(abcd), axis=1)\n",
    "        # sse = sss - part_sse, r2 = 1 - sse / sss -> simplified it\n",
    "        # only works if we assume zero mean, which I do in the fit.\n",
    "        r2[fdx, :(tl - lg + 1)] = part_sse / sss[fdx, :(tl - lg + 1)]\n",
    "        coeffs[fdx, :(tl - lg + 1)] = abcd\n",
    "    best_freqs = np.argmax(correlate1d(r2, np.ones(75), mode=\"nearest\"), axis=0)\n",
    "    final_r2 = np.zeros((tl, ))\n",
    "    final_lg = np.zeros((tl, ))\n",
    "    sigma = 2.0  # this gets us to 12 timesteps away... meaning we can span ~5 sec gaps\n",
    "    fs = 2.0\n",
    "    n_std = 6  # 6 gets us down to 1.5e-8, which is nonzero enough for me\n",
    "    c_normal = np.exp(-0.5 * np.square((np.arange(fs * sigma * n_std * 2 + 1) - (fs * sigma * n_std)) / sigma))\n",
    "    c_normal /= np.sum(c_normal)\n",
    "    for idx, fdx in tqdm(enumerate(best_freqs), total=best_freqs.size, ncols=100):\n",
    "        offset = round(lgs[fdx] / m)\n",
    "        lg = int(lgs[fdx])\n",
    "        final_lg[idx] = lg\n",
    "        if idx + offset + lg >= tl:\n",
    "            break\n",
    "\n",
    "        # calculate dynamic time warp alignment\n",
    "        alignment = dtw(signal[idx + offset:idx + lg + offset], signal[idx:idx + lg], keep_internals=True)\n",
    "        # alignment.plot(type=\"twoway\", offset=-2 * np.sqrt(np.sum(np.square(coeffs[fdx, idx]))))\n",
    "        # this will necessarily be a function even in the worst case\n",
    "        new_xs = 0.5 * (alignment.index1 + alignment.index2)\n",
    "        new_ys = alignment.index2 - alignment.index1\n",
    "        # interpolate it, then correlate, rotate it back, then perform the warp\n",
    "        # cut off the first and last bit, helping to guarantee it's in the interp range\n",
    "        ixs = np.arange(lg * 2 - 5) * 0.5 + 1.0\n",
    "        # 1-1e-8 is a bit of a kludge to get back a function but it's fine innit\n",
    "        iys = interp1d(new_xs, new_ys * (1.0 - 1e-8))(ixs)\n",
    "        max_dev = 5.0  # can be up to 1 second off\n",
    "        miys = np.mean(iys).clip(min=-max_dev, max=max_dev)\n",
    "        siys = iys.clip(miys - max_dev, miys + max_dev)\n",
    "        spys = correlate1d(siys, c_normal, mode=\"constant\")\n",
    "        rix = ixs - 0.5 * spys\n",
    "        riy = ixs + 0.5 * spys\n",
    "        sse_dtw = np.sum(np.square(spys)) * 16.0  # abs(abs(x-30)-15)-7.5 -> r^2 = 0\n",
    "        sst_dtw = np.sum(np.square(ixs - np.mean(ixs)))\n",
    "        qys = interp1d(np.arange(lg), signal[idx + offset:idx + lg + offset])(rix)\n",
    "        tys = interp1d(np.arange(lg), signal[idx:idx + lg])(riy)\n",
    "        sse_fin = np.sum(np.square(qys - tys))\n",
    "        sst_fin = np.sum(np.square(tys - np.mean(tys)))\n",
    "        cr2 = 1.0 - sse_dtw / sst_dtw - sse_fin / sst_fin\n",
    "\n",
    "#         if idx == 130:\n",
    "#             _, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#             ax1.plot(rix, riy)\n",
    "#             ax1.plot(alignment.index1, alignment.index2)\n",
    "#             ax2.plot(np.arange(lg), signal[idx + offset:idx + lg + offset])\n",
    "#             ax2.plot(riy, qys)\n",
    "#             ax2.plot(riy, tys)\n",
    "#             plt.show()\n",
    "\n",
    "        # sse = np.sum(np.square(signal[idx:idx + lg] - signal[idx + offset:idx + lg + offset]))\n",
    "        # cr2 = 1.0 - sse / max(sss[fdx, idx], sss[fdx, idx + offset])\n",
    "        final_r2[idx:idx + offset + lg] = np.maximum(final_r2[idx:idx + offset + lg], cr2)\n",
    "#     _, ax = plt.subplots(1, 1)\n",
    "#     ax.imshow(correlate1d(r2, np.ones(75), mode=\"nearest\"), aspect=\"auto\")\n",
    "#     ax.plot(best_freqs, c=\"r\")\n",
    "#     plt.show()\n",
    "    return final_r2.clip(min=0.0), final_lg\n",
    "\n",
    "def get_mse(sig, winsize=75):\n",
    "    c0 = correlate(np.square(sig), np.ones(winsize)) / winsize\n",
    "    return np.minimum(c0[:1 - winsize], c0[winsize - 1:])\n",
    "\n",
    "def get_mse_fracs(sig, winsize=75):\n",
    "    mse0 = get_mse(sig[:, 0], winsize)\n",
    "    mse1 = get_mse(sig[:, 1], winsize)\n",
    "    return mse0 / (mse0 + mse1), mse1 / (mse0 + mse1)\n",
    "\n",
    "def get_cdf_diff(residuals_1, residuals_2):\n",
    "    ord1 = np.sort(residuals_1)\n",
    "    ord2 = np.sort(residuals_2)\n",
    "    n = residuals_1.size\n",
    "    m = residuals_2.size\n",
    "    start = 0\n",
    "    if ord1[0] < ord2[0]:\n",
    "        start = np.max(np.argwhere(ord1 < ord2[0]).squeeze())\n",
    "        sup = (start + 1) / n\n",
    "#         print(1, sup)\n",
    "    elif ord2[0] < ord1[0]:\n",
    "        sup = np.max(np.argwhere(ord2 < ord1[0]).squeeze()) / m\n",
    "#         print(2, sup)\n",
    "    else:\n",
    "        sup = 0.0\n",
    "    \n",
    "    for idx in range(start, n-1):\n",
    "        lw = ord2 <= ord1[idx]\n",
    "        up = ord2 < ord1[idx + 1]\n",
    "        if np.count_nonzero(up) == 0:\n",
    "            # none of them are less than the upper bound\n",
    "            sup = max(sup, (idx + 1) / n)\n",
    "#             print(3, sup)\n",
    "        else:\n",
    "            if np.count_nonzero(lw) == 0:\n",
    "                ldx = 0\n",
    "            else:\n",
    "                ldx = np.max(np.argwhere(lw).squeeze())\n",
    "            udx = np.max(np.argwhere(up).squeeze())\n",
    "            for jdx in range(ldx, udx + 1):\n",
    "                sup = max(sup, abs((idx +  1) / n - (jdx + 1) / m))\n",
    "    if ord1[-1] < ord2[-1]:\n",
    "        lw = np.max(np.argwhere(ord2 <= ord1[-1]).squeeze()) / m\n",
    "        sup = max(sup, 1.0 - lw)\n",
    "#         print(5, sup, lw)\n",
    "#     _, ax = plt.subplots(1, 1)\n",
    "#     o1e = np.concatenate((np.ones(1) * ord1[0] - 0.15, np.repeat(ord1, 2), np.ones(1) * ord1[-1] + 0.15))\n",
    "#     o2e = np.concatenate((np.ones(1) * ord2[0] - 0.15, np.repeat(ord2, 2), np.ones(1) * ord2[-1] + 0.15))\n",
    "#     ax.plot(o1e, np.repeat(np.arange(n + 1) / n, 2))\n",
    "#     ax.plot(o2e, np.repeat(np.arange(m + 1) / m, 2))\n",
    "#     plt.show()\n",
    "#     print(sup)\n",
    "    return sup\n",
    "\n",
    "def fit_line(samples, coords):\n",
    "    A = np.stack((coords, np.ones(coords.shape)), axis=-1)\n",
    "    pi = np.dot(np.linalg.inv(np.dot(A.T, A)), A.T)\n",
    "    return np.dot(pi, samples)\n",
    "\n",
    "def get_residuals(samples, coords, mb):\n",
    "    A = np.stack((coords, np.ones(coords.shape)), axis=-1)\n",
    "    pred = np.dot(A, mb)\n",
    "    return samples - pred\n",
    "\n",
    "def ks_test(samples, coords, discontinuity, p, borders):\n",
    "    sample_a = samples[borders:discontinuity - borders]\n",
    "    coords_a = coords[borders:discontinuity - borders]\n",
    "    sample_b = samples[discontinuity + borders:-borders]\n",
    "    coords_b = coords[discontinuity + borders: -borders]\n",
    "    mba = fit_line(sample_a, coords_a)\n",
    "    mbb = fit_line(sample_b, coords_b)\n",
    "    mbt = fit_line(samples[borders:-borders], coords[borders:-borders])\n",
    "    res_a = get_residuals(sample_a, coords_a, mba)\n",
    "    res_b = get_residuals(sample_b, coords_b, mbb)\n",
    "    res_t = get_residuals(samples[borders:-borders], coords[borders:-borders], mbt)\n",
    "#     cdfd_a = get_cdf_diff(res_a, res_t)\n",
    "#     cdfd_b = get_cdf_diff(res_b, res_t)\n",
    "    cdfd_t = get_cdf_diff(np.concatenate((res_a, res_b)), res_t)\n",
    "    n = samples.size - 4 * borders\n",
    "    m = n + 2 * borders\n",
    "    comp = np.sqrt(-np.log(0.5 * p) * 0.5 * (m + n) / (m * n))\n",
    "#     print(comp, cdfd_t)#cdfd_a, cdfd_b)\n",
    "    \n",
    "#     _, ax = plt.subplots(1, 1)\n",
    "#     ax.plot(coords, samples)\n",
    "#     ax.axvline(x=discontinuity + coords[0])\n",
    "#     plt.show()\n",
    "    res = cdfd_t < comp\n",
    "#     if res:\n",
    "#         print(comp, cdfd_t)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41bb2364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:13<00:00,  2.71it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ricky = ricker(101, 10)\n",
    "# morry = morlet2(101, 12, 1.7) * 2743 / 2168\n",
    "# _, ax = plt.subplots(1, 1)\n",
    "# ax.plot(ricky, c=\"r\")\n",
    "# ax.plot(morry, c=\"k\")\n",
    "# plt.show()\n",
    "\n",
    "# %%\n",
    "# 211107_015105 -- this is a good one\n",
    "# 211102_003909 -- another goodie. Lots of sleep, some time not in bed at the beginning.\n",
    "#               -- No wake up though.\n",
    "# 211101_002730 -- Excellent. 5 Sleep cycles visible. One spot not flipped right.\n",
    "######\n",
    "# the above ones are all old\n",
    "# 220103_232249 - this one has a long gap out of bed, very disturbed sleep, but a few deep sleep and REM blobs\n",
    "# 220105_005821 - pretty solid sleep, clear sleep cycles visible relatively evenly spaced, more deep sleep at the start and more REM at the end.\n",
    "# \n",
    "dt = \"220105_005821\"\n",
    "\n",
    "gl = sorted(glob(f\"sleepypi/run{dt}/*.pkl.gz\"))\n",
    "\n",
    "streams = []\n",
    "times = []\n",
    "\n",
    "# get timezone offset for local START of night, so that DST is handled appropriately\n",
    "uctdiff = datetime.strptime(dt, \"%y%m%d_%H%M%S\").astimezone().utcoffset()\n",
    "tzoffset = (uctdiff.days * 86400 + uctdiff.seconds) * 1000  # timezone offset from utc\n",
    "\n",
    "for idx in trange(len(gl)):\n",
    "    with gzip.open(gl[idx], \"rb\") as f:\n",
    "        p = pickle.load(f)\n",
    "        data_stream, fhist, gz, fri, tstamps, video = p\n",
    "        streams.append(data_stream)\n",
    "        times.append(tstamps.astype(np.int64) * 50 + 1609459200000 + tzoffset)\n",
    "        with VideoWriter(gl[idx][:-6] + \"mp4\") as vid:\n",
    "            vid.from_array(video)\n",
    "\n",
    "n = np.concatenate(streams, axis=0)\n",
    "# convert times back to epoch time in milliseconds, then to np.datetime64 in ms\n",
    "timestamps = np.concatenate(times, axis=0).astype(\"<M8[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87867e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rms0 = np.sqrt(get_mse(n[:, 0]))\n",
    "rms1 = np.sqrt(get_mse(n[:, 1]))\n",
    "rms_comb = np.sqrt(np.square(rms0) + np.square(rms1))\n",
    "\n",
    "sig0, sm0 = adjust_for_zero_trend(n[:, 0])\n",
    "sig1, sm1 = adjust_for_zero_trend(n[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea15a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "omega = 10.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)  # indices ~50-85 are breathing frequencies\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "z_wave = wavefinding_cwt(np.cumsum(sig0 + 1.0j * sig1), widths_morlet, omega)\n",
    "mags_z = np.abs(z_wave) / rms_comb.clip(1e-5)\n",
    "\n",
    "_, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True)\n",
    "# ax1.plot(sig0)\n",
    "# ax1.plot(sig1)\n",
    "ax1.plot(n[:, 0])\n",
    "ax1.plot(n[:, 1])\n",
    "ax1.plot(rms0, linestyle=\":\")\n",
    "ax1.plot(rms1, linestyle=\":\")\n",
    "# ax1.plot(sm0, linestyle=\":\")\n",
    "# ax1.plot(sm1, linestyle=\":\")\n",
    "ax2.plot(np.cumsum(sig0))\n",
    "ax2.plot(np.cumsum(sig1))\n",
    "ax3.imshow(mags_z.clip(max=np.percentile(mags_z, 98.0)), aspect=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fd3fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████▉| 161742/161815 [01:41<00:00, 1598.68it/s]\n",
      "100%|████████████████████████████████████████████████████▉| 161742/161815 [01:40<00:00, 1610.51it/s]\n"
     ]
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "r2_x, lgs = find_periodic(n[:, 0], freqs[40:80])\n",
    "r2_y, _ = find_periodic(n[:, 1], freqs[40:80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149e7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "x_env = np.abs(hilbert(n[:, 0]))\n",
    "y_env = np.abs(hilbert(n[:, 1]))\n",
    "\n",
    "fx, fy = get_mse_fracs(n)\n",
    "\n",
    "cutoff = 0.45\n",
    "\n",
    "r2 = r2_x * fx + r2_y * fy\n",
    "count = np.sum(r2 >= cutoff)\n",
    "rmses = np.sqrt(np.sum((r2 >= cutoff).astype(float) * (np.square(n[:, 0]) + np.square(n[:, 1]))) / count)\n",
    "condit = np.logical_and(r2 >= cutoff, np.logical_and(np.abs(n[:, 0]) < rms0 * 3.5, np.abs(n[:, 1]) < rms1 * 3.5))\n",
    "starts = np.argwhere(np.logical_and(condit[1:], np.logical_not(condit[:-1]))).squeeze()\n",
    "stops = np.argwhere(np.logical_and(np.logical_not(condit[1:]), condit[:-1])).squeeze()\n",
    "lrg_cond = np.argwhere(stops - starts > 35).squeeze()\n",
    "starts = starts[lrg_cond]\n",
    "stops = stops[lrg_cond]\n",
    "sm_cond = np.argwhere(starts[1:] - stops[:-1] > 2).squeeze()\n",
    "starts = starts[np.concatenate((np.array([0]), sm_cond + 1))]\n",
    "stops = stops[np.concatenate((sm_cond, np.array([stops.size - 1])))]\n",
    "\n",
    "mx_lead, mx_lag, bx_lead, bx_lag, rmsx_lead, rmsx_lag = fit_linear_lead_and_lag(n[:, 2], winsize=155)\n",
    "my_lead, my_lag, by_lead, by_lag, rmsy_lead, rmsy_lag = fit_linear_lead_and_lag(n[:, 3], winsize=155)\n",
    "mw_lead, mw_lag, bw_lead, bw_lag, rmsw_lead, rmsw_lag = fit_linear_lead_and_lag(n[:, 4], winsize=31)\n",
    "mz_lead, mz_lag, bz_lead, bz_lag, rmsz_lead, rmsz_lag = fit_linear_lead_and_lag(n[:, 5], winsize=31)\n",
    "\n",
    "x_rms_frac = rmsx_lead / (rmsx_lead + rmsx_lag)\n",
    "y_rms_frac = rmsy_lead / (rmsy_lead + rmsy_lag)\n",
    "bx_best = x_rms_frac * bx_lag + (1.0 - x_rms_frac) * bx_lead # np.where(rmsx_lead < rmsx_lag, bx_lead, bx_lag)\n",
    "by_best = y_rms_frac * by_lag + (1.0 - y_rms_frac) * by_lead # np.where(rmsy_lead < rmsy_lag, by_lead, by_lag)\n",
    "w_rms_frac = rmsw_lead / (rmsw_lead + rmsw_lag)\n",
    "z_rms_frac = rmsz_lead / (rmsz_lead + rmsz_lag)\n",
    "bw_best = w_rms_frac * bw_lag + (1.0 - w_rms_frac) * bw_lead # np.where(rmsx_lead < rmsx_lag, bx_lead, bx_lag)\n",
    "bz_best = z_rms_frac * bz_lag + (1.0 - z_rms_frac) * bz_lead # np.where(rmsy_lead < rmsy_lag, by_lead, by_lag)\n",
    "\n",
    "b_best = np.stack((bx_best, by_best), axis=-1)\n",
    "\n",
    "res_x = rpt.KernelCPD(kernel=\"linear\", min_size=100).fit(n[:, 2:4]).predict(pen=4)\n",
    "# res_y = rpt.KernelCPD(kernel=\"linear\", min_size=100).fit(n[:, 4:6]).predict(pen=25)\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(timestamps, my_lead)\n",
    "ax1.plot(timestamps, my_lag)\n",
    "ax1.plot(timestamps, bx_best)\n",
    "ax1.plot(timestamps, by_best)\n",
    "ax1.plot(timestamps, bw_best)\n",
    "ax1.plot(timestamps, bz_best)\n",
    "ax1.plot(timestamps, rmsy_lead.clip(max=10.0) * 20)\n",
    "ax1.plot(timestamps, rmsy_lag.clip(max=10.0) * 20)\n",
    "ax1.plot(timestamps, n[:, 2:4], alpha=0.6)\n",
    "ax1.plot(timestamps, n[:, 4:6], alpha=0.2)\n",
    "# ax1.legend(\"mylead,mylag,bxbest,bybest,mrsylead,rmsylag,n2,n3,n4,n5\".split(\",\"))\n",
    "ax2.plot(timestamps, n[:, 0:2])\n",
    "ax2.axhline(y=rmses * 10.0)\n",
    "ax2.axhline(y=rmses * -10.0)\n",
    "ax2.plot(timestamps, r2 * 1e-3, c=\"k\", alpha=0.3, linestyle=\":\")\n",
    "# ax2.plot(timestamps, x_env)\n",
    "# ax2.plot(timestamps, y_env)\n",
    "\n",
    "keep = np.ones(len(res_x), dtype=bool)\n",
    "\n",
    "for idx in range(len(res_x) - 2):\n",
    "    xs = np.arange(res_x[idx], res_x[idx + 2])\n",
    "    res_a = ks_test(n[res_x[idx]: res_x[idx + 2], 2], xs, res_x[idx + 1] - res_x[idx], 0.01, 8)\n",
    "    res_b = ks_test(n[res_x[idx]: res_x[idx + 2], 3], xs, res_x[idx + 1] - res_x[idx], 0.01, 8)\n",
    "    if res_a and res_b:\n",
    "        keep[idx + 1] = False\n",
    "\n",
    "res_x = np.array(res_x)[keep]\n",
    "\n",
    "keep = np.ones(len(res_x), dtype=bool)\n",
    "for idx in range(len(res_x) - 2):\n",
    "    xs = np.arange(res_x[idx], res_x[idx + 2])\n",
    "    res_a = ks_test(n[res_x[idx]: res_x[idx + 2], 2], xs, res_x[idx + 1] - res_x[idx], 0.02, 8)\n",
    "    res_b = ks_test(n[res_x[idx]: res_x[idx + 2], 3], xs, res_x[idx + 1] - res_x[idx], 0.02, 8)\n",
    "    if res_a and res_b:\n",
    "        keep[idx + 1] = False\n",
    "\n",
    "res_x = np.array(res_x)[keep]\n",
    "\n",
    "for idx, ln in enumerate(res_x[:-1]):\n",
    "    ax1.axvline(x=timestamps[ln])\n",
    "    frac = np.count_nonzero(condit[ln:res_x[idx + 1]]) / (res_x[idx + 1] - ln)\n",
    "    if frac < 0.5:\n",
    "        ax1.axvspan(timestamps[ln], timestamps[res_x[idx + 1]], color=\"r\", alpha=0.2, ec=None)\n",
    "# for idx, ln in enumerate(res_y[:-1]):\n",
    "#     ax1.axvline(x=timestamps[ln])\n",
    "for idx in range(starts.size):\n",
    "    ax2.axvspan(timestamps[starts[idx]], timestamps[stops[idx]], color=\"g\", alpha=0.2)\n",
    "# for ln in res_y[:-1]:\n",
    "#     ax1.axvline(x=timestamps[ln], c=\"r\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "rms_slide_0 = np.zeros_like(rms0)\n",
    "rms_slide_1 = np.zeros_like(rms1)\n",
    "rms_avg_0 = np.zeros_like(starts, dtype=float)\n",
    "rms_avg_1 = np.zeros_like(starts, dtype=float)\n",
    "windows = np.zeros_like(rms0)\n",
    "windows_wide = np.zeros_like(rms0)\n",
    "rms_avg_0[2] = 1.5\n",
    "parity = np.zeros_like(rms0)\n",
    "\n",
    "for idx in range(starts.size):\n",
    "    lg = stops[idx] - starts[idx]\n",
    "    win = 1.0 / (1.0 + np.exp((np.abs(np.arange(lg) - lg / 2.0 - 0.5) + 4.0 - lg / 2.0).clip(min=-10.0)))\n",
    "    win_wide = 1.0 / (1.0 + np.exp((0.3 * np.abs(np.arange(lg) - lg / 2.0 - 0.5) + 5.0 - 0.3 * lg / 2.0).clip(min=-10.0)))\n",
    "    win_lop = 1.0 / (1.0 + np.exp((-0.015 * np.arange(lg) + 5.0).clip(min=-10.0)))\n",
    "    winsum = np.sum(win)\n",
    "    if winsum == 0:\n",
    "        print(idx, lg, starts[idx], stops[idx], starts.size)\n",
    "    windows[starts[idx]:stops[idx]] = win\n",
    "    windows_wide[starts[idx]:stops[idx]] = win_wide * win_lop\n",
    "    rms_avg_0[idx] = np.sqrt(np.sum(np.square(n[starts[idx]:stops[idx], 0]) * win) / winsum)\n",
    "    rms_avg_1[idx] = np.sqrt(np.sum(np.square(n[starts[idx]:stops[idx], 1]) * win) / winsum)\n",
    "    parity[idx] = np.sign(np.sum(n[starts[idx]:stops[idx], 0] * n[starts[idx]:stops[idx], 1]))\n",
    "    rms_slide_0[starts[idx]:stops[idx]] = rms_avg_0[idx]\n",
    "    rms_slide_1[starts[idx]:stops[idx]] = rms_avg_1[idx]\n",
    "    if idx == 0:\n",
    "        rms_slide_0[:starts[0]] = rms_avg_0[0]\n",
    "        rms_slide_1[:starts[0]] = rms_avg_1[0]\n",
    "    else:\n",
    "        lg = starts[idx] - stops[idx - 1]\n",
    "        rms_slide_0[stops[idx - 1]:starts[idx]] = np.linspace(rms_avg_0[idx - 1], rms_avg_0[idx], lg + 1)[:-1]\n",
    "        rms_slide_1[stops[idx - 1]:starts[idx]] = np.linspace(rms_avg_1[idx - 1], rms_avg_1[idx], lg + 1)[:-1]\n",
    "rms_slide_0[stops[-1]:] = rms_avg_0[-1]\n",
    "rms_slide_1[stops[-1]:] = rms_avg_1[-1]\n",
    "\n",
    "sig_norm_0 = n[:, 0] / rms_slide_0.clip(min=1e-9)\n",
    "sig_norm_1 = n[:, 1] / rms_slide_1.clip(min=1e-9)\n",
    "\n",
    "sqsecdrv = np.zeros_like(n)\n",
    "sqsecdrv[1:-1, 0] = np.square(sig_norm_0[:-2] + sig_norm_0[2:] - 2.0 * sig_norm_0[1:-1]) * 0.25\n",
    "sqsecdrv[1:-1, 1] = np.square(sig_norm_1[:-2] + sig_norm_1[2:] - 2.0 * sig_norm_1[1:-1]) * 0.25\n",
    "sqsecdrv = sqsecdrv.clip(max=4.0)\n",
    "\n",
    "spline0 = cspline1d(sig_norm_0, lamb=1)\n",
    "spline1 = cspline1d(sig_norm_1, lamb=1)\n",
    "noise0 = spline0 - sig_norm_0\n",
    "noise1 = spline1 - sig_norm_1\n",
    "noise0_rms = np.zeros_like(noise0)\n",
    "noise1_rms = np.zeros_like(noise0)\n",
    "flip0 = np.zeros_like(noise0)\n",
    "flip1 = np.zeros_like(noise0)\n",
    "keep = np.ones_like(starts, dtype=bool)\n",
    "slopes0 = np.zeros_like(starts, dtype=float)\n",
    "slopes1 = np.zeros_like(starts, dtype=float)\n",
    "snr0 = np.zeros_like(starts, dtype=float)\n",
    "snr1 = np.zeros_like(starts, dtype=float)\n",
    "diff_sl = np.zeros_like(starts, dtype=float)\n",
    "mult0 = np.zeros_like(starts, dtype=float)\n",
    "mult1 = np.zeros_like(starts, dtype=float)\n",
    "mid_mult0 = np.ones_like(rms_slide_0)\n",
    "mid_mult1 = np.ones_like(rms_slide_1)\n",
    "\n",
    "\n",
    "for idx in range(starts.size):\n",
    "    lg = stops[idx] - starts[idx]\n",
    "    win = 1.0 / (1.0 + np.exp((np.abs(np.arange(lg) - lg / 2.0 - 0.5) + 7.0 - lg / 2.0).clip(min=-10.0)))\n",
    "    winsum = np.sum(win)\n",
    "#     n0_val = np.sqrt(np.sum(np.square(noise0[starts[idx]:stops[idx]]) * win) / winsum)\n",
    "#     n1_val = np.sqrt(np.sum(np.square(noise1[starts[idx]:stops[idx]]) * win) / winsum)\n",
    "    n0_val = np.sum(sqsecdrv[starts[idx]:stops[idx], 0] * win) / winsum\n",
    "    n1_val = np.sum(sqsecdrv[starts[idx]:stops[idx], 1] * win) / winsum\n",
    "    per_val_0 = np.sum(r2_x[starts[idx]:stops[idx]] * win) / winsum\n",
    "    per_val_1 = np.sum(r2_y[starts[idx]:stops[idx]] * win) / winsum\n",
    "    noise0_rms[starts[idx]:stops[idx]] = n0_val\n",
    "    noise1_rms[starts[idx]:stops[idx]] = n1_val\n",
    "    m0 = spline0[starts[idx] + 1:stops[idx]] - spline0[starts[idx]:stops[idx] - 1]\n",
    "    m1 = spline1[starts[idx] + 1:stops[idx]] - spline1[starts[idx]:stops[idx] - 1]\n",
    "    slopes0[idx] = np.mean(m0[m0>0]) + np.mean(m0[m0<0])\n",
    "    slopes1[idx] = np.mean(m1[m1>0]) + np.mean(m1[m1<0])\n",
    "    diff_sl[idx] = np.sign(slopes0[idx] + parity[idx] * slopes1[idx])\n",
    "    snr0[idx] = n0_val\n",
    "    snr1[idx] = n1_val\n",
    "    if min(n0_val, n1_val) > 0.03:  # empirical value that does pretty well\n",
    "        windows[starts[idx]:stops[idx]] = 0.0\n",
    "        keep[idx] = False\n",
    "    mult0[idx] = np.exp(-35.0 * n0_val) * per_val_0\n",
    "    mult1[idx] = np.exp(-35.0 * n1_val) * per_val_1\n",
    "    mid_mult0[starts[idx]:stops[idx]] = mult0[idx]\n",
    "    mid_mult1[starts[idx]:stops[idx]] = mult1[idx]\n",
    "    flip0[starts[idx]:stops[idx]] = diff_sl[idx]\n",
    "    flip1[starts[idx]:stops[idx]] = diff_sl[idx] * parity[idx]\n",
    "    if idx == 0:\n",
    "        mid_mult0[:starts[0]] = mult0[idx]\n",
    "        mid_mult1[:starts[0]] = mult1[idx]\n",
    "        flip0[:starts[0]] = diff_sl[idx]\n",
    "        flip1[:starts[0]] = diff_sl[idx] * parity[idx]\n",
    "    else:\n",
    "        lg = starts[idx] - stops[idx - 1]\n",
    "        mid_mult0[stops[idx - 1]:starts[idx]] = np.linspace(mult0[idx - 1], mult0[idx], lg + 1)[:-1]\n",
    "        mid_mult1[stops[idx - 1]:starts[idx]] = np.linspace(mult1[idx - 1], mult1[idx], lg + 1)[:-1]\n",
    "        flip0[stops[idx - 1]:stops[idx-1] + lg // 2] = diff_sl[idx - 1]\n",
    "        flip1[stops[idx - 1]:stops[idx-1] + lg // 2] = diff_sl[idx - 1] * parity[idx - 1]\n",
    "        flip0[stops[idx-1] + lg // 2:starts[idx]] = diff_sl[idx]\n",
    "        flip1[stops[idx-1] + lg // 2:starts[idx]] = diff_sl[idx] * parity[idx]\n",
    "flip0[stops[-1]:] = diff_sl[-1]\n",
    "flip1[stops[-1]:] = diff_sl[-1] * parity[-1]\n",
    "\n",
    "starts = starts[keep]\n",
    "stops = stops[keep]\n",
    "slopes0 = slopes0[keep]\n",
    "slopes1 = slopes1[keep]\n",
    "rms_avg_0 = rms_avg_0[keep]\n",
    "rms_avg_1 = rms_avg_1[keep]\n",
    "snr0 = snr0[keep]\n",
    "snr1 = snr1[keep]\n",
    "mult0 = mult0[keep]\n",
    "mult1 = mult1[keep]\n",
    "\n",
    "for idx in range(starts.size):\n",
    "    rms_slide_0[starts[idx]:stops[idx]] = rms_avg_0[idx]\n",
    "    rms_slide_1[starts[idx]:stops[idx]] = rms_avg_1[idx]\n",
    "    mid_mult0[starts[idx]:stops[idx]] = mult0[idx]\n",
    "    mid_mult1[starts[idx]:stops[idx]] = mult1[idx]\n",
    "    if idx == 0:\n",
    "        rms_slide_0[:starts[0]] = rms_avg_0[0]\n",
    "        rms_slide_1[:starts[0]] = rms_avg_1[0]\n",
    "        mid_mult0[:starts[0]] = mult0[idx]\n",
    "        mid_mult1[:starts[0]] = mult1[idx]\n",
    "    else:\n",
    "        lg = starts[idx] - stops[idx - 1]\n",
    "        rms_slide_0[stops[idx - 1]:starts[idx]] = np.linspace(rms_avg_0[idx - 1], rms_avg_0[idx], lg + 1)[:-1]\n",
    "        rms_slide_1[stops[idx - 1]:starts[idx]] = np.linspace(rms_avg_1[idx - 1], rms_avg_1[idx], lg + 1)[:-1]\n",
    "        mid_mult0[stops[idx - 1]:starts[idx]] = np.linspace(mult0[idx - 1], mult0[idx], lg + 1)[:-1]\n",
    "        mid_mult1[stops[idx - 1]:starts[idx]] = np.linspace(mult1[idx - 1], mult1[idx], lg + 1)[:-1]\n",
    "rms_slide_0[stops[-1]:] = rms_avg_0[-1]\n",
    "rms_slide_1[stops[-1]:] = rms_avg_1[-1]\n",
    "mid_mult0[stops[-1]:] = mult0[-1]\n",
    "mid_mult1[stops[-1]:] = mult1[-1]\n",
    "\n",
    "sig_norm_0 = n[:, 0] / rms_slide_0.clip(min=1e-9)\n",
    "sig_norm_1 = n[:, 1] / rms_slide_1.clip(min=1e-9)\n",
    "\n",
    "omega = 6.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)  # indices ~50-85 are breathing frequencies\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)[30:90]\n",
    "x_wave = wavefinding_cwt(sig_norm_0 * windows, widths_morlet, omega)\n",
    "y_wave = wavefinding_cwt(sig_norm_1 * windows, widths_morlet, omega)\n",
    "mags_z = np.abs(x_wave) + np.abs(y_wave)\n",
    "\n",
    "_, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True)\n",
    "ax1.plot(n[:, 0])\n",
    "ax1.plot(n[:, 1])\n",
    "ax1.plot(rms_slide_0)\n",
    "ax1.plot(rms_slide_1)\n",
    "ax1.legend([\"x\", \"y\", \"rmsx\", \"rmsy\"])\n",
    "ax2.plot(sig_norm_0 * windows)\n",
    "ax2.plot(sig_norm_1 * windows)\n",
    "ax2.plot(sig_norm_0.clip(min=-5.0, max=5.0), alpha=0.2)\n",
    "ax2.plot(sig_norm_1.clip(min=-5.0, max=5.0), alpha=0.2)\n",
    "# ax2.plot(flip0)\n",
    "# ax2.plot(flip1)\n",
    "# ax2.plot(sqsecdrv[:, 0] * windows)\n",
    "# ax2.plot(sqsecdrv[:, 1] * windows)\n",
    "# ax2.plot(r2_x * windows)\n",
    "# ax2.plot(r2_y * windows)\n",
    "# ax2.plot(mid_mult0)\n",
    "# ax2.plot(mid_mult1)\n",
    "ax2.plot(np.minimum(noise0_rms, noise1_rms) * 100.0)\n",
    "ax2.legend([\"sn0\", \"sn1\", \"f0\", \"f1\", \"c0\", \"c1\", \"r2x\", \"r2y\", \"mm0\", \"mm1\", \"n0\", \"n1\"])\n",
    "ax3.imshow(mags_z.clip(max=np.percentile(mags_z, 99.9)), aspect=\"auto\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533b29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next up, we want to flip the signals upright\n",
    "# and then combine them into one signal (after adjusting for their SNR and r2 of their periodicity)\n",
    "# For each segment, cspline1d, determine if up-slopes or down-slopes have larger slope magnitudes\n",
    "# upright segments should have more gradual upwards slopes\n",
    "# (there's a slight pause after exhaling before inhaling, doesn't happen at inhale-exhale transition)\n",
    "plt.close(\"all\")\n",
    "\n",
    "angles = np.arctan2(mid_mult1, mid_mult0)\n",
    "\n",
    "sig0_scale = -n[:, 0] * flip0 * np.square(np.cos(angles)) / rms_slide_0.clip(min=1e-9)\n",
    "sig1_scale = -n[:, 1] * flip1 * np.square(np.sin(angles)) / rms_slide_1.clip(min=1e-9)\n",
    "sig = sig0_scale + sig1_scale\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.plot(sig_norm_0 * windows)\n",
    "ax.plot(sig_norm_1 * windows)\n",
    "ax.plot(sig * windows)\n",
    "ax.plot(np.square(np.cos(angles)) * windows)\n",
    "ax.plot(np.square(np.sin(angles)) * windows)\n",
    "ax.legend(\"sn0,sn1,comb,cos2,sin2\".split(\",\"))\n",
    "plt.show()\n",
    "\n",
    "# still TO DO:\n",
    "# funky shit going on with scales of signorm traces at 1050, ~4000, 114000, 115000, probably other places\n",
    "# 118800 is upside down\n",
    "# add in r2 from periodicity - not sure of the best way to do this - adding their effects? multiplying? It is exp so probs adding...\n",
    "# make sure at least that the parity is correct - get correlation between two signals and make sure that result follows\n",
    "# somehow 110000 gets into the periodic signal section?? How?? It's so angular and small\n",
    "# 95000 - high frequency but clearly noise. Should not have made it into periodic.\n",
    "# 140600 - also should not have made it in\n",
    "# 137200 - also should not have made it in\n",
    "# 142775 - nopers\n",
    "# 142600 - should have  made it in\n",
    "# 15350 - no\n",
    "# 14990 - no\n",
    "# 2100 - kinda no\n",
    "# 4600-4800 - no, out of bed\n",
    "# 6000 - no\n",
    "# 8950 - 9250 - no\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588a52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after combining, find breathing frequency\n",
    "omega = 5.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)  # indices ~50-85 are breathing frequencies\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "wave = wavefinding_cwt(sig * windows, widths_morlet, omega)\n",
    "\n",
    "angles = np.angle(wave)\n",
    "mags = np.square(np.abs(wave))\n",
    "cols = np.mod(angles * 0.5 / np.pi + 0.5, 1)\n",
    "col_spec = cols - 0.5\n",
    "dcol = col_spec[:, 1:] - col_spec[:, :-1]\n",
    "dcol[dcol < -0.5] += 1.0\n",
    "dcol[dcol > 0.5] -= 1.0\n",
    "dcols = cm.hsv(dcol.clip(-0.00625, 0.00625) * 80.0 + 0.5)[..., :3]\n",
    "dcols *= (mags[:, 1:, None] / np.percentile(mags, 95.0)).clip(max=1.0)\n",
    "cols = cm.hsv(cols)[..., :3]\n",
    "cols *= mags[..., None]\n",
    "\n",
    "# ddcol = dcol[50:85] - dcol[49:84]\n",
    "scale = 500.0\n",
    "ddcol = dcol[1:] - dcol[:-1]\n",
    "sharpen_data = correlate1d(np.exp(-scale * np.abs(dcol)), np.exp(-0.5 * np.square((np.arange(101) - 50.0) / 25.0)), axis=1)\n",
    "rate_data = np.square(mags[50:90, 1:]) * ddcol[49:89].clip(0.0) * sharpen_data[50:90]\n",
    "inst_rate = np.sum(np.arange(50, 90)[:, None] * rate_data, axis=0) / np.sum(rate_data, axis=0).clip(1e-5)\n",
    "inst_var = np.sum(np.square(np.arange(50, 90)[:, None] - inst_rate) * rate_data, axis=0) / np.sum(rate_data, axis=0).clip(1e-5)\n",
    "inst_std = np.sqrt(inst_var)\n",
    "inst_std_sm = correlate(inst_std, np.ones(31), mode=\"same\")\n",
    "inst_rate_fix = np.copy(inst_rate)\n",
    "\n",
    "for idx in range(starts.size):\n",
    "    lgb = min(stops[idx] - starts[idx], 100)\n",
    "    lge = min(stops[idx - 1] - starts[idx - 1], 100)\n",
    "    begin_avg = np.mean(inst_rate[starts[idx]:starts[idx] + lgb])\n",
    "    if idx == 0:\n",
    "        start_side = begin_avg\n",
    "        inst_rate_fix[:starts[0]] = begin_avg\n",
    "    elif idx == starts.size - 1:\n",
    "        final_side = np.mean(inst_rate[stops[idx] - lge:stops[idx]])\n",
    "        inst_rate_fix[stops[-1]:] = final_side\n",
    "    else:\n",
    "        end_avg = np.mean(inst_rate[stops[idx - 1] - lge:stops[idx - 1]])\n",
    "        space = starts[idx] - stops[idx - 1]\n",
    "        inst_rate_fix[stops[idx - 1]:starts[idx]] = np.linspace(end_avg, begin_avg, space + 1)[:-1]\n",
    "\n",
    "inst_rate_fix = cspline1d(inst_rate_fix, 0.1)\n",
    "inst_rate_sm = cspline1d(inst_rate_fix, 1e10)\n",
    "        \n",
    "stats_size = 1300\n",
    "interp_rate = np.concatenate((np.ones(stats_size) * start_side, inst_rate_fix, np.ones(stats_size) * final_side))\n",
    "\n",
    "curve_size = 15\n",
    "curve_fw = curve_size * 2 + 1\n",
    "x = np.arange(curve_fw) - curve_size\n",
    "A = np.stack((np.square(x), x, np.ones(curve_fw)), axis=1)\n",
    "krn = np.dot(np.linalg.inv(np.dot(A.T, A)), A.T)\n",
    "gskrn = np.exp(-0.5 * np.square((np.arange(401) - 200)/100))\n",
    "\n",
    "crv = np.square(100.0 * correlate(interp_rate, krn[0], mode=\"same\")[stats_size:-stats_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ea0900-9fdb-445a-a848-f0e5a00f2a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring 2 ( 15 )\n",
      "ignoring 3 ( 14 )\n",
      "ignoring 5 ( 11 )\n",
      "ignoring 9 ( 20 )\n",
      "ignoring 10 ( 14 )\n",
      "ignoring 11 ( 5 )\n",
      "ignoring 12 ( 34 )\n",
      "ignoring 15 ( 20 )\n",
      "ignoring 16 ( 4 )\n",
      "ignoring 17 ( 44 )\n",
      "ignoring 18 ( 12 )\n",
      "ignoring 21 ( 24 )\n",
      "ignoring 22 ( 4 )\n",
      "ignoring 24 ( 22 )\n",
      "ignoring 27 ( 4 )\n",
      "ignoring 28 ( 16 )\n",
      "ignoring 29 ( 10 )\n",
      "ignoring 30 ( 21 )\n",
      "ignoring 31 ( 35 )\n",
      "ignoring 32 ( 30 )\n",
      "ignoring 33 ( 21 )\n",
      "ignoring 34 ( 22 )\n",
      "ignoring 38 ( 5 )\n",
      "ignoring 39 ( 8 )\n",
      "ignoring 40 ( 17 )\n",
      "ignoring 42 ( 10 )\n",
      "ignoring 43 ( 14 )\n",
      "ignoring 44 ( 10 )\n",
      "ignoring 49 ( 3 )\n",
      "ignoring 51 ( 3 )\n",
      "ignoring 52 ( 9 )\n",
      "ignoring 55 ( 22 )\n",
      "ignoring 59 ( 13 )\n",
      "ignoring 62 ( 19 )\n",
      "ignoring 63 ( 50 )\n",
      "ignoring 64 ( 9 )\n",
      "ignoring 66 ( 47 )\n",
      "ignoring 69 ( 13 )\n",
      "ignoring 71 ( 39 )\n",
      "ignoring 73 ( 9 )\n",
      "ignoring 74 ( 39 )\n",
      "ignoring 76 ( 14 )\n",
      "ignoring 77 ( 8 )\n",
      "ignoring 80 ( 12 )\n",
      "ignoring 81 ( 11 )\n",
      "ignoring 82 ( 26 )\n",
      "ignoring 83 ( 40 )\n",
      "ignoring 84 ( 16 )\n",
      "ignoring 86 ( 48 )\n",
      "ignoring 87 ( 40 )\n",
      "ignoring 95 ( 33 )\n",
      "ignoring 98 ( 20 )\n",
      "ignoring 99 ( 40 )\n",
      "ignoring 100 ( 13 )\n",
      "ignoring 101 ( 3 )\n",
      "ignoring 103 ( 14 )\n",
      "ignoring 105 ( 9 )\n",
      "ignoring 106 ( 11 )\n",
      "ignoring 111 ( 24 )\n",
      "ignoring 112 ( 14 )\n",
      "ignoring 114 ( 9 )\n",
      "ignoring 116 ( 43 )\n",
      "ignoring 117 ( 3 )\n",
      "ignoring 118 ( 9 )\n",
      "ignoring 119 ( 8 )\n",
      "ignoring 120 ( 11 )\n",
      "ignoring 121 ( 3 )\n",
      "ignoring 124 ( 3 )\n",
      "ignoring 131 ( 12 )\n",
      "ignoring 132 ( 49 )\n",
      "ignoring 133 ( 15 )\n",
      "ignoring 134 ( 28 )\n",
      "ignoring 137 ( 43 )\n",
      "ignoring 138 ( 15 )\n",
      "ignoring 140 ( 10 )\n",
      "ignoring 142 ( 5 )\n",
      "ignoring 143 ( 10 )\n",
      "ignoring 147 ( 26 )\n",
      "ignoring 149 ( 35 )\n",
      "ignoring 151 ( 34 )\n",
      "ignoring 154 ( 14 )\n",
      "ignoring 156 ( 11 )\n",
      "ignoring 157 ( 20 )\n",
      "ignoring 160 ( 23 )\n",
      "ignoring 161 ( 11 )\n",
      "ignoring 162 ( 3 )\n",
      "ignoring 163 ( 21 )\n",
      "ignoring 165 ( 22 )\n",
      "ignoring 167 ( 21 )\n",
      "ignoring 168 ( 7 )\n",
      "0 [] []\n",
      "1 [] []\n",
      "2 [] []\n",
      "3 [] []\n",
      "4 [] []\n",
      "5 [] []\n",
      "6 [] []\n",
      "7 [] []\n",
      "8 [] []\n",
      "9 [] []\n",
      "10 [] []\n",
      "11 [] []\n",
      "12 [] []\n",
      "13 [] []\n",
      "14 [] []\n",
      "15 [] []\n",
      "16 [] []\n",
      "17 [] []\n",
      "18 [] []\n",
      "19 [] []\n",
      "20 [] []\n",
      "21 [] []\n",
      "22 [] []\n",
      "23 [] []\n",
      "24 [] []\n",
      "25 [] []\n",
      "26 [] []\n",
      "27 [] []\n",
      "28 [] []\n",
      "29 [] []\n",
      "30 [] []\n",
      "31 [] []\n",
      "32 [] []\n",
      "33 [] []\n",
      "34 [] []\n",
      "35 [] []\n",
      "36 [] []\n",
      "37 [] []\n",
      "38 [] []\n",
      "39 [] []\n",
      "40 [] []\n",
      "41 [] []\n",
      "42 [] []\n",
      "43 [] []\n",
      "44 [] []\n",
      "45 [] []\n",
      "46 [] []\n",
      "47 [] []\n",
      "48 [] []\n",
      "49 [] []\n",
      "50 [] []\n",
      "51 [] []\n",
      "52 [] []\n",
      "53 [] []\n",
      "54 [] []\n",
      "55 [] []\n",
      "56 [] []\n",
      "57 [] []\n",
      "58 [] []\n",
      "59 [] []\n",
      "60 [] []\n",
      "61 [] []\n",
      "62 [] []\n",
      "63 [] []\n",
      "64 [] []\n",
      "65 [] []\n",
      "629\n",
      "682\n",
      "293\n",
      "deleting 2\n",
      "223\n",
      "deleting 3\n",
      "171\n",
      "deleting 4\n",
      "321\n",
      "deleting 5\n",
      "245\n",
      "deleting 6\n",
      "174\n",
      "deleting 7\n",
      "402\n",
      "774\n",
      "1445\n",
      "239\n",
      "deleting 11\n",
      "129\n",
      "deleting 12\n",
      "73\n",
      "deleting 13\n",
      "61\n",
      "deleting 14\n",
      "723\n",
      "1336\n",
      "1339\n",
      "407\n",
      "42\n",
      "deleting 19\n",
      "253\n",
      "deleting 20\n",
      "965\n",
      "252\n",
      "deleting 22\n",
      "129\n",
      "deleting 23\n",
      "187\n",
      "deleting 24\n",
      "262\n",
      "deleting 25\n",
      "403\n",
      "deleting 26\n",
      "200\n",
      "deleting 27\n",
      "347\n",
      "deleting 28\n",
      "49\n",
      "deleting 29\n",
      "1424\n",
      "234\n",
      "deleting 31\n",
      "915\n",
      "231\n",
      "deleting 33\n",
      "181\n",
      "deleting 34\n",
      "676\n",
      "444\n",
      "deleting 36\n",
      "418\n",
      "1356\n",
      "1238\n",
      "229\n",
      "deleting 40\n",
      "172\n",
      "deleting 41\n",
      "28\n",
      "deleting 42\n",
      "183\n",
      "deleting 43\n",
      "105\n",
      "deleting 44\n",
      "101\n",
      "deleting 45\n",
      "259\n",
      "deleting 46\n",
      "262\n",
      "deleting 47\n",
      "178\n",
      "deleting 48\n",
      "297\n",
      "deleting 49\n",
      "597\n",
      "753\n",
      "1847\n",
      "554\n",
      "200\n",
      "deleting 54\n",
      "836\n",
      "656\n",
      "266\n",
      "deleting 57\n",
      "224\n",
      "deleting 58\n",
      "306\n",
      "deleting 59\n",
      "106\n",
      "deleting 60\n",
      "667\n",
      "57\n",
      "deleting 62\n",
      "236\n",
      "deleting 63\n",
      "191\n",
      "deleting 64\n",
      "56\n",
      "deleting 65\n",
      "23\n",
      "[ 12902  14837  37931  39578  40785  64988  66227  69894  72227  75043\n",
      " 101928 105270 109465 111751 114500 118235 138411 140786 142443 144873\n",
      " 146734 148175 156365]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.close(\"all\")\n",
    "\n",
    "slp = np.square(10.0 * correlate(interp_rate, krn[1], mode=\"same\"))[curve_fw:-curve_fw]\n",
    "slp[stats_size-curve_fw:curve_fw-stats_size] *= windows_wide[:-1]\n",
    "slp_sm = correlate(slp, gskrn, mode=\"same\")[curve_fw:-curve_fw] / np.sum(gskrn)\n",
    "\n",
    "# thoughts:\n",
    "# if slp_sm > 0.85 (empirical) then possibly counts\n",
    "# Join together groups of those which are > 0.85 if they're within a certain distance\n",
    "# If not large enough, get rid of it.\n",
    "# But then there's also a physiology question...\n",
    "# if there's a good bout of movement, you probably don't go into REM right away? Or no...\n",
    "rem_starts = np.argwhere(np.logical_and(slp_sm[1:] >= 0.85, slp_sm[:-1] < 0.85)).squeeze()\n",
    "rem_stops = np.argwhere(np.logical_and(slp_sm[1:] < 0.85, slp_sm[:-1] >= 0.85)).squeeze()\n",
    "\n",
    "keep = np.ones(starts.size + 1, dtype=bool)\n",
    "for idx in range(starts.size - 1):\n",
    "    rem_lg = starts[idx + 1] - stops[idx]\n",
    "    if rem_lg <= 50:\n",
    "        keep[idx + 1] = False\n",
    "        print(\"ignoring\", idx, \"(\", rem_lg, \")\")\n",
    "\n",
    "ns_starts = starts[keep[:-1]] + stats_size\n",
    "ns_stops = stops[keep[1:]] + stats_size\n",
    "\n",
    "keep = np.ones(rem_starts.size + 1, dtype=bool)\n",
    "for idx in range(rem_starts.size - 1):\n",
    "    gap_lg = rem_starts[idx + 1] - rem_stops[idx]\n",
    "    if gap_lg <= 360:\n",
    "        keep[idx + 1] = False\n",
    "\n",
    "rem_starts = rem_starts[keep[:-1]]\n",
    "rem_stops = rem_stops[keep[1:]]\n",
    "\n",
    "rem_starts_cut = []\n",
    "rem_stops_cut = []\n",
    "\n",
    "# 6 possibilities:\n",
    "# starts and stops without any gaps - accept both start and stop.\n",
    "# starts within a gap, gap ends before stop - move the start to the end of the gap.\n",
    "# gap starts after start, before stop, gap ends after stop - move stop up to the start of the gap.\n",
    "# gap encompasses start and stop - delete start and stop.\n",
    "# start and stop encompass gap - add in gap stop and start into start and stop.\n",
    "# gap ends after start and a new one begins before stop - cut off the start and the end.\n",
    "\n",
    "for idx in range(rem_starts.size):\n",
    "    starts_in_gap = np.logical_and(ns_starts > rem_starts[idx], ns_starts < rem_stops[idx])\n",
    "    stops_in_gap = np.logical_and(ns_stops > rem_starts[idx], ns_stops < rem_stops[idx])\n",
    "    starts_in_gap_args = np.argwhere(starts_in_gap).squeeze(1)\n",
    "    stops_in_gap_args = np.argwhere(stops_in_gap).squeeze(1)\n",
    "    print(idx, starts_in_gap_args, stops_in_gap_args)\n",
    "    \n",
    "    if not (np.any(starts_in_gap) or np.any(stops_in_gap)):\n",
    "        last_start = np.argmax(ns_starts <= rem_starts[idx])\n",
    "        if ns_stops[last_start] < rem_starts[idx]:\n",
    "            rem_starts_cut.append(rem_starts[idx])\n",
    "            rem_stops_cut.append(rem_stops[idx])\n",
    "        continue\n",
    "    \n",
    "    if np.count_nonzero(starts_in_gap) == np.count_nonzero(stops_in_gap):\n",
    "        if np.all(starts_in_gap_args == stops_in_gap_args):\n",
    "            # we have gaps to open up in the thing, but all of them are contained\n",
    "            rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "            rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "        else:\n",
    "            # keep the start and end\n",
    "            rem_starts_cut.append(rem_starts[idx])\n",
    "            rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "            rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "            rem_stops_cut.append(rem_stops[idx])\n",
    "        continue\n",
    "    elif np.count_nonzero(starts_in_gap) == 0:\n",
    "        rem_starts_cut.append(rem_starts[idx])\n",
    "        rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "    elif np.count_nonzero(stops_in_gap) == 0:\n",
    "        rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "        rem_stops_cut.append(rem_stops[idx])\n",
    "    else:\n",
    "        if starts_in_gap_args[0] == stops_in_gap_args[0]:\n",
    "            rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "            rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "            rem_stops_cut.append(rem_stops[idx])\n",
    "        else:\n",
    "            rem_starts_cut.append(rem_starts[idx])\n",
    "            rem_starts_cut.extend(ns_starts[starts_in_gap].tolist())\n",
    "            rem_stops_cut.extend(ns_stops[stops_in_gap].tolist())\n",
    "        continue\n",
    "\n",
    "rem_starts = np.array(rem_starts_cut)\n",
    "rem_stops = np.array(rem_stops_cut)\n",
    "\n",
    "m_lead, m_lag, b_lead, b_lag, rmse_lead, rmse_lag = fit_linear_lead_and_lag(interp_rate, stats_size + 1)\n",
    "\n",
    "rmse_cutoff = (np.minimum(rmse_lead[stats_size:-stats_size], rmse_lag[stats_size:-stats_size]) - 2.2).clip(min=0.0)\n",
    "possible_rem = correlate(rmse_cutoff, gskrn, mode=\"same\") / np.sum(gskrn)\n",
    "\n",
    "keep = np.ones(rem_starts.size, dtype=bool)\n",
    "\n",
    "for idx in range(rem_starts.size):\n",
    "    rem_lg = rem_stops[idx] - rem_starts[idx]\n",
    "    print(rem_lg)\n",
    "    if rem_lg <= 360 or not np.any(possible_rem[rem_starts[idx] - stats_size:rem_stops[idx] - stats_size] > 0.075) or not np.any(inst_rate_sm[rem_starts[idx] - stats_size:rem_stops[idx] - stats_size] > 66.0):\n",
    "        keep[idx] = False\n",
    "        print(\"deleting\", idx)\n",
    "\n",
    "rem_starts = rem_starts[keep] - stats_size\n",
    "rem_stops = rem_stops[keep] - stats_size\n",
    "        \n",
    "print(rem_starts.size)\n",
    "\n",
    "slp = slp[stats_size-curve_fw:curve_fw-stats_size]\n",
    "slp_sm = slp_sm[stats_size-curve_fw:curve_fw-stats_size]\n",
    "\n",
    "# _, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True)\n",
    "_, (ax3, ax4) = plt.subplots(2, 1, sharex=True)\n",
    "# ax1.imshow((ddcol * scale).clip(0.0, 1.0), aspect=\"auto\")\n",
    "# ax2.imshow(sharpen_data, aspect=\"auto\")\n",
    "ax3.imshow(np.square(mags).clip(max=np.percentile(np.square(mags), 99.95)), aspect=\"auto\")\n",
    "ax4.imshow(rate_data.clip(max=np.percentile(rate_data, 99.95)), aspect=\"auto\")\n",
    "ax4.plot((inst_rate - 50.0) * np.where(windows[:-1] > 0.0, 1.0, np.nan), c=\"r\", linestyle=\":\")\n",
    "ax4.plot(inst_std_sm * 0.04, c=\"y\", linestyle=\":\")\n",
    "ax4.plot(crv + 30, c=\"g\")\n",
    "ax4.plot(slp + 40, c=\"m\")\n",
    "ax4.plot(slp_sm + 40, c=\"b\")\n",
    "# ax3.plot(m_lead[stats_size:-stats_size] + 100, c=\"r\", linestyle=\":\")\n",
    "# ax3.plot(m_lag[stats_size:-stats_size] + 100, c=\"r\", linestyle=\":\")\n",
    "ax3.plot(np.minimum(rmse_lead[stats_size:-stats_size], rmse_lag[stats_size:-stats_size]).clip(0.0, 10.0) * 10.0, c=\"g\", linestyle=\":\")\n",
    "ax3.plot(np.maximum(rmse_lead[stats_size:-stats_size], rmse_lag[stats_size:-stats_size]) / np.minimum(rmse_lead[stats_size:-stats_size], rmse_lag[stats_size:-stats_size]).clip(1.5e-1) * 10.0, c=\"m\", linestyle=\":\")\n",
    "ax3.plot(possible_rem * 40.0, c=\"y\", linestyle=\":\")\n",
    "ax4.plot(windows_wide * 5, c=\"b\")\n",
    "ax3.plot(inst_rate_fix, c=\"r\")\n",
    "ax3.plot(inst_rate_sm, c=\"g\")\n",
    "ax3.plot((inst_rate_sm[1:] - inst_rate_sm[:-1]) * 1000 + 100, c=\"r\")\n",
    "\n",
    "\n",
    "for idx in range(rem_starts.size):\n",
    "    ax4.axvspan(rem_starts[idx], rem_stops[idx], color=\"g\", alpha=0.2)\n",
    "\n",
    "for idx in range(res_x.size):\n",
    "    ax3.axvline(x=res_x[idx])\n",
    "# for idx in range(ns_starts.size):\n",
    "#     ax3.axvspan(ns_starts[idx] - stats_size, ns_stops[idx] - stats_size, color=\"g\", alpha=0.2)\n",
    "    \n",
    "print(rem_starts)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(10, 10))\n",
    "# ax1.imshow(mags.clip(max=np.percentile(mags, 95.0)), aspect=\"auto\")\n",
    "# ax2.plot(final_sm)\n",
    "# ax2.scatter(apneas, final_sm[apneas], c=\"k\", marker=\"+\", zorder=1000)\n",
    "# ax3.imshow(dcols, aspect=\"auto\")\n",
    "# plt.show()\n",
    "\n",
    "# _, ax = plt.subplots(1, 1)\n",
    "# ax.hist(pk_dist * 0.2, bins=120, range=(0.1, 24.1))\n",
    "# plt.show()\n",
    "\n",
    "# _, ax = plt.subplots(1, 1, sharex=True)\n",
    "# ax.plot(timestamps, final)\n",
    "# plt.show()\n",
    "\n",
    "# plt.close(\"all\")\n",
    "# _, ax = plt.subplots(1, 1)\n",
    "# ax.imshow(mags_z.clip(max=np.percentile(mags_z, 99.9)), aspect=\"auto\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8cc4d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "# classify stages of wakefulness and sleep, movement, and out-of-bed periods\n",
    "nps = -n[:, 0] * flip0 - n[:, 1] * flip1\n",
    "non_per = np.where(windows == 0.0, sig, np.nan)\n",
    "winsize = 51\n",
    "mse = correlate(np.square(nps), np.ones(winsize), mode=\"full\") / winsize\n",
    "mse_lead = mse[:1 - winsize]\n",
    "mse_lag = mse[winsize - 1:]\n",
    "sm_min_mse = np.correlate(np.sqrt(np.minimum(mse_lead, mse_lag)), np.ones(1501), mode=\"same\") / 1501\n",
    "sm_max_mse = np.correlate(np.sqrt(np.maximum(mse_lead, mse_lag)), np.ones(1501), mode=\"same\") / 1501\n",
    "mins = np.where(windows == 0.0, np.sqrt(np.minimum(mse_lead, mse_lag)), np.nan)\n",
    "maxs = np.where(windows == 0.0, np.sqrt(np.maximum(mse_lead, mse_lag)), np.nan)\n",
    "prob_mvmt = 1.0 / (1.0 + np.exp(4.0 - 2.0 * sm_max_mse))\n",
    "prob_nib = 1.0 / (1.0 + np.exp(6.0 * sm_min_mse - 4.5))\n",
    "nib_cond = np.logical_and(np.sqrt(np.minimum(mse_lead, mse_lag)) < 1e-4, windows == 0.0)\n",
    "nib_maybe = np.logical_and(np.sqrt(np.minimum(mse_lead, mse_lag)) < 2e-4, windows == 0.0)\n",
    "nib_cond[0] = False\n",
    "nib_cond[-1] = False\n",
    "nib_starts = np.argwhere(np.logical_and(nib_cond[1:], np.logical_not(nib_cond[:-1]))).squeeze()\n",
    "nib_stops = np.argwhere(np.logical_and(nib_cond[:-1], np.logical_not(nib_cond[1:]))).squeeze()\n",
    "\n",
    "# to properly categorize me as being out of bed...\n",
    "# first, take the windows *between* where I'm classified as moving/in bed/etc\n",
    "# if they're a) pretty short and b) don't go too much above the threshold and c) if they do, it's brief\n",
    "# then remove that stop and next start to join them up\n",
    "# finally, if any window is less than 10 seconds, it doesn't count.\n",
    "# Apneas might also be caught by this (and the under-10-second segments might also be apneas)\n",
    "# worth flagging anything that might fit!\n",
    "# also, out-of-bed signals probably need to be bracketed  by movement on either side...\n",
    "# getting movement right is important.\n",
    "\n",
    "# movement is anything that's not periodic, and has larger amplitude than everything around it.\n",
    "# I'm not sure what the shortest duration movement could be...\n",
    "# for sure as low as 1 second, but more than, say, 2 frames?\n",
    "\n",
    "# there is one other condition, where the signal was definitely part of the breathing signal\n",
    "# but was not really periodic - some abnormality. Worth flagging those.\n",
    "# not always something that weird... sometimes just a breath that I didn't pause at the bottom, or did\n",
    "# or a particularly fast inhale or exhale, like a sigh.\n",
    "# How do we tell?\n",
    "# first of all, they're short - typically 2 seconds or less.\n",
    "# second of all, they aren't significantly bigger or smaller than the surrounding signal, about the same rms\n",
    "# might be worth, once I collect enough data, classifying them with a machine learning classifier of sorts\n",
    "\n",
    "#nib_stops[:-1] - nib_starts[1:]\n",
    "\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.plot(sig, alpha=0.2, linestyle=\":\")\n",
    "ax.plot(non_per)\n",
    "# ax.plot(prob_mvmt, linestyle=\":\")\n",
    "# ax.plot(prob_nib, linestyle=\":\")\n",
    "ax.plot(mins)\n",
    "ax.plot(maxs)\n",
    "ax.plot(nib_cond.astype(float))\n",
    "ax.legend([\"sig\", \"sig nonper\", \"min\", \"max\", \"notinbed\"])\n",
    "\n",
    "for idx in range(nib_starts.size):\n",
    "    ax.axvspan(nib_starts[idx], nib_stops[idx], color=\"g\", alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.scatter(mins, maxs)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "# the next step is to categorize the periodic segments\n",
    "# Artificially segment sleep segments into overlapping chunks\n",
    "# try to classify everything within each chunk - maybe assign probability to all classifications somehow\n",
    "# for each overlap piece, choose the highest overall probability weighted classification\n",
    "# this happens with at least three statistics - mean, slope and stderr of the breathing frequency\n",
    "# for a decreasing slope with fairly small stderr, it's falling asleep\n",
    "# for flat slope, small stderr, low frequency, it's deep sleep\n",
    "# for moderate slope, high stderr, REM sleep\n",
    "# for mild slope, low to moderate stderr, high frequency, it's awake\n",
    "# not sure how to categorize \"light sleep\", I don't have enough data yet\n",
    "# I think I can probably use the breathing style as well\n",
    "# ie how much time does each segment spend near 0 compared to at its peaks\n",
    "# I think when I'm awake, I don't spend very much time in the exhaled state\n",
    "# whereas in deep sleep, there very often is a pause between one exhale and the next inhale\n",
    "\n",
    "# how to do probability? First off, decide on a threshold and sharpness, use sigmoid to determine which side\n",
    "# each factor I measure can then be given eg a bayes factor or something. This gets tedious but it would be good\n",
    "# give each type of sleep a weight with which it corresponds to that factor - eg low frequency is critical for deep\n",
    "# the pause between breaths is a good signal for deep sleep, but only about 80% confidence. Reverse for awake, roughly\n",
    "# etc.\n",
    "\n",
    "# can I do this for the non-periodic stuff too? Two factors: duration, and rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find apneas, and cheyne-stokes style breathing\n",
    "# ie this is peak picking - can we use scipy or do I need to do it manually by breathing freq (sliding min/max filts)?\n",
    "\n",
    "# around 55900 there's a weird bit\n",
    "# 36600 cheynish\n",
    "# 338750 - cheynish, also some got excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20153800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final = flip_components_indiv_and_combine(n[:, :2], res_x)\n",
    "final_sm = cspline1d(final, lamb=5.0)\n",
    "\n",
    "pks, *_ = find_peaks(final_sm, prominence=0.7)\n",
    "pk_dist = pks[1:] - pks[:-1]\n",
    "apneas = pks[:-1][pk_dist > 30]\n",
    "\n",
    "lgth = final.shape[0]\n",
    "omega = 10.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "cwt_morlet = wavefinding_cwt(final, widths_morlet, omega)\n",
    "angles = np.angle(cwt_morlet)\n",
    "mags = np.square(np.abs(cwt_morlet))\n",
    "cols = np.mod(angles * 0.5 / np.pi + 0.5, 1)\n",
    "col_spec = cols - 0.5\n",
    "dcol = col_spec[:, 1:] - col_spec[:, :-1]\n",
    "dcol[dcol < -0.5] += 1.0\n",
    "dcol[dcol > 0.5] -= 1.0\n",
    "dcols = cm.hsv(dcol.clip(-0.00625, 0.00625) * 80.0 + 0.5)[..., :3]\n",
    "dcols *= (mags[:, 1:, None] / np.percentile(mags, 95.0)).clip(max=1.0)\n",
    "cols = cm.hsv(cols)[..., :3]\n",
    "cols *= mags[..., None]\n",
    "\n",
    "# ddcol = dcol[50:85] - dcol[49:84]\n",
    "scale = 500.0\n",
    "ddcol = dcol[1:] - dcol[:-1]\n",
    "rate_data = mags[50:85, 1:] * ddcol[49:84].clip(0.0) * np.exp(-scale * np.abs(dcol[50:85]))\n",
    "inst_rate = np.sum(np.arange(50, 85)[:, None] * rate_data, axis=0) / np.sum(rate_data, axis=0).clip(1e-5)\n",
    "inst_var = np.sum(np.square(np.arange(50, 85)[:, None] - inst_rate) * rate_data, axis=0) / np.sum(rate_data, axis=0).clip(1e-5)\n",
    "inst_std = np.sqrt(inst_var)\n",
    "inst_std_sm = correlate(inst_std, np.ones(31), mode=\"same\")\n",
    "\n",
    "_, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True)\n",
    "ax1.imshow((ddcol * scale).clip(0.0, 1.0), aspect=\"auto\")\n",
    "ax2.imshow(np.exp(-scale * np.abs(dcol)), aspect=\"auto\")\n",
    "ax3.imshow(mags.clip(max=np.percentile(mags, 98.0)), aspect=\"auto\")\n",
    "ax4.imshow(rate_data, aspect=\"auto\")\n",
    "ax4.plot(inst_rate - 50.0, c=\"r\", linestyle=\":\")\n",
    "# ax4.plot(inst_std_sm, c=\"y\", linestyle=\":\")\n",
    "plt.show()\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(10, 10))\n",
    "ax1.imshow(mags.clip(max=np.percentile(mags, 95.0)), aspect=\"auto\")\n",
    "ax2.plot(final_sm)\n",
    "ax2.scatter(apneas, final_sm[apneas], c=\"k\", marker=\"+\", zorder=1000)\n",
    "ax3.imshow(dcols, aspect=\"auto\")\n",
    "plt.show()\n",
    "\n",
    "_, ax = plt.subplots(1, 1)\n",
    "ax.hist(pk_dist * 0.2, bins=120, range=(0.1, 24.1))\n",
    "plt.show()\n",
    "\n",
    "_, ax = plt.subplots(1, 1, sharex=True)\n",
    "ax.plot(timestamps, final)\n",
    "plt.show()\n",
    "\n",
    "quit()\n",
    "\n",
    "n1 = norm_complex(n0_resz, n1_resz)\n",
    "n2 = norm_complex(n2_resz, n3_resz)\n",
    "\n",
    "n1 = smooth_and_norm_complex_stitch(n0_resz, n1_resz, n2_resz, n3_resz)\n",
    "\n",
    "m1, r1, res1, skew1 = compute_stats(n[:, 0], n[:, 1], winsize)\n",
    "m2, r2, res2, skew2 = compute_stats(n[:, 2], n[:, 3], winsize)\n",
    "\n",
    "# sig1 = smooth_and_norm(res1)\n",
    "# sig2 = smooth_and_norm(res2)\n",
    "\n",
    "\n",
    "# %%\n",
    "# CWT\n",
    "\n",
    "lgth = n1.shape[0]\n",
    "omega = 2.0\n",
    "fs = 5.0\n",
    "freqs = np.logspace(0.1, -1.4, 150)\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "cwt_morlet = wavefinding_cwt(n1, widths_morlet, omega)\n",
    "angles = np.angle(cwt_morlet)\n",
    "mags = np.square(np.abs(cwt_morlet))\n",
    "cols = np.mod(angles * 0.5 / np.pi + 0.5, 1)\n",
    "col_spec = cols - 0.5\n",
    "dcol = col_spec[:, 1:] - col_spec[:, :-1]\n",
    "dcol[dcol < -0.5] += 1.0\n",
    "dcol[dcol > 0.5] -= 1.0\n",
    "dcols = cm.hsv(dcol.clip(-0.0125, 0.0125) * 40.0 + 0.5)[..., :3]\n",
    "# dcols = cm.twilight_shifted(dcol.clip(-0.05, 0.05) * 10.0 + 0.5)[..., :3]\n",
    "dcols *= (mags[:, 1:, None] / np.percentile(mags, 95.0)).clip(max=1.0)\n",
    "cols = cm.hsv(cols)[..., :3]\n",
    "cols *= mags[..., None]\n",
    "\n",
    "sharps = np.sum(mags[:50], axis=0)\n",
    "sharps *= 0.001  # empirical constant\n",
    "sharps[0] = 0.0\n",
    "sharps[-1] = 0.0\n",
    "\n",
    "fatties = np.sum(mags[-50:], axis=0)\n",
    "fatties *= 0.002\n",
    "fatties[0] = 0.0\n",
    "fatties[-1] = 0.0\n",
    "\n",
    "mainline = np.sum(mags[50:-50], axis=0)\n",
    "mainline *= 0.004\n",
    "mainline[0] = 1.0\n",
    "mainline[-1] = 1.0\n",
    "\n",
    "summed = np.sum(mags, axis=0, keepdims=True)\n",
    "wt_mean = np.sum(np.arange(freqs.size)[:, None] * mags, axis=0, keepdims=True) / summed\n",
    "stdev = np.sqrt(\n",
    "    np.sum(mags * np.square(np.arange(freqs.size)[:, None] - wt_mean), axis=0) / summed\n",
    ")\n",
    "\n",
    "movement = np.logical_or(sharps > 0.5, np.abs(n1) > 5.00).astype(int)\n",
    "stationary = (mainline < 0.5).astype(int)\n",
    "\n",
    "mvmt_start = np.argwhere(movement[1:] - movement[:-1] == 1).squeeze()\n",
    "mvmt_end = np.argwhere(movement[1:] - movement[:-1] == -1).squeeze()\n",
    "stn_start = np.concatenate(\n",
    "    (\n",
    "        np.array([-2]),\n",
    "        np.argwhere(stationary[1:] - stationary[:-1] == 1).squeeze(),\n",
    "        np.array([lgth]),\n",
    "    )\n",
    ")\n",
    "stn_end = np.concatenate(\n",
    "    (\n",
    "        np.array([-1]),\n",
    "        np.argwhere(stationary[1:] - stationary[:-1] == -1).squeeze(),\n",
    "        np.array([lgth + 1]),\n",
    "    )\n",
    ")\n",
    "\n",
    "before_args = np.argmax(\n",
    "    np.mod(stn_end[:, None] - mvmt_start[None, :], lgth * 2), axis=0\n",
    ")\n",
    "before_okay = np.logical_and(\n",
    "    0 < mvmt_start - stn_end[before_args], mvmt_start - stn_end[before_args] < 15\n",
    ")\n",
    "before_dist = (mvmt_start - stn_start[before_args]) * before_okay.astype(int)\n",
    "mvmt_start_adj = mvmt_start - before_dist\n",
    "\n",
    "after_args = np.argmax(np.mod(mvmt_end[None, :] - stn_start[:, None], lgth * 2), axis=0)\n",
    "after_okay = np.logical_and(\n",
    "    0 < stn_start[after_args] - mvmt_end, stn_start[after_args] - mvmt_end < 15\n",
    ")\n",
    "after_dist = (stn_end[after_args] - mvmt_end) * after_okay.astype(int)\n",
    "mvmt_end_adj = mvmt_end + after_dist\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(10, 10))\n",
    "ax1.imshow(mags.clip(max=np.percentile(mags, 95.0)), aspect=\"auto\")\n",
    "ax2.plot(np.real(n1))\n",
    "ax2.plot(np.imag(n1))\n",
    "ax2.plot(np.abs(n1))\n",
    "ax2.plot(sharps.clip(max=2.0) * 10.0, linestyle=\":\")\n",
    "# ax2.plot(mainline, c=\"k\")\n",
    "# ax2.plot(stdev[0], c=\"b\")\n",
    "# ax2.plot(movement * 3.0, c=\"k\")\n",
    "# ax2.plot(stationary * 2.0, c=\"b\")\n",
    "ax2.scatter(mvmt_start_adj, np.ones_like(mvmt_start_adj), c=\"k\", zorder=1000)\n",
    "ax2.scatter(mvmt_end_adj, np.ones_like(mvmt_end_adj), c=\"b\", zorder=1001)\n",
    "ax3.imshow(dcols, aspect=\"auto\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(dcols, aspect=\"auto\")\n",
    "fig.set_size_inches(10, 4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "# dto = datetime.strptime(dt, \"%y%m%d_%H%M%S\")\n",
    "# start = dto.hour + dto.minute / 60.0 + dto.second / 3600.0\n",
    "# end = start + n.shape[0] / (5.0 * 3600.0)\n",
    "# t = start + np.arange(n.shape[0]) / (5.0 * 3600.0) - (24.0 if start > 12 else 0.0)\n",
    "\n",
    "# nfft = 64\n",
    "# f, tfft, s = spectrogram(sig1, fs=5.0, nfft=nfft, mode=\"magnitude\", nperseg=nfft)\n",
    "# spec = 10.0 * np.log10(np.square(s))\n",
    "\n",
    "# %%\n",
    "# _, (ax1, ax2) = plt.subplots(2, 1)\n",
    "# ax1.imshow(\n",
    "#     np.flip(spec, axis=0),\n",
    "#     vmin=np.percentile(spec, 20.0),\n",
    "#     vmax=np.percentile(spec, 99.5),\n",
    "#     extent=[t[0], t[-1], f[0], f[-1]],\n",
    "# )\n",
    "# ax2.plot(m1)\n",
    "# ax2.plot(r2 * 2.0 + 3)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "omega = 10.0\n",
    "widths_morlet = omega * fs / (freqs * 2 * np.pi)\n",
    "cwt_morlet = wavefinding_cwt(n1, widths_morlet, omega)\n",
    "angles = np.angle(cwt_morlet)\n",
    "mags = np.abs(cwt_morlet)\n",
    "cols = np.mod(angles * 0.5 / np.pi + 0.5, 1)\n",
    "col_spec = cols - 0.5\n",
    "dcol = col_spec[:, 1:] - col_spec[:, :-1]\n",
    "dcol[dcol < -0.5] += 1.0\n",
    "dcol[dcol > 0.5] -= 1.0\n",
    "dcols = cm.hsv(dcol.clip(-0.0125, 0.0125) * 40.0 + 0.5)[..., :3]\n",
    "# dcols = cm.twilight_shifted(dcol.clip(-0.05, 0.05) * 10.0 + 0.5)[..., :3]\n",
    "dcols *= (mags[:, 1:, None] / np.percentile(mags, 95.0)).clip(max=1.0)\n",
    "cols = cm.hsv(cols)[..., :3]\n",
    "cols *= mags[..., None]\n",
    "\n",
    "sharps = np.sum(mags[:50], axis=0)\n",
    "sharps *= 0.001 if omega > 4.0 else 0.005\n",
    "\n",
    "fatties = np.sum(mags[-50:], axis=0)\n",
    "fatties *= 0.002 if omega > 4.0 else 0.01\n",
    "\n",
    "mainline = np.sum(mags[50:-50], axis=0)\n",
    "mainline *= 0.004 if omega > 4.0 else 0.01\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(10, 10))\n",
    "ax1.imshow(mags.clip(max=np.percentile(mags, 95.0)), aspect=\"auto\")\n",
    "ax2.plot(np.real(n1))\n",
    "ax2.plot(np.imag(n1))\n",
    "ax2.plot(sharps)\n",
    "ax2.plot(fatties)\n",
    "ax2.plot(mainline, c=\"k\")\n",
    "ax3.imshow(dcols, aspect=\"auto\")\n",
    "fig.set_size_inches(10, 10)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ec702ec99fc876cfdbf044ecf43ee07871ec82d7de3b25f9d67add9fa01c6f9"
  },
  "kernelspec": {
   "display_name": "Python [conda env:sleepypi]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
